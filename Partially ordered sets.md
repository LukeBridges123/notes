## General Definitions and Examples
### Poset axioms
A partially ordered set (poset) is a set with a relation $\leq$ defined on it which is reflexive, transitive, and antisymmetric (i.e. if $a \leq b$ and $b \leq a$ then $a = b$). In contrast to a totally ordered set, the law of trichotomy (for any two elements $a, b$, we have $a \leq b$ or $b \leq a$) is not required to hold; thus it is possible to have "incomparable" elements of the poset. 
### Examples
Some examples of posets which are not totally ordered sets: the integers, ordered by divisibility; subsets of $[n]$, ordered by containment (this is called the "Boolean algebra" $B_n$); partitions of $[n]$, ordered by "refinement". The order on that last one (often denoted $\Pi_n$) is defined so that a partition $p$ is less than or equal to a partition $q$ if $q$'s blocks can be obtained by taking unions of $p$'s blocks; so, e.g. $\{1\}\{2\}\{3, 4\}$ is less than $\{1, 2\}\{3, 4\}$. 
### Maximal/minimal and minimum/maximum elements
An element $x$ of a poset is maximal if there is no $y$ with $x < y$, and a maximum if, for all $y$, we have $y \leq x$. In other words, maximal = nothing else is greater, maximum = everything else is less. Minimal and minimum elements are defined similarly. Minimum and maximum elements, if they exist, are unique, while minimal and maximal elements need not be. For instance, in the poset of integers greater than 1 ordered by divisibility, every prime is a minimal element, but there is no minimum element. (When all the positive integers are ordered by divisibility, 1 is the minimum element.) Note that in a totally ordered set, being a maximal (minimal) element is equivalent to being a maximum (minimum) element, so the distinction is unimportant. In a partially ordered set, a maximum element is necessarily a maximal element (and, if there is a maximum element, there must be no other maximum or maximal elements), but there can be maximal elements that aren't maximum elements (as the example with the primes shows).
### Isomorphisms
An isomorphism of posets $P_1, P_2$ is a bijection $f: P_1 \to P_2$ such that $x \leq y$ if and only if $f(x) \leq f(y)$; equivalently, it is an order-preserving bijection whose inverse is order-preserving.
## Chains and Antichains
### Definitions
A chain in a poset $P$ is a set of elements which are all comparable, or equivalently, a subset of $P$ which is a totally ordered set using the order inherited from $P$. An antichain is a subset of $P$ where no two elements are comparable. A chain cover is a set of disjoint chains whose union is the whole set. A maximum chain or antichain is one for which no larger chain or antichain exists, while a maximum chain or antichain is one for which no element can be added without making the set no longer be a chain or antichain. 

### Dilworth's Theorem
Theorem: the size of the smallest possible chain cover of a poset equals the size of any maximum antichain in that poset.

Proof: let $c$ be the number of chains in the smallest possible chain cover, $a$ be the number of elements in any maximal antichain. The easy part of the proof is showing that $c \geq a$: in order to cover the poset, all the elements of a maximal antichain must show up somewhere in the chain cover, but each chain can have at most one element from the antichain, so there must be at least as many chains as elements in the antichain. 

Now to prove that $c \leq a$, we use strong induction (supposing that the theorem holds for posets of size less than n) and divide into two cases.

Case 1: there exists a maximal antichain A, say of size $k$, which contains at least one non-maximal element and at least one non-minimal element. (For an example of this case, see below.) Since A is maximal, each element of the poset not in A must be comparable to at least one element of A (since, if there were an element not in A that isn't comparable to any elements in A, you could use that element to extend A). It turns out that the poset can be (almost) partitioned into two sets--say, an "upper" half U consisting of all the elements which are greater than or equal to at least one element in A, and a "lower" half L consisting of all the elements which are less than or equal to at least one element in A. This is an "almost" partition because U and L are not disjoint; rather, $U \cap L = A$. To see this, note that each element in $A$ is certainly in both $U$ and $L$. If there were an element $x$ such that $x$ is in U and L but not A then we would have $a_1, a_2$ in A such that $a_1 \leq x$ and $x \leq a_2$, but that would mean that $a_1$ and $a_2$ are comparable, contradicting the fact that A is an antichain. Note also that both $U$ and $L$ have elements which are not elements of A since A contains at least one non-maximal element (so that $U$ must have an element not in $A$) and at least one non-minimal element (so that $L$ must have an element not in A).

So, bringing it all together, we have two nonempty sets $U$ and $L$ that almost divide the poset in two, except that they share the elements of $A$. Both $U$ and $L$ must have size less than $n$, so by the induction hypothesis, Dilworth's theorem must hold in each of them. Both also clearly contain $A$ as a maximal antichain of length $k$, so by Dilworth's theorem they must each have a chain cover consisting of $k$ chains, for $2k$ chains total. Each of the chains in the cover of $U$ consists of an element of $A$ as its least element plus maybe some others, and similarly for all the chains in $L$. Thus for each $a \in A$ the chain in $U$ with $a$ at its bottom can be joined with the chain in $L$ with $a$ at its top, and this turns the $2k$ chains into $k$ chains, as desired.

Case 2: all maximal antichains consist solely of maximal elements or solely of minimal elements. In that case, each maximal antichain must contain all the maximal elements or all the minimal elements, since any missed maximal/minimal element could be added to the rest without disrupting the antichain property. Take a minimal element $x$ and a maximal element $y$ with $x \leq y$; it will always be possible to find such elements (else the poset would be infinite), and they will be distinct except when the poset as a whole is an antichain. Again let $k$ be the length of the largest chain in the poset $P$, and consider the new poset $P'$ given by removing $x$ and $y$. Given that any maximal antichain in the $P$ consists only of maximal or only of minimal elements, the same will be true of $P'$. In $P'$, however, a maximal antichain can, at best, get all of the minimal elements in $P$ except $x$, or all of the maximal elements except $y$. Thus the longest antichain in $P'$ has length at most $k-1$, and so by the induction hypothesis $P'$ has a maximal chain cover of $k-1$ chains. As for $P$, it can be covered by a maximal chain cover from $P'$ plus the chain $\{x, y\}$, for $k$ chains total.
#### Examples
For an example of how case 1 in the proof works, consider $B_3$. There, $\{1\}, \{2\}, \{3\}$ forms a maximal antichain, $U$ consists of all the sets of size $\geq$ 1, and $L$ consists of all the sets of size $\leq$ 1. An example of a chain cover of $U$ would be $\{\{1\}, \{1, 2\}, \{1, 2, 3\}\}, \{\{2\}, \{2, 3\}\}, \{\{3\}, \{1, 3\}\}$, and an example of a chain cover of $L$ would be $\{\{1,\}, \{\}\}, \{\{2\}\}, \{\{3\}\}$. These can then be merged in the obvious way to get a chain cover of $B_3$ using 3 chains.   

For an example of applying Dilworth's theorem, consider the following proposition: given a permutation $p$, let $d$ be the smallest integer such that $p$ is a union of $d$ increasing subsequences; then the longest decreasing subsequence in $p$ consists of $d$ elements. To prove this, define a new order on $[n]$ by having $p(i) \leq p(j)$ if and only if $p(i) \leq p(j)$ in the standard order on the integers, The chains in this partial order will be increasing subsequences of the permutation, while antichains will be decreasing subsequences, and applying Dilworth's theorem gives the desired result. 
## Incidence Algebras and MÃ¶bius Inversion
The following results apply to finite posets as well as certain classes of infinite posets. In particular, a poset must be "locally finite" (each interval, i.e. the set of all elements $z$ with $x \leq z \leq y$ for some fixed $x, y$, is finite), and each "principal ideal" (the set of all elements less than $x$ for some fixed $x$) must be finite as well. (An "ideal" is a set $I$ of elements from the poset such that, whenever $x \in I$ and $y \leq x$, $y \in I$ also. A principal ideal is an ideal generated by one element). 

### Definitions
The incidence algebra of a poset $P$, $I(P)$, is defined as the set of all real-valued functions on the closed intervals of $P$. By convention, all functions in the incidence algebra take a value of 0 on the empty interval. Equivalently, the incidence algebra can be defined in terms of functions from ordered pairs of elements from the poset to $R$, with the convention that $f(x, y) = 0$ when $x$ is not less than or equal to $y$ (including both when $y > x$ and when $x$ and $y$ are incomparable). 

Per the name, the incidence algebra is indeed an algebra over $R$. Addition and scalar multiplication are defined pointwise as expected, and multiplication is defined as follows:

$$(fg)(x, y) = \sum_{x \leq z \leq y}f(x, z)g(z, y)$$In the case where the poset is finite, there's a fairly natural isomorphism with the algebra of upper-triangular matrices. If you take a linear extension of the poset, so that the elements are labeled $x_1, \dots, x_n$ in an order-preserving way, then functions in the incidence algebra can be thought of as matrices where entry $i, j$ is $f(x_i, x_j)$. Because we never have $x_i > x_j$ when $i < j$, the entries below the diagonal will all be zero. Addition and scalar multiplication carry over just fine between the two ways of thinking about the incidence algebra, and as for multiplication, say we have elements $f, g$ of the incidence algebra represented as matrices $F, G$ (using the same linear extension for both, of course); then for the matrix product $FG$, the $i, j$ entry will be:

$$\sum_{k=1}^n f(x_i, x_k)g(x_k, x_j) = \sum_{k=i}^j f(x_i, x_k)g(x_k, x_j) = \sum_{x_i \leq x_k \leq x_j}f(x_i, x_k)g(x_k, x_j)$$ so that the $i, j$ entry equals $(fg)(x_i, x_j)$ as desired. (Above, the first equality comes from the fact that all terms with $k < i$ or $k > j$ will be zero, and the second comes from the fact that, for all $k$ in the interval of integers $[i, j]$, $f(x_i, x_k)$ will be zero when $x_k$ is not in the interval of poset elements $[x_i, x_j]$, so we can ignore those terms and sum only over $k$ with $x_k$ in that interval.)

The incidence algebra has an additive identity (the constant function $f(x, y) = 0$) and a multiplicative identity (the Kronecker delta, $\delta(x, y) = 1$ when $x = y$, 0 otherwise). 
### The Zeta Function
On any poset we can define the "zeta function" $\zeta(x, y) = 1$ if $x \leq y$, 0 otherwise. So the zeta function is a sort of indicator function for nonempty intervals. Its main purpose has to do with multiplication in the incidence algebra: $(f \cdot \zeta)(x, y) \sum_{x \leq z \leq y} f(x, z)\zeta(z, y) = \sum_{x \leq z \leq y}f(x, z)$; this shows up more dramatically in the MÃ¶bius inversion formula below. Another purpose turns out to be in counting *multichains* between two elements.
#### Counting Multichains and Chains
All of the above requires the poset to be locally finite, otherwise the sums being taken will diverge.

Define a multichain to be multiset $a_1, \dots, a_n$ with $a_1 \leq \dots \leq a_n$. (So, basically a chain with repetition allowed and strict inequalities relaxed to $\leq$.) For any two elements $x, y$, the number of multichains starting with $x$ and ending with $y$, i.e. $a_0 \leq \dots \leq a_k$ with $a_0 = x, a_k = y$, equals $\zeta^k(x, y)$. Proof: by induction. $\zeta^0 = \delta$, and this lines up with the fact that there is 1 single-element multichain between $x, y$ if and only if $x = y$. Similarly $\zeta^1(x, y) = 1$ if $x \leq y$ (in which case $x \leq y$ is the only two-element multichain between $x$ and $y$) and $0$ otherwise (in which case there are no multichains between $x$ and $y$). For the induction step, suppose this holds for all integers less than $k$. With each multichain $a_0, \dots a_k$, we can break that down into $y$ and the element immediately before it (i.e. $a_{k-1}$) and everything between $x$ and $a_{k-1}$. So to get the number of $k$-element multichains, we sum, over all elements $z$ that could serve as $a_{k-1}$, i.e. over all $z$ with $x \leq z \leq y$, the number of multichains $x = a_0 \leq \dots \leq a_{k-1} = z$. By the induction hypothesis that is $\zeta^{k-1}(x, z)$, so the sum is $\sum_{x \leq z \leq y}\zeta^{k-1}(x, z) = \sum_{x \leq z \leq y}\zeta^{k-1}(x, z)\zeta(z, y) = \zeta^k(x, y)$, as desired. 

As an analogue or application of this, if we take the poset to be the integers ordered by divisibility, $\zeta^2(1, n)$ is the number of multichains $1 \leq m \leq n$, which can be identified straightforwardly with the number of integers $m$ with $m|n$. 

Similarly, the number of chains of length $k$ between $x$ and $y$ (defining "length" to be one less than the number of elements) is $(\zeta - \delta)^k(x, y)$. Starting with $k = 1$, $(\zeta - \delta)(x, y)$ is 1 when $x < y$ and 0 otherwise; this corresponds to the fact that, when $x < y$, $x$ and $y$ give the only chain that fits all the requirements, when $x > y$, no chain is possible, and when $x = y$, because repetition isn't allowed and the inequalities are strict, we can't have any chains starting and ending with $x$. Then the induction proceeds in the same way. 
### The MÃ¶bius Function
That the zeta function should have an inverse can be seen, at least in the case of a finite poset, as following from the invertibility of the matrix corresponding to the zeta function (which is an upper-triangular matrix with ones along the diagonal, meaning it has a determinant of 1). The existence of this inverse (the MÃ¶bius mu function) can be proven, and a formula for the function given, by this recurrence:
$$\mu(x, y) = -\sum_{x \leq z < y}\mu(x, z)$$ with $\mu(x, x) = 1$. 

Proof: we need to have $\mu\zeta = \delta$, so that $(\mu \cdot \zeta)(x, y) = 1$ when $x = y$, $0$ otherwise. That first condition gives you the boundary condition $\mu(x, x)$ = 1, since we need to have $(\mu \cdot \zeta)(x, x) = \sum_{x \leq z \leq x}\zeta(x, z)\mu(z, x) = \zeta(x, x)\mu(x, x)$ be 1 and $\zeta(x, x) = 1$. As for the recurrence, we know that when $x < y$, we have $0 = (\mu \cdot \zeta)(x, y) =  \sum_{x \leq z \leq y}\mu(x, z)\zeta(z, y) = \sum_{x \leq z < y}\mu(x, z)\zeta(z, y) + \mu(x, y)$, and then subtracting off the sum from both sides gives $\mu(x, y) = - \sum_{x \leq z < y}\mu(x, z)\zeta(z, y)$, as desired. 

A variant of this is: $\mu(x, y) = -\sum_{x < z \leq y}\mu(z, y)$, which has a closely analogous proof using instead the fact that $(\zeta \cdot \mu)(x, y) = 0$ (i.e. that $\mu$ is a right inverse of $\zeta$, while the first proof used the fact that $\mu$ is a left inverse of $\zeta$).
#### Examples
##### Nonnegative integers with the standard order
In this case, we have $\mu(x, x) = 1$, and when $x < y$, we have $\mu(x, y) = -1$  if $y = x + 1$ and $\mu(x, y) = 0$ otherwise. The first fact is true of any poset. The second is an expression of the similarly general fact that, for any $y$ that covers $x$, $\mu(x, y) = -1$, since $0 = \sum_{x \leq z \leq y}\zeta(x, z)\mu(z, y) = \zeta(x, x)\mu(x, y) + \zeta(x, y)\mu(y, y) = \mu(x, y) + 1$. Then the rest follows inductively, using the formula $\mu(x, x + k)$ (with $k > 1$) = $-\sum_{x \leq z < x + k} = -(1 - 1 + 0 + ... + 0) = 0$. 
##### $B_n$
This helps explain the alternating signs that show up in inclusion-exclusion: letting $S, T$ be elements of $B_n$ with $S \subseteq T$, we have $\mu(S, T) = (-1)^{|T-S|}$ (that is, the exponent is the cardinality of $T$ without the elements of $T$.) To prove this, use induction on $|T-S|$ (call it $m$); when $m = 0$ we have $S = T$ and so $\mu(S, T) = 1$. For that matter, when $|T-S| = 1$, $T$ covers $S$, so we have $\mu(S, T) = -1$ like before. Now suppose that $\mu(S, T) = (-1)^{|T-S|}$ whenever $|T-S| < m$, for some $m$, and take $S, T$ with $|T-S| = m$. We know from the recurrence for $\mu$ that $\mu(S, T) = -\sum_{S \subseteq R \subset T}\mu(S, R)$, and by the induction hypothesis this equals $-\sum_{S \subseteq R \subset T}(-1)^{|R - S|}$. Looking at all the sets $R$ in the interval $[S, T)$, we see that there is only one with $|R - S| = 0$, that there are $m$ with $|R - S| = 1$ (since you can choose any one of the elements in $T$ that aren't in $S$ and add it to $S$ to get a set $R$ with this property, and there are $m$ such elements), and in general, for each $i$ with $0 \leq i < m$, there will be $\binom{m}{i}$ sets $R$ with $|R - S| = i$. Thus the sum becomes $-\sum_{i=0}^{m-1}\binom{m}{i}(-1)^i$.  Using the binomial theorem, $(-1 + 1)^m = \sum_{i = 0}^m \binom{m}{i}(-1)^i = (-1)^m + \sum_{i=0}^{m-1} \binom{m}{i}(-1)^i$. But also, $(-1 + 1)^m = 0$, so we have $(-1)^m = -\sum_{i=0}^{m-1}\binom{m}{i}(-1)^i = \mu(S, T)$, as desired. 
##### Positive integers ordered by divisibility
With some slight modification, this gives the classical MÃ¶bius mu function, defined by $\mu(n) = (-1)^k$ where $k$ is the number of prime factors of $n$ if $n$ is squarefree, and $\mu(n) = 0$ if $n$ is not squarefree. Of course, the MÃ¶bius function on posets takes two arguments, so instead we have $\mu(x, y) = (-1)^k$ where $k$ is the number of prime factors of $y/x$ if $y/x$ is squarefree, and $\mu(x, y) = 0$ if $y/x$ is not squarefree. However, the interval $[x, y]$ is isomorphic to $[1, y/x]$, so we can get away with working with intervals that start with $1$, i.e. $[1, n]$. In that case we call $\mu(1, n)$ just $\mu(n)$ and this ends up being the classical mu function. 

When $n$ is squarefree, so that $n = p_1\cdots p_k$ for primes $p_1 \cdots p_k$, then we have $\mu(n) = (-1)^k$ because each divisor of $n$ can be identified with a subset of $p_1, \dots, p_k$ and divisibility is equivalent to containment of those subsets. (1 corresponds to the empty set here.) Therefore the interval $[1, n]$ is isomorphic to $B_k$, and $\mu(1, n) = \mu(S, T)$ where $|T| = k, |S| = 0$, so $\mu(1, n) = (-1)^k$. When $n$ is not squarefree, one can instead think of divisors of $n$ as multisets of the prime factors of $n$, with the multiplicity of each prime factor being at most the multiplicity of the corresponding prime factor in $n$. However, this approach doesn't lead to a nice formula for $\mu(n)$ in such cases.

Instead, we can use induction, more specifically strong induction on $n$. $\mu(1) = 1, \mu(2) = \mu(3) = -1$ was established above, and $\mu(4) = 0$ follows from the fact that $\mu(1, 4) = -(\mu(1, 1) + \mu(1, 2)) = -(1 - 1) = 0$. Now suppose that, for a given $n$, the formula for $\mu(k)$ holds whenever $k < n$. By the recurrence for $\mu$ we know that $\mu(n) = -\sum_{d|n, d \neq n}\mu(d)$, that is, where $d$ ranges over all proper divisors of $n$. By the induction hypothesis, $\mu(d) = 0$ whenever $d$ is not squarefree, so we need only focus on the squarefree divisors of $n$. Letting $n = p_1^{i_1}\cdots p_k^{i_k}$, the squarefree divisors of $n$ are precisely the divisors of $p_1\cdots p_k$. In that case, the sum of $\mu(d)$ over all squarefree divisors of $n$ is just the sum of $\mu(d)$ over all elements of the closed interval $[1, p_1 \cdots p_k]$: that is, $\mu(n) = -\sum_{d \in [1, p_1 \cdots p_k]}\mu(d)$. However, the sum of $\mu$ over the elements of any closed interval is 0, so we get $\mu(n) = 0$. Continuing inductively over all non-squarefree integers then completes the proof. 
##### Direct product of two posets
Given two posets $P$, $Q$, define the "direct product" poset $P \times Q$ as a poset of ordered pairs $(p, q)$(one element from $p$, one element from $q$) with $(p, q) \leq (p', q')$ if and only if $p \leq_P p'$ and $q \leq_Q q'$. Then the MÃ¶bius function for $P \times Q$ is given by $\mu((p, q), (p', q')) = (\mu_P(p, p'))(\mu_Q(q, q'))$.  
### MÃ¶bius Inversion
Take any poset where all principal ideals are finite (since this ensures that sums over all elements less than some fixed element $y$ are convergent). Let a function on the poset elements $g$ be defined by $g(y) = \sum_{x \leq y}f(x)$, for some other function $f$. Then we have: $$f(y) = \sum_{x \leq y}g(x)\mu(x, y)$$ Proof: 

### Some Applications
#### Finite Differences
Take two sequences $a_n, b_n$ with $b_n = \sum_{i=0}^n a_n$. This can be rephrased as $b_n$ (considered as a function whose domain is the nonnegative integers) being $\sum_{i \leq n}a_n$, where $i$ ranges over all possible elements of the poset of nonnegative integers, ordered in the usual way. By MÃ¶bius inversion we have $a_n = \sum_{i \leq n}b_n\mu(i, n)$. But $\mu(n, n) = 1, \mu(n-1, n) = -1$, and all the other $\mu$ are just 0, so we get $a_n = b_n - b_{n-1}$. 
#### Binomial Inversion
Take two sequences $a_n$ and $b_n$ related by $b_n = \sum_{i = 0}^n \binom{n}{i}a_i$. If we introduce the functions on $B_n$ defined by $g(T) = b_{|T|}$, $f(S) = a_{|S|}$, then in fact we have $g([n]) = \sum_{S \subseteq [n]}f(S)$, since for any $i$, $a_i$ will appear exactly $\binom{n}{i}$ times in that sum, once for each $i$-element subset of $[n]$. By MÃ¶bius inversion, we have $f([n]) = \sum_{S \subseteq [n]} g(S)\mu(S, [n])$. The LHS is just $a_n$, and as for the RHS, each $g(S)\mu(S, [n])$ term becomes $b_{|S|}(-1)^{n-|S|}$, with $\binom{n}{i}$ such terms for each $i$. Collecting terms we get $$a_n = \sum_{i = 0}^n \binom{n}{i}b_i (-1)^{n - i}$$
#### The original MÃ¶bius Inversion
Take two sequences $a_n$ and $b_n$ defined by $b_n = \sum_{d|n}a_d$; this means that $b_n = \sum_{d \in [1, n]}a_d$, where the interval is defined using the poset of integers ordered by divisibility. Then using MÃ¶bius inversion, $a_n = \sum_{d \in [1, n]}b_d\mu(d, n)$. Equivalently, $a_n = \sum_{d \in [1, n]}b_d \mu(1, \frac{n}{d})$. Then using the convention $\mu(1, x) = \mu(x)$, we have $a_n = \sum_{d|n}b_d\mu(\frac{n}{d})$, which is the original statement of MÃ¶bius inversion. 
#### Inclusion-Exclusion
Given subsets $S_1 \dots S_n$ of a set $S$, define functions $B_n \to \mathbb{Z}$ by $f(I) = |\bigcap_{i \in I}S_i|$ ($I$ for a set of indices) and $g(I) = |\bigcap_{i \in I}S_i - \bigcup_{j \notin I}S_j|$. That is, $f(I)$ counts the number of elements in the intersection of the subsets indexed by elements of $I$, while $g(I)$ counts the number of elements that show up in that intersection, but not in any other subsets. These are related by the sort of sum that shows up in the dual version of MÃ¶bius inversion, namely $$f(I) = \sum_{J \supseteq I}g(J)$$ As for why: for any element $x \in \bigcap_{i \in I}S_i$, we can find a set of indices describing exactly the subsets where $x$ shows up; that is, there is some set of indices $i_1, \dots, i_k$ such that $x \in S_{i_1} \dots x \in S_{i_k}$, but $x \notin S_m$ for any index $m$ which is not one of the $i_1, \dots, i_k$. Any such set of indices will contain $I$ as a subset. For each $x$, there will be only one such set of indices, hence each element is counted only once on the RHS, and no other elements get counted. 

Now using MÃ¶bius inversion, we get $g(I) = \sum_{J \supseteq I}f(J)\mu(I, J) = \sum_{J \supseteq I}f(J)$ 