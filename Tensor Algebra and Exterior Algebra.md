# Multilinear Forms
Define a *k-linear form* on a vector space $V$ to be a function $V^k \to \R$ (or to whatever base field $V$ may have) which is linear in each argument. That is, if $f$ is a k-linear form, then for any index $i$, $f(v_1, \dots, av_i + bw_i, \dots, v_k) = af(v_1, \dots, v_i, \dots, v_k) + bf(v_1, \dots, w_i, \dots, v_k)$. Bilinear forms are a special case of this, including, for instance, the inner product; the determinant of an $n \times n$ matrix, considered as a function of its $n$ columns, is an $n$-linear form. A 1-linear form is simply a linear functional, i.e. an element of the [[Dual Vector Spaces|dual space]].
## Symmetric and Alternating Forms
Two important special types of bilinear forms are *symmetric* forms, for which $f(v, w) = f(w, v)$, and *alternating forms*, for which $f(v, w) = -f(w, v)$. (In the specific case of bilinear forms, The inner product (in a real vector space) is symmetric, while the determinant of a $2 \times 2$ matrix is alternating, as is the Lie bracket in any dimension.

The space of all bilinear forms is the direct sum of the space $\operatorname{Sym}^2(V^*)$ of symmetric forms and $\wedge^2(V^*)$ of alternating forms. Let $f(v, w)$ be a bilinear form; then we can define its "symmetrization" $Sf$ to be $\frac{1}{2}(f(v, w) + f(w, v))$ and its "alternation" $Af$ to be $\frac{1}{2}(f(v, w) - f(w, v))$. Note that $Sf + Af = f$, so $S + A = I$. Furthermore, $S$ and $A$ are projections onto the spaces of symmetric and alternating forms, $S(Sf) = Sf$ and $A(Af) = Af$. 

We can transfer these ideas to the more general setting of k-linear forms. A symmetric form is defined as one which is invariant under interchanges of arguments, $f(v_1, \dots, v_i, \dots, v_j, \dots, v_k) = f(v_1, \dots, v_j, \dots, v_i, \dots, v_k)$ for any possible transposition. Since we can build any permutation out of transpositions, this implies that a symmetric form is invariant under any permutation of its arguments. A skew-symmetric form is defined in much the same way, except that interchanging two of its arguments multiplies the result by $-1$. In that case applying an even permutation to the arguments leaves it the same, while applying an odd permutation changes its sign. Once again we can define projections $S$ and $A$ from the set of all k-linear forms to the sets of symmetric and skew-symmetric k-linear forms. For $S$, we have $(Sf)(v_1, \dots, v_k) = \frac{1}{k!}\sum_\sigma f(v_{\sigma(1)}, \dots, v_{\sigma(k)})$ where the index ranges over all permutations of $\{1, \dots, k\}$. Similarly we have $(Af)(v_1, \dots, v_k) = \frac{1}{k!}\sum_\sigma \operatorname{sign}(\sigma)f(v_{\sigma(1)}, \dots, v_{\sigma(k)})$. 

Our result about decomposing bilinear forms into symmetric and skew-symmetric parts does not transfer over to general $k$-forms, however.
## Tensor Product of Multilinear Forms
The simplest way to combine two multilinear forms, say a $p$-linear form and a $q$-linear form, into a $p+q$-linear form, is the *tensor product*. If $f, g$ are two such forms, we define $(f \otimes g)(v_1, \dots, v_{p+q}) = f(v_1, \dots, v_p)g(v_{p+1}, \dots, v_{p+q})$. This is then linear in each argument: letting $1 \leq i \leq p$, for example, we have $(f \otimes g)(v_1, \dots, av_i + bw_i, \dots, v_{p+q}) = f(v_1, \dots, av_i + bw_i , \dots, v_p)g(v_{p+1}, \dots, v_{p+q})$ = $(af(v_1, \dots, v_i, \dots, v_p) + bf(v_1, \dots, w_i, \dots, v_p))g(v_{p+1}, \dots, v_{p+q})$ = $af(v_1, \dots, v_i, \dots, v_p)g(v_{p+1}, \dots, v_{p+q}) + bf(v_1, \dots, w_i, \dots, v_p)g(v_{p+1}, \dots, v_{p+q})$ = $a(f \otimes g)(v_1, \dots, v_i, v_{p+q}) + b(f \otimes g)(v_1, \dots, w_i, \dots, v_{p+q})$. Obviously the same proof will work when $p+1 \leq i \leq p+q$. Thus the tensor product indeed gives us a $p+q$-linear form.  

The tensor product is associative: $(f \otimes g) \otimes h = f \otimes (g \otimes h)$, where $f, g, h$ are $p, q, r$-linear forms, respectively. This follows essentially just from the associativity of multiplication: $((f \otimes g) \otimes h)(v_1, \dots, v_{p+q+r}) = (f \otimes g)(v_1, \dots, v_{p+q}) \cdot h(v_{p+q+1}, \dots, v_{p+q+r})=$ $f(v_1, \dots, v_p)g(v_{p+1}, \dots, v_{p+q})h(v_{p+q+1},\dots, v_{p+q+r})$; this in turn is equal to $f(v_1, \dots, v_p) \cdot (g \otimes h)(v_{p+1}, \dots, v_{p+q+r}) = (f \otimes (g \otimes h))(v_1, \dots, v_{p+q+r})$. Thus $(f \otimes g) \otimes h = f \otimes (g \otimes h)$.   

The tensor product is also bilinear, or in other words satisfies the distributive laws $(af + bg) \otimes h = a(f \otimes h) + b(g \otimes h)$ and $f \otimes (ag + bh) = a(f \otimes g) + b(f \otimes h)$. This follows simply from the fact that $$((af + bg) \otimes h)(v_1, \dots, v_{p+q}) = (af + bg)(v_1, \dots, v_p) \cdot h(v_{p+1}, \dots, v_{p+q}) = af(v_1, \dots, v_p)h(v_{p+1}, \dots, v_{p+q}) + bg(v_1, \dots, v_p)h(v_{p+1}, \dots, v_{p+q}) = a(f \otimes h)(v_1, \dots, v_{p+q}) + b(g \otimes h)(v_1, \dots, v_{p+q})$$; a similar argument handles the other case. 
## Wedge Product
We can combine an alternating $p$-form and an alternating $q$-form into an alternating $p+q$ form using a bilinear product operation called the "wedge product". We define $(f \wedge g)(v_1, \dots, v_{p+q}) = \frac{1}{p!q!} \sum_\pi \operatorname{sign}(\pi)f(v_{\pi(1)}, \dots, v_{\pi(p)}) \cdot g(v_{\pi(p+1)}, \dots, v_{\pi(p+q)})$. In other words, we have $f \wedge g = \frac{(p+q)!}{p!q!} A(f \otimes g)$ --remember that we put a $\frac{1}{(p+q)!}$ in the definition of the alternation operator $A$, so we need to cancel it out and then put on a factor of $\frac{1}{p!q!}$. 

For convenience in what follows, we will introduce some non-standard notation and define the "non-normalized" alternation operator $\ol{A}$ by $(\ol{A}f)(v_1, \dots, v_k) = \sum_\sigma \operatorname{sign}(\sigma)f(v_{\sigma(1)}, \dots, v_{\sigma(k)})$, that is, $A$ without the normalizing factor of $\frac{1}{k!}$. This is no longer a projection onto the space of alternating forms (applying it to an alternating form $g$ gives $k!g$ instead of just $g$), but it will make our calculations below simpler. In this notation, $f \wedge g = \frac{1}{p!q!}\ol{A}(f \otimes g)$. That this is alternating follows just from the fact that the alternation operator always outputs alternating forms. This can also be written $\sum_\sigma \operatorname{sign}(\sigma)f(v_{\sigma(1)}, \dots, v_{\sigma(p)}) \cdot g(v_{\sigma(p+1)}, \dots, v_{\sigma(p+q)})$ where we take the first sum just over all permutations of the indices and the second sum over *ascending subsets* of indices, defined as ....

Addition distributes over the wedge product: $(af + bg) \wedge h = a(f \wedge h) + b(g \wedge h)$, and $f \wedge (ag + bh) = a(f \wedge g) + b(f \wedge h)$; in other words the wedge product is bilinear. The wedge product is associative: $(f \wedge g) \wedge h = f \wedge (g \wedge h)$. It is anticommutative in a certain way: $f \wedge g = (-1)^{pq}(g \wedge f)$. These properties follow essentially from the corresponding properties of the tensor product and the linearity of the alternation operator. 

For distributivity on the left, we have $$(af + bg) \wedge h = \frac{1}{p!q!}\ol{A}(af + bg \otimes h) = \frac{1}{p!q!}\ol{A}(a(f \otimes h) + b(g \otimes h)) = \frac{a}{p!q!}\ol{A}(f \otimes h) + \frac{b}{p!q!}\ol{A}(g \otimes h) = a(f \wedge h) + b(g \wedge h)$$. A similar argument suffices to show that $f \wedge (ag + bh) = a(f \wedge g) + b(f \wedge h)$. 

For anticommutativity, we want to prove $\frac{1}{p!q!}\ol{A}(f \otimes g) = \frac{(-1)^{pq}}{p!q!}\ol{A}(g \otimes f)$. Expanding out the definition, we have $\ol{A}(f \otimes g)(v_1, \dots, v_{p+q}) = \sum_\pi \operatorname{sign}(\pi) f(v_{\pi(1)}, \dots, v_{\pi(p)})g(v_{\pi(p+1)}, \dots, v_{\pi(p+q)})$, and $\ol{A}(g \otimes f)(v_1, \dots, v_{p+q}) = \sum_\pi \operatorname{sign}(\pi)  g(v_{\pi(1)}, \dots, v_{\pi(q)})f(v_{\pi(q+1)}, \dots, v_{\pi(p+q)})$. Note that the exact same terms show up in each sum, up to a potential change of sign. Take for instance a term from the first sum, $f(v_{\pi(1)}, \dots, v_{\pi(p)})g(v_{\pi(p+1)}, \dots, v_{\pi(p+q)})$. Now let $\sigma$ be the permutation that sends $1$ to $q+1$, $2$ to $q+2$, and so on, up to sending $p$ to $p+q$; and sends $p+1$ to $1$, $p+2$ to $2$, and so on. Then $f(v_{\pi\sigma(1)}, \dots, v_{\pi\sigma(p)})g(v_{\pi\sigma(p+1)}, \dots, v_{\pi\sigma(p+q)}) = f(v_{\pi(q+1)}, \dots, v_{\pi(p+q)})g(v_{\pi(1)}, \dots, v_{\pi(q)})$, which is exactly a term from the second sum. Thus to every term $\operatorname{sign}(\pi)f(v_{\pi(q+1)}, \dots, v_{\pi(p+q)})g(v_{\pi(1)}, \dots, v_{\pi(q)})$ in the second sum, there corresponds a term $\operatorname{sign}(\pi \sigma)f(v_{\pi\sigma(1)}, \dots, v_{\pi\sigma(p)})g(v_{\pi\sigma(p+1)}, \dots, v_{\pi\sigma(p+q)}) = \operatorname{sign}(\sigma)\operatorname{sign}(\pi)f(v_{\pi\sigma(1)}, \dots, v_{\pi\sigma(p)})g(v_{\pi\sigma(p+1)}, \dots, v_{\pi\sigma(p+q)})$ in the first sum, meaning that we can rewrite the first sum as $\operatorname{sign}(\sigma)$ times the second sum. Thus $g \wedge f = \sign(\sigma) f \wedge g$. All we need to do now is calculate $\sign(\sigma)$. In one-line notation $\sigma$ is equal to $(q+1, q+2, \dots, q + p, 1, 2, \dots, q)$. Recall now that a permutation is even/odd if and only if it has an even/odd number of [[Cycles and Inversions in Permutations|inversions]], i.e. pairs of indices $i < j$ such that $\pi(j) < \pi(i)$. This then corresponds to the number of pairs of numbers such that $a < b$ but $b$ is written before $a$ in the one-line notation. In the one-line notation above, clearly each pair of a number from $q+1, \dots, q+p$ and $1, \dots, q$ is an inversion, and there are no other inversions. Since there are $pq$ such pairs, there are $pq$ inversions, so $\sign(\sigma) = (-1)^{pq}$. 

For associativity ... 

We can now justify the $\wedge^k (V^*)$ notation for the space of alternating $k$-forms....we can also arbitrarily declare $\wedge^0 (V^*) = \R$, and then define the "Grassman algebra" or "exterior algebra" $\wedge^* (V^*) = \oplus_{k=0}^n \wedge^K (V^*)$. This is then an associative algebra which contains $\R$ and $V^*$ in a natural way....

## Basis for the Exterior Algebra
Let $e_1, \dots, e_n$ be a basis for $V$, and let $e^1, \dots, e^n$ be the corresponding dual basis. Then the set of all alternating $p$-forms of the form $e^{i_1} \wedge \dots \wedge e^{i_p}$, with $i_1 < \dots < i_p$, is a basis of $\wedge^p (V^*)$. 

For example, $e^1 \wedge e^2, e^2 \wedge e^3, e^1 \wedge e^3$ is a basis of $\wedge^2 (\R^3)^*$. 

Proof: for linear independence, say that $0 = \sum a_I e^I$ where the sum is taken over all  ascending multiindices $I = (i_1, \dots, i_p)$. Take some other multiindex $J = (j_1, \dots, j_p)$ and evaluate this on $e_J$. Note that $e^Ie_J = e^{i_1}(e_{j_1}) \cdots e^{i_p}(e_{j_p}) = \delta^{i_1}_{j_1} \cdots \delta^{i_p}_{j_p}$, which is $1$ only when $I = J$. Thus $0(e_J) = (\sum a_I e^I)(e_J) = a_J$, so $a_J = 0$. 

To show that these span $\wedge^p (V^*)$, given a $p$-form $\omega$, consider $\sum_I \omega(e_I)e^I$. Evaluating this at $e_J$ gets $\omega(e_J)$, so this sum agrees with $\omega$ on all ascending $p$-tuples of basis vectors. This then implies that it agrees on all $p$-tuples of basis vectors, by the alternating property, and multilinearity implies that the sum takes the same values as $\omega$ on all $p$-tuples of vectors. Thus $\omega$ is a linear combination of the $e^I$. 
### Dimension of The Exterior Algebra
Since there are $\binom{n}{p}$ ascending multiindices $1 \leq i_1 < \dots i_p \leq n$, there are $\binom{n}{p}$ elements in the standard basis of $\wedge^p (V^*)$ where $V$ is an $n$-dimensional vector space. For $p > n$, there are no nonzero elements of $\wedge^p (V^*)$.  The entire exterior algebra $\wedge^* (V^*)$ has $\sum_p \binom{n}{p} = 2^n$ elements in its basis. 

(N.B. in the following, to save on asterisks, we abuse notation slightly and identify $V^*$ with $V$.) We can extend any linear map $L: V \to W$ to a unique linear map $L': \wedge^*(V) \to \wedge^*(W)$ with the following properties: $L' = I$ on $\wedge^0(V)$ (which is $\R$), $L' = L$ on $\wedge^1(V)$ (which is $V$), and $L'$ is an algebra homomorphism, i.e. $L'(x \wedge y) = L'(x) \wedge L'(y)$ for all $x, y$.

FIX LATER 

Proof: we just define $L'(1) = 1, L'(e^1 \wedge \dots \wedge e^n) = L(e^1) \wedge \dots \wedge L(e^n)$, and extend by linearity. To show that it is an algebra homomorphism, for any wedge product $a \wedge b$ we have $a = \sum_I a_Ie^I$ and $b = \sum_J b_Je^J$ where $I, J$ range over the multiindices of the right length. Then $L'(a \wedge b) = \sum_{I, J} a_Ib_J L'(e^I \wedge e^J)$ by linearity. But $L'(e_I \wedge e_J) = L(e^I) \wedge L(e^J)$ (applying $L$ to each basis vector individually)..... which is then equal to $L'(a) \wedge L'(b)$. Thus $L'$ ....

Note that $\wedge^n(V^*)$ has dimension $1$; all alternating $n$-forms are scalar multiples of the determinant. 

## Interior Multiplication
Interior multiplication or "contraction" is an operation that takes k-linear forms to (k-1)-linear forms. For any vector $v$ and $k$-form $\omega(v_1, \dots, v_k)$, we define the interior multiplication of $\omega$ by $v$, $i_v(\omega)$, by $i_v(\omega)(v_1, \dots, v_{k-1}) = \omega(v, v_1, \dots, v_{k-1})$. In the special case of a $0$-form we define $i_v(\omega) = 0$, and for a $1$-form we define $i_v(\omega) = \omega(v)$.

When $\omega$ is alternating, we note that interior multiplication is nilpotent ($i_v \circ i_v = 0$) and is a skew-derivation, $i_v(\omega \wedge \nu) = i_v(\omega) \wedge \nu + (-1)^k \omega \wedge i_v(\nu)$, where $\omega$ is a k-form. 

The first fact follows from the fact that, if $\omega$ is alternating and two of its arguments are equal, the result is $0$ (regardless of what the other arguments are). 
# General Tensors
