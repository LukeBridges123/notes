$\newcommand{\sign}{\operatorname{sign}}$
# Multilinear Forms
Define a *k-linear form* on a vector space $V$ to be a function $V^k \to \R$ (or to whatever base field $V$ may have) which is linear in each argument. That is, if $f$ is a k-linear form, then for any index $i$, $f(v_1, \dots, av_i + bw_i, \dots, v_k) = af(v_1, \dots, v_i, \dots, v_k) + bf(v_1, \dots, w_i, \dots, v_k)$. Bilinear forms are a special case of this, including, for instance, the inner product; the determinant of an $n \times n$ matrix, considered as a function of its $n$ columns, is an $n$-linear form. A 1-linear form is simply a linear functional, i.e. an element of the [[Dual Vector Spaces|dual space]].
## Symmetric and Alternating Forms
Two important special types of bilinear forms are *symmetric* forms, for which $f(v, w) = f(w, v)$, and *alternating forms*, for which $f(v, w) = -f(w, v)$. (In the specific case of bilinear forms, The inner product (in a real vector space) is symmetric, while the determinant of a $2 \times 2$ matrix is alternating, as is the Lie bracket in any dimension.

The space of all bilinear forms is the direct sum of the space $\operatorname{Sym}^2(V^*)$ of symmetric forms and $\wedge^2(V^*)$ of alternating forms. Let $f(v, w)$ be a bilinear form; then we can define its "symmetrization" $Sf$ to be $\frac{1}{2}(f(v, w) + f(w, v))$ and its "alternation" $Af$ to be $\frac{1}{2}(f(v, w) - f(w, v))$. Note that $Sf + Af = f$, so $S + A = I$. Furthermore, $S$ and $A$ are projections onto the spaces of symmetric and alternating forms, $S(Sf) = Sf$ and $A(Af) = Af$. 

We can transfer these ideas to the more general setting of k-linear forms. A symmetric form is defined as one which is invariant under interchanges of arguments, $f(v_1, \dots, v_i, \dots, v_j, \dots, v_k) = f(v_1, \dots, v_j, \dots, v_i, \dots, v_k)$ for any possible transposition. Since we can build any permutation out of transpositions, this implies that a symmetric form is invariant under any permutation of its arguments. A skew-symmetric form is defined in much the same way, except that interchanging two of its arguments multiplies the result by $-1$. In that case applying an even permutation to the arguments leaves it the same, while applying an odd permutation changes its sign. Once again we can define projections $S$ and $A$ from the set of all k-linear forms to the sets of symmetric and skew-symmetric k-linear forms. For $S$, we have $(Sf)(v_1, \dots, v_k) = \frac{1}{k!}\sum_\sigma f(v_{\sigma(1)}, \dots, v_{\sigma(k)})$ where the index ranges over all permutations of $\{1, \dots, k\}$. Similarly we have $(Af)(v_1, \dots, v_k) = \frac{1}{k!}\sum_\sigma \operatorname{sign}(\sigma)f(v_{\sigma(1)}, \dots, v_{\sigma(k)})$. 

Our result about decomposing bilinear forms into symmetric and skew-symmetric parts does not transfer over to general $k$-forms, however.

....
## Tensor Product of Multilinear Forms
The simplest way to combine two multilinear forms, say a $p$-linear form and a $q$-linear form, into a $p+q$-linear form, is the *tensor product*. If $f, g$ are two such forms, we define $(f \otimes g)(v_1, \dots, v_{p+q}) = f(v_1, \dots, v_p)g(v_{p+1}, \dots, v_{p+q})$. This is then linear in each argument: letting $1 \leq i \leq p$, for example, we have $(f \otimes g)(v_1, \dots, av_i + bw_i, \dots, v_{p+q}) = f(v_1, \dots, av_i + bw_i , \dots, v_p)g(v_{p+1}, \dots, v_{p+q})$ = $(af(v_1, \dots, v_i, \dots, v_p) + bf(v_1, \dots, w_i, \dots, v_p))g(v_{p+1}, \dots, v_{p+q})$ = $af(v_1, \dots, v_i, \dots, v_p)g(v_{p+1}, \dots, v_{p+q}) + bf(v_1, \dots, w_i, \dots, v_p)g(v_{p+1}, \dots, v_{p+q})$ = $a(f \otimes g)(v_1, \dots, v_i, v_{p+q}) + b(f \otimes g)(v_1, \dots, w_i, \dots, v_{p+q})$. Obviously the same proof will work when $p+1 \leq i \leq p+q$. Thus the tensor product indeed gives us a $p+q$-linear form.  

The tensor product is associative: $(f \otimes g) \otimes h = f \otimes (g \otimes h)$, where $f, g, h$ are $p, q, r$-linear forms, respectively. This follows essentially just from the associativity of multiplication: $((f \otimes g) \otimes h)(v_1, \dots, v_{p+q+r}) = (f \otimes g)(v_1, \dots, v_{p+q}) \cdot h(v_{p+q+1}, \dots, v_{p+q+r})=$ $f(v_1, \dots, v_p)g(v_{p+1}, \dots, v_{p+q})h(v_{p+q+1},\dots, v_{p+q+r})$; this in turn is equal to $f(v_1, \dots, v_p) \cdot (g \otimes h)(v_{p+1}, \dots, v_{p+q+r}) = (f \otimes (g \otimes h))(v_1, \dots, v_{p+q+r})$. Thus $(f \otimes g) \otimes h = f \otimes (g \otimes h)$.   

The tensor product is also bilinear, or in other words satisfies the distributive laws $(af + bg) \otimes h = a(f \otimes h) + b(g \otimes h)$ and $f \otimes (ag + bh) = a(f \otimes g) + b(f \otimes h)$. This follows simply from the fact that $$((af + bg) \otimes h)(v_1, \dots, v_{p+q}) = (af + bg)(v_1, \dots, v_p) \cdot h(v_{p+1}, \dots, v_{p+q}) = af(v_1, \dots, v_p)h(v_{p+1}, \dots, v_{p+q}) + bg(v_1, \dots, v_p)h(v_{p+1}, \dots, v_{p+q}) = a(f \otimes h)(v_1, \dots, v_{p+q}) + b(g \otimes h)(v_1, \dots, v_{p+q})$$; a similar argument handles the other case. 
## Wedge Product
We can combine an alternating $p$-form and an alternating $q$-form into an alternating $p+q$ form using a bilinear product operation called the "wedge product". We define $(f \wedge g)(v_1, \dots, v_{p+q}) = \frac{1}{p!q!} \sum_\pi \operatorname{sign}(\pi)f(v_{\pi(1)}, \dots, v_{\pi(p)}) \cdot g(v_{\pi(p+1)}, \dots, v_{\pi(p+q)})$. In other words, we have $f \wedge g = \frac{(p+q)!}{p!q!} A(f \otimes g)$ --remember that we put a $\frac{1}{(p+q)!}$ in the definition of the alternation operator $A$, so we need to cancel it out and then put on a factor of $\frac{1}{p!q!}$. 

For convenience in what follows, we will introduce some non-standard notation and define the "non-normalized" alternation operator $\ol{A}$ by $(\ol{A}f)(v_1, \dots, v_k) = \sum_\sigma \operatorname{sign}(\sigma)f(v_{\sigma(1)}, \dots, v_{\sigma(k)})$, that is, $A$ without the normalizing factor of $\frac{1}{k!}$. This is no longer a projection onto the space of alternating forms (applying it to an alternating form $g$ gives $k!g$ instead of just $g$), but it will make our calculations below simpler. In this notation, $f \wedge g = \frac{1}{p!q!}\ol{A}(f \otimes g)$. That this is alternating follows just from the fact that the alternation operator always outputs alternating forms. This can also be written $\sum_\sigma \operatorname{sign}(\sigma)f(v_{\sigma(1)}, \dots, v_{\sigma(p)}) \cdot g(v_{\sigma(p+1)}, \dots, v_{\sigma(p+q)})$ where we take the first sum just over all permutations of the indices and the second sum over *ascending subsets* of indices, defined as ....

Addition distributes over the wedge product: $(af + bg) \wedge h = a(f \wedge h) + b(g \wedge h)$, and $f \wedge (ag + bh) = a(f \wedge g) + b(f \wedge h)$; in other words the wedge product is bilinear. The wedge product is associative: $(f \wedge g) \wedge h = f \wedge (g \wedge h)$. It is anticommutative in a certain way: $f \wedge g = (-1)^{pq}(g \wedge f)$. These properties follow essentially from the corresponding properties of the tensor product and the linearity of the alternation operator. 

For distributivity on the left, we have $$(af + bg) \wedge h = \frac{1}{p!q!}\ol{A}(af + bg \otimes h) = \frac{1}{p!q!}\ol{A}(a(f \otimes h) + b(g \otimes h)) = \frac{a}{p!q!}\ol{A}(f \otimes h) + \frac{b}{p!q!}\ol{A}(g \otimes h) = a(f \wedge h) + b(g \wedge h)$$. A similar argument suffices to show that $f \wedge (ag + bh) = a(f \wedge g) + b(f \wedge h)$. 

For anticommutativity, we want to prove $\frac{1}{p!q!}\ol{A}(f \otimes g) = \frac{(-1)^{pq}}{p!q!}\ol{A}(g \otimes f)$. Expanding out the definition, we have $\ol{A}(f \otimes g)(v_1, \dots, v_{p+q}) = \sum_\pi \operatorname{sign}(\pi) f(v_{\pi(1)}, \dots, v_{\pi(p)})g(v_{\pi(p+1)}, \dots, v_{\pi(p+q)})$, and $\ol{A}(g \otimes f)(v_1, \dots, v_{p+q}) = \sum_\pi \operatorname{sign}(\pi)  g(v_{\pi(1)}, \dots, v_{\pi(q)})f(v_{\pi(q+1)}, \dots, v_{\pi(p+q)})$. Note that the exact same terms show up in each sum, up to a potential change of sign. Take for instance a term from the first sum, $f(v_{\pi(1)}, \dots, v_{\pi(p)})g(v_{\pi(p+1)}, \dots, v_{\pi(p+q)})$. Now let $\sigma$ be the permutation that sends $1$ to $q+1$, $2$ to $q+2$, and so on, up to sending $p$ to $p+q$; and sends $p+1$ to $1$, $p+2$ to $2$, and so on. Then $f(v_{\pi\sigma(1)}, \dots, v_{\pi\sigma(p)})g(v_{\pi\sigma(p+1)}, \dots, v_{\pi\sigma(p+q)}) = f(v_{\pi(q+1)}, \dots, v_{\pi(p+q)})g(v_{\pi(1)}, \dots, v_{\pi(q)})$, which is exactly a term from the second sum. Thus to every term $\operatorname{sign}(\pi)f(v_{\pi(q+1)}, \dots, v_{\pi(p+q)})g(v_{\pi(1)}, \dots, v_{\pi(q)})$ in the second sum, there corresponds a term $\operatorname{sign}(\pi \sigma)f(v_{\pi\sigma(1)}, \dots, v_{\pi\sigma(p)})g(v_{\pi\sigma(p+1)}, \dots, v_{\pi\sigma(p+q)}) = \operatorname{sign}(\sigma)\operatorname{sign}(\pi)f(v_{\pi\sigma(1)}, \dots, v_{\pi\sigma(p)})g(v_{\pi\sigma(p+1)}, \dots, v_{\pi\sigma(p+q)})$ in the first sum, meaning that we can rewrite the first sum as $\operatorname{sign}(\sigma)$ times the second sum. Thus $g \wedge f = \sign(\sigma) f \wedge g$. All we need to do now is calculate $\sign(\sigma)$. In one-line notation $\sigma$ is equal to $(q+1, q+2, \dots, q + p, 1, 2, \dots, q)$. Recall now that a permutation is even/odd if and only if it has an even/odd number of [[Cycles and Inversions in Permutations|inversions]], i.e. pairs of indices $i < j$ such that $\pi(j) < \pi(i)$. This then corresponds to the number of pairs of numbers such that $a < b$ but $b$ is written before $a$ in the one-line notation. In the one-line notation above, clearly each pair of a number from $q+1, \dots, q+p$ and $1, \dots, q$ is an inversion, and there are no other inversions. Since there are $pq$ such pairs, there are $pq$ inversions, so $\sign(\sigma) = (-1)^{pq}$. 

For associativity ... 

We can now justify the $\wedge^k (V^*)$ notation for the space of alternating $k$-forms....we can also arbitrarily declare $\wedge^0 (V^*) = \R$, and then define the "Grassman algebra" or "exterior algebra" $\wedge^* (V^*) = \oplus_{k=0}^n \wedge^K (V^*)$. This is then an associative algebra which contains $\R$ and $V^*$ in a natural way

