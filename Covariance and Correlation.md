$\newcommand{\cov}{\operatorname{Cov}}$
# Covariance
## Definition
For random variables $X, Y$ with means $\mu_X, \mu_Y$, we define their covariance $\cov(X, Y)$ to be $E((X - \mu_X) \cdot (Y - \mu_Y))$. This is, as the name suggests, a measure of how much $X$ and $Y$ "vary together". To see why this should intuitively be the case, consider the following argument. If, whenever $X$ is far above its mean, $Y$ is also, and when $X$ is far above its mean, $Y$ is also, $(X - \mu_X) \cdot (Y - \mu_Y)$ will tend to be a product of two terms with the same sign, so $E((X - \mu_X)(Y - \mu_Y))$ will be positive. The same applies when the relationship is reversed; in that case $(X - \mu_X) \cdot (Y -\mu_Y)$ will tend to be negative. If $X$ and $Y$ "have little to do with each other", roughly speaking, then we expect that $(X - \mu_X) \cdot (Y - \mu_Y)$ will be positive sometimes and negative sometimes in equal measure, so $E((X - \mu_X)(Y - \mu_Y))$ will be close to $0$. 

For some special cases of the above, note that when $X = Y$, we have $\cov(X, Y) = \cov(X, X) = E((X - \mu_X)^2) = \var(X)$, which is always nonnegative. If $X$ and $Y$ are independent, then $E((X - \mu_X)(Y - \mu_Y)) = E(X - \mu_X)E(Y - \mu_Y) = 0 \cdot 0 = 0$. However, $\cov(X, Y) = 0$ does not imply that $X$ and $Y$ are independent. We instead say that, if $\cov(X, Y) = 0$, then $X$ and $Y$ are *uncorrelated*. 

By analogy with the formula $\var(X) = E(X^2) - E(X)^2$, we have $\cov(X, Y) = E(XY) - E(X)E(Y)$. 
## Covariance as an Inner Product
The covariance can be regarded as an inner product on the space of random variables (if, for technical reasons discussed below, we restrict ourselves to random variables with mean $0$). Recall the properties of an inner product on a real vector space: bilinearity, symmetry, and positive-definiteness, which here mean $\cov(aX + bY, Z) = a\cov(X, Z) + b\cov(Y, Z)$; $\cov(X, Y) = \cov(Y, X)$; and $\cov(X, X) \geq 0$ for all $X$, with equality only if $X = 0$. 

Bilinearity can be shown with a simple manipulation: $\cov(aX + bY, Z) = E((aX + bY - \mu_{aX+bY})(Z - \mu_Z))$, by definition; then $\mu_{aX+bY} = a\mu_X + b\mu_Y$, so this becomes $E(((aX - a\mu_X) + (bY - b\mu_Y))(Z - \mu_Z))=E(a(X - \mu_X)(Z - \mu_Z) + b(Y - \mu_Y)(Z - \mu_Z))$; then linearity of expectation implies this is equal to $aE((X - \mu_X)(Z-\mu_Z)) + bE((Y - \mu_Y)(Z - \mu_Z)) = a\cov(X, Z) + b\cov(Y, Z)$. 

Symmetry follows immediately from the definition: $E((X - \mu_X)(Y - \mu_Y))$ is clearly symmetric in $X$ and $Y$.

That $\cov(X, X) \geq 0$ follows simply from the fact that it is the variance of $X$, which is always nonnegative. However, any constant random variable, zero or otherwise, has a variance of zero, so $\cov$ is not positive-definite on the space of all random variables. Thus, we must restrict ourselves to the subspace of random variables with mean $0$. 

With this setup out of the way, we get geometric interpretations of a few probabilistic ideas. By analogy with $\langle x, x \rangle = |x|^2$, we have that $\var(X)$ acts like a squared norm induced by the inner product $\cov$, so $\sqrt{\var(X)} = \sigma_X$ is the corresponding norm. Since variables are uncorrelated if $\cov(X, Y) = 0$, uncorrelatedness is the same as orthogonality (and independence is not identical to orthogonality, but does imply it). The formula $\var(X + Y) = \var(X) + \var(Y)$ when $X, Y$ are independent is then analogous to the Pythagorean theorem, $|x + y|^2 = |x|^2 + |y|^2$ when $x, y$ are orthogonal. We can also use the inner-product interpretation to get a bound on the covariance in terms of the variance: $|\cov(X, Y)| \leq \sqrt{\var(X)\var(Y)}$, or $|\cov(X, Y)| \leq \sigma_X \sigma_Y$, by the Cauchy-Schwarz inequality $|\langle x, y \rangle|^2 \leq |x|^2 |y|^2$. 

Finally, note that the correlation coefficient $r$ defined below by $\frac{\cov(X, Y)}{\sigma_X\sigma_Y}$ will then be analogous to the cosine of the angle between two vectors, since $\cos(\theta) = \frac{\langle x, y \rangle}{|x||y|}$ for vectors in Euclidean space.
# The Correlation Coefficient *r*
## The "Linear-Plus-Noise" Model of Correlation
One approach to deriving the coefficient of correlation $r$ is to consider situations where one random variable $Y$ is a linear function of some other random variable $X$, along with some unknown factors (possibly just random error, possibly other important factors) which we represent by another random variable. That is, assuming without loss of generality that $X, Y$ both have a mean of $0$, we have $Y = mX + B$ where $m$ is a constant and $B$ is a random variable independent of $X$ with mean $0$. 

A typical example of this sort of situation comes when $Y$ represents measurements of some physical quantity $X$. If our measurements were perfect, we would have $Y = X$; if they aren't systematically biased, but are subject to random errors, we have $Y = X + B$ for some unknown "noise" term $B$ which is independent of $X$. We generally don't know $B$, 

In the case of perfect correlation, where $Y$ is just a linear function of $X$, $B$ should be identically $0$; since we've already assumed that it has a mean of $0$, this is equivalent to its standard deviation $\sigma_B$ equalling $0$. In the case of no correlation, where $Y$ is independent of $X$, we should have $m = 0$, so $Y = B$. 

To handle the general case, where we have some correlation but not perfect correlation, recall the intuitive idea of what $r$ should represent--how much of the variation in $Y$ comes from variation in $X$. Letting $\sigma_X, \sigma_Y, \sigma_B$ be the standard deviations of $X, Y, B$, we have $\sigma_Y = \sqrt{m^2\sigma_X^2 + \sigma_B^2}$, by applying a standard formula to $Y = mX + B$. If we want to have $r$ be the proportion of this that "comes from $X$", we can define $r$ to be the quotient the quotient $$r= \frac{\sigma_{mX}}{\sigma_Y} = \frac{m\sigma_X}{\sigma_Y} = \frac{m\sigma_X}{\sqrt{m^2\sigma_X^2 + \sigma_B^2}}$$ This gives the answers we expect in the limiting cases of perfect correlation and no correlation. If $\sigma_B = 0$, then this reduces to $r = \frac{m\sigma_X}{\sqrt{m^2\sigma_X^2}} = \frac{m\sigma_X}{|m|\sigma_X} = \frac{m}{|m|}$, which is 1 or -1 depending on the sign of $m$. If $m = 0$ (so $Y$ is independent of $X$), we instead get $r = 0$.

By squaring both sides we get $r^2 = m^2\var(X)/\var(Y) = m^2\var(X)/(m^2\var(X) + \var(B))$, so $r^2$ is the proportion of the variance of $Y$ that comes from the variance of $X$. 

In practice we rarely, if ever, have a precise specification of the noise variable $B$ and its standard deviation, or of the coefficient $m$, so these expressions generally cannot be used to calculate $r$. Below, we will see methods to calculate $r$ simply from the joint distribution of $X$ and $Y$, as well as to calculate $r$ for a finite set of $(x, y)$ points. We will then be able to use the expressions above to recover $m$ given $r, \sigma_X, \sigma_Y$. 
## Correlation as Normalized Covariance
Another approach to the correlation coefficient is to regard it as a dimensionless, "normalized" version of the covariance. We define $r = \frac{\cov(X, Y)}{\sigma_X\sigma_Y}$. In the case where $Y = mX + B$ as above, this gives us the same $r$ that we defined as $\frac{m\sigma_X}{\sigma_Y}$. To see this, we expand out $\cov(X, Y)$ as $E((X - \mu_X)(Y - \mu_Y)) = E(XY)$, since in the linear-plus-noise model we assume $\mu_X = \mu_Y = 0$. Since $Y = mX + B$, we have $E(XY) = E(X(mX + B)) = E(mX^2) + E(XB) = mE(X^2) + E(X)E(B) = mE(X^2)$, with the last two steps following from the independence of $X$ and $B$ and the fact that $E(X) = E(B) = 0$. Since $\var(X) = E(X^2) - E(X)^2 = E(X^2)$, we have $\frac{\cov(X, Y)}{\sigma_X\sigma_Y} = \frac{mE(X^2)}{\sigma_X\sigma_Y} = \frac{m\sigma_X^2}{\sigma_X\sigma_Y} = \frac{m\sigma_X}{\sigma_Y}$, which is exactly the expression for $r$ we got in the previous section. 

