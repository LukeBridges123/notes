# Binomial Distribution
The binomial distribution shows up whenever you do $n$ independent "trials" of something, each of which has a probability $p$ of succeeding (for example, biased but independent coin flips); it tells you the probability that $k$ of those trials will succeed. The probability mass function can be derived easily: any given string of $n$ trials that includes exactly $k$ successes will have a probability $p^k (1-p)^{n-k}$ of occurring, and there are $\binom{n}{k}$ such strings, hence the probability of $k$ successes is $\binom{n}{k}p^k (1-p)^{n-k}$.--thus explaining the name "[[Binomial and multinomial coefficients|binomial]] distribution". (Note, incidentally, that $1 = 1^n = (p + (1-p))^n = \sum_k \binom{n}{k}p^k (1-p)^{n-k}$ by the binomial theorem, so we can tell just from the pmf that this is a valid probability distribution where the probabilities sum to $1$.)

Note that the binomial distribution for $n$ trials is the same as the sum of $n$ i.i.d Bernoulli trials, i.e. $\sum_{i=1}^n X_i$ where each $X_i$ is $1$ with probability $p$ and 0 with probability $1-p$. This means that, by the [[Random Variables#Variance|general rules for sums of i.i.d. variables]], the binomial distribution will have mean $np$, variance $np(1-p)$, and standard deviation $\sqrt{np(1-p)}$. 

# Geometric Distribution
As in the binomial distribution, the geometric distribution describes processes where we repeatedly do independent trials that each have a probability $p$ of succeeding. Here, though, we look for the probability that our first success will happen on the $k$th trial, i.e. that we will have $k-1$ failures followed by a success. This is then equal to $p(1-p)^{k-1}$. The expected value is $E(X) = \sum_k kp(1-p)^{k-1} = p\sum_k k(1-p)^{k-1}$. Note that $\sum_k (1-p)^k = \frac{1}{1 - (1 - p)} = \frac{1}{p}$, and taking the derivative with respect to $p$ gets us $\sum_k k(1-p)^{k-1} \cdot -1 =  -\frac{1}{p^2}$, or $\sum_k k(1-p)^{k-1} = \frac{1}{p^2}$. Thus $E(X) = p \sum_k k(1-p)^{k-1} = \frac{p}{p^2} = 1/p$. (Hence the name "geometric distribution"--it ends up being tied to geometric series.)

Sometimes a different convention is used, where $X = k$ if there are $k$ failures before a success, not if the success happens on the $k$th trial. In that case, a similar computation gets an expected value of $\frac{1-p}{p}$. 
# Hypergeometric Distribution
The hypergeometric distribution models sampling without replacement from a finite population. Say we have a total population of $N$ individuals, $M$ of which have a certain characteristic. Suppose further that we sample $n$ individuals "uniformly at random without replacement" from the population--"uniformly" meaning that each $n$-element subset has an equal chance of being selected. We then ask for the probability that $k$ of the individuals in the sample are have the given characteristic, i.e. that they are from the sub-population of $M$ individuals. 

Clearly there are $\binom{N}{n}$ total ways to choose $n$ individuals from the population, $\binom{M}{k}$ ways to choose $k$ from the sub-population, and $\binom{N - M}{n - k}$ ways to choose $n - k$ from the rest. Thus there are $\binom{M}{k}\binom{N-M}{n-k}$ ways to pick a sample with $k$ individuals from the sub-population, for a probability of $\binom{M}{k}\binom{N-M}{n-k}/\binom{N}{n}$ of getting $k$ such individuals. 

Let the $n$ elements of a given sample be labeled $X_1, \dots, X_n$; think of each $X_i$ as a random variable which is $1$ if that element is a success (from the subpopulation of $M$) and $0$ otherwise. The probability of $k$ successes is then $P(X=k) = P(X_1 + \dots + X_n = k)$. The probability that $X_i = 1$ is just $\frac{M}{N}$ ; of course the $X_i$ are not independent, but we can still use linearity of expectation to get $E(X) = E(X_1 + \dots + X_n) = E(X_1) + \dots + E(X_n) = n\frac{M}{N}$. Letting $\frac{M}{N} = p$, this is identical to the mean of a binomial distribution with parameters $n, p$. Indeed, if we hold $\frac{M}{N}$ constant while letting $N$ go to infinity, then the hypergeometric distribution approaches the binomial distribution. Intuitively: for a finite population, drawing one success from the population changes the probability of future successes from $\frac{M}{N}$ to $\frac{M-1}{N-1}$; for a large enough population, we can treat these changes as negligible, so that each draw in the sample has a probability $p$ of succeeding. You then recover the binomial distribution. 

# Negative Binomial Distribution
For another variant on the binomial distribution, which is also a generalization of the geometric distribution, say we again have independent Bernoulli trials with a probability $p$ of success; instead of looking for the probability that there will be $k$ successes in $n$ total trials (fixed number of trials, variable number of successes), we look for the probability that there will be $k$ failures before $n$ total successes (fixed number of successes, variable number of trials). The case $n = 1$ gives the geometric distribution.

The probability of getting any particular string of $k$ failures and $n$ successes is $p^n (1-p)^k$. To count the number of such strings, note that we require the last element of the string to be a success, but otherwise we can distribute the remaining $n-1$ successes and $k$ failures however we want among the first $(n-1) + k$ positions in the string. There are $\binom{n + k - 1}{k}$ or $\binom{n+k-1}{n-1}$ ways to do that, hence $P(X =k) = \binom{n + k - 1}{k}p^n (1-p)^k$. 

# Poisson Distribution
In the [[Continuous Distributions#Exponential Distribution|exponential distribution]], we have events occuring randomly at a constant rate $\lambda$, in the sense that any interval $[t_1, t_2]$ will have $\lambda(t_2 - t_1)$ events on average, and events occur independently from one another. The exponential distribution gives us the probability that we will have to wait $t$ seconds before the next event occurs. The Poisson distribution starts from much the same setup, but instead tells us the probability that, within the next (say) second, $k$ events will occur. It is given by $P(X = k) = \frac{a^k e^{-a}}{k!}$. (The parameter $a$ will turn out to equal the expected number of events in the time interval under consideration, $a = \lambda t$.)
## Derivation of the Poisson Distribution
### Discrete Approximation
Let $t$ be the time interval and $\lambda$ be the rate at which events occur. Then divide $t$ into $n$ intervals, each of width $\epsilon = \frac{t}{n}$, and suppose that at most one event can occur in each interval. Whether an event occurs in a given interval is modeled by a Bernoulli trial; the expectation of this trial should be $\lambda \epsilon$, so the probability that an event will occur in any given interval is $p = \lambda \epsilon$. If we then want the probability that exactly $k$ events will occur, this is just given by a [[Discrete Distributions#Binomial Distribution|binomial distribution]] with parameters $p, n$. Thus $P(X = k) = \binom{n}{k}p^k (1-p)^{n-k}$, or, expanding out $p$, $\binom{n}{k}(\lambda \epsilon)^k (1 - \lambda \epsilon)^{n-k}$. 
### Continuous-Time Limit
Now we take the limit as $\epsilon$ goes to $0$ (and $n$, the number of sub-intervals, goes to infinity). Rewrite $P(X=k)$ as $\frac{n!}{(n-k)!k!}(\lambda \epsilon)^k (1 - \lambda \epsilon)^n (1 - \lambda \epsilon)^{-k}$. 

As $\epsilon$ goes to $0$, $(1 - \lambda \epsilon)$ goes to $1$ while $k$ stays constant, so the last term goes to $1$. For the second-to-last term, we can rewrite $\lambda \epsilon = \lambda \frac{t}{n} = \frac{a}{n}$ and then use $\lim_{n \to \infty} (1 - \frac{a}{n})^n = e^{-a}$.

As $n$ goes to infinity and $k$ stays fixed, and we can make the approximation $\frac{n!}{(n-k)!} = n^k$--the former is the number of ways to sample $k$ individuals without replacement from a population of $n$, the latter is the number of ways to do the same thing with replacement, and as the sample gets small relative to the population, it matters less and less whether you sample with or without replacement. 

Combining all this, we have that $P(X = k)$ approaches $\frac{n^k}{k!}(\lambda \epsilon)^k e^{-a}$. Merging the $n^k$ and the $(\lambda \epsilon)^k$ gets $(\lambda \epsilon n)^k = (\lambda t)^k = a^k$. Thus $P(X = k)$ approaches $\frac{a^k e^{-a}}{k!}$. 
## Properties
### Expectation
The definition of $a$ seems to imply that it should be the expected value of the Poisson distribution. Indeed $E(X) = \sum_k k\frac{a^k e^{-a}}{k!} = \sum_k \frac{a^k e^{-a}}{(k-1)!} = ae^{-a} \sum_k \frac{a^{k-1}}{(k-1)!} = ae^{-a}e^a = a$. 
### Mode 
The Poisson distribution is unimodal. To see this (and find the mode), note first that the sequence of probabilities for each $k$ is a constant multiple of the sequence $1, a, \frac{a^2}{2}, \dots, \frac{a^k}{k!}, \dots$ Note that $\frac{a^k}{k!} \leq \frac{a^{k+1}}{(k+1)!}$ if and only if $1 \leq \frac{a}{k+1}$ (dividing both sides by $a^k$ and multiplying by $k!$). This happens if and only if $k \leq a - 1$ Thus the last term which is larger than its predecessor is $\frac{a^{k+1}}{(k+1)!}$ where $k$ is the greatest integer less than $a-1$, or in other words $k$ is the likeliest value of the Poisson distribution if $k-1 = \operatorname{floor}(a-1)$. In the special case where $a$ is an integer, this happens when $a = k$. 

### Variance 
