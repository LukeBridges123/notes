# Binomial Distribution
The binomial distribution shows up whenever you do $n$ independent "trials" of something, each of which has a probability $p$ of succeeding (for example, biased but independent coin flips); it tells you the probability that $k$ of those trials will succeed. The probability mass function can be derived easily: any given string of $n$ trials that includes exactly $k$ successes will have a probability $p^k (1-p)^{n-k}$ of occurring, and there are $\binom{n}{k}$ such strings, hence the probability of $k$ successes is $\binom{n}{k}p^k (1-p)^{n-k}$.--thus explaining the name "[[Binomial and multinomial coefficients|binomial]] distribution". (Note, incidentally, that $1 = 1^n = (p + (1-p))^n = \sum_k \binom{n}{k}p^k (1-p)^{n-k}$ by the binomial theorem, so we can tell just from the pmf that this is a valid probability distribution where the probabilities sum to $1$.)

Note that the binomial distribution for $n$ trials is the same as the sum of $n$ i.i.d Bernoulli trials, i.e. $\sum_{i=1}^n X_i$ where each $X_i$ is $1$ with probability $p$ and 0 with probability $1-p$. This means that, by the [[Random Variables#Variance|general rules for sums of i.i.d. variables]], the binomial distribution will have mean $np$, variance $np(1-p)$, and standard deviation $\sqrt{np(1-p)}$. 

# Geometric Distribution
As in the binomial distribution, the geometric distribution describes processes where we repeatedly do independent trials that each have a probability $p$ of succeeding. Here, though, we look for the probability that our first success will happen on the $k$th trial, i.e. that we will have $k-1$ failures followed by a success. This is then equal to $p(1-p)^{k-1}$. The expected value is $E(X) = \sum_k kp(1-p)^{k-1} = p\sum_k k(1-p)^{k-1}$. Note that $\sum_k (1-p)^k = \frac{1}{1 - (1 - p)} = \frac{1}{p}$, and taking the derivative with respect to $p$ gets us $\sum_k k(1-p)^{k-1} \cdot -1 =  -\frac{1}{p^2}$, or $\sum_k k(1-p)^{k-1} = \frac{1}{p^2}$. Thus $E(X) = p \sum_k k(1-p)^{k-1} = \frac{p}{p^2} = 1/p$. (Hence the name "geometric distribution"--it ends up being tied to geometric series.)

Sometimes a different convention is used, where $X = k$ if there are $k$ failures before a success, not if the success happens on the $k$th trial. In that case, a similar computation gets an expected value of $\frac{1-p}{p}$. 
# Hypergeometric Distribution
The hypergeometric distribution models sampling without replacement from a finite population. Say we have a total population of $N$ individuals, $M$ of which have a certain characteristic. Suppose further that we sample $n$ individuals "uniformly at random without replacement" from the population--"uniformly" meaning that each $n$-element subset has an equal chance of being selected. We then ask for the probability that $k$ of the individuals in the sample are have the given characteristic, i.e. that they are from the sub-population of $M$ individuals. 

Clearly there are $\binom{N}{n}$ total ways to choose $n$ individuals from the population, $\binom{M}{k}$ ways to choose $k$ from the sub-population, and $\binom{N - M}{n - k}$ ways to choose $n - k$ from the rest. Thus there are $\binom{M}{k}\binom{N-M}{n-k}$ ways to pick a sample with $k$ individuals from the sub-population, for a probability of $\binom{M}{k}\binom{N-M}{n-k}/\binom{N}{n}$ of getting $k$ such individuals. 

Let the $n$ elements of a given sample be labeled $X_1, \dots, X_n$; think of each $X_i$ as a random variable which is $1$ if that element is a success (from the subpopulation of $M$) and $0$ otherwise. The probability of $k$ successes is then $P(X=k) = P(X_1 + \dots + X_n = k)$. The probability that $X_i = 1$ is just $\frac{M}{N}$ ; of course the $X_i$ are not independent, but we can still use linearity of expectation to get $E(X) = E(X_1 + \dots + X_n) = E(X_1) + \dots + E(X_n) = n\frac{M}{N}$. Letting $\frac{M}{N} = p$, this is identical to the mean of a binomial distribution with parameters $n, p$. Indeed, if we hold $\frac{M}{N}$ constant while letting $N$ go to infinity, then the hypergeometric distribution approaches the binomial distribution. Intuitively: for a finite population, drawing one success from the population changes the probability of future successes from $\frac{M}{N}$ to $\frac{M-1}{N-1}$; for a large enough population, we can treat these changes as negligible, so that each draw in the sample has a probability $p$ of succeeding. You then recover the binomial distribution. 

# Negative Binomial Distribution
For another variant on the binomial distribution, which is also a generalization of the geometric distribution, say we again have independent Bernoulli trials with a probability $p$ of success; instead of looking for the probability that there will be $k$ successes in $n$ total trials (fixed number of trials, variable number of successes), we look for the probability that there will be $k$ failures before $n$ total successes (fixed number of successes, variable number of trials). The case $n = 1$ gives the geometric distribution.

The probability of getting any particular string of $k$ failures and $n$ successes is $p^n (1-p)^k$. To count the number of such strings, note that we require the last element of the string to be a success, but otherwise we can distribute the remaining $n-1$ successes and $k$ failures however we want among the first $(n-1) + k$ positions in the string. There are $\binom{n + k - 1}{k}$ or $\binom{n+k-1}{n-1}$ ways to do that, hence $P(X =k) = \binom{n + k - 1}{k}p^n (1-p)^k$. 

# Poisson Distribution
