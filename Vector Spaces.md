## Vector Space Axioms
Motivation: vector space is any set of things that can be added together and multiplied by numbers/scalars (where "numbers" means elements of a field), such that addition and multiplication satisfy the properties found in addition and scalar multiplication for R^n, important classes of functions, and so on.

A vector space consists of a set of vectors (say V) and a field F; we say that V is a vector space over F.

Addition must be associative and commutative, have an identity ("0"), and have inverses (i.e. for every vector v, exists a vector -v $\in$ V s.t. v + (-v) = 0). Scalar multiplication must also be associative, and the multiplicative identity in the field ("1") must also be an identity for scalar multiplication of vectors. Importantly, a vector space must be closed under addition and scalar multiplication, i.e. adding or scaling a vector in V gives another vector in V. Finally, addition and scalar multiplication must satisfy the distributive laws a(u + v) = au + av and (a+b)v = av + bv (where u and v are vectors, a and b are scalars).

Usually what prevents something from being a vector space is closure, existence of an additive identity, or existence of inverses. (In particular, for a subset of a vector space, those are the only things that could prevent it from being a vector space itself--see below.) 

Some examples of vector spaces: any field can be considered a "vector space over itself", with addition of vectors being the already-defined addition of elements of the field. Similarly, the set of n-tuples of elements of a field (with addition and scalar multiplication defined componentwise) and functions from some set to a field (with addition and scalar multiplication defined as  (f+g)(x) = f(x) + g(x) and (af)(x) = a(f(x))) are also vector spaces. 

### Vector Space Arithmetic
In a vector space, the scalars in the field interact with the vectors in some intuitively clear ways. (N.B. 0 and 1 are the additive and multiplicative identities in the field, -1 is the additive inverse of 1, **0** is the additive identity in the vector space.) For example:

For any vector v, 0v = **0**. Proof: by the multiplicative identity axiom, 1v = v, so (1+0)v = v. Thus, by the distributive property, 1v + 0v = v + 0v = v. Subtracting v from both sides gets 0v = **0**. Some other results, generally with similar proofs, are: for all v, -1(v) = -v; for all scalars a, a**0** = **0**; -(-v) = v; if av = **0** then either a = 0 or v = **0**.

## Subspaces
A *subspace* of some vector space is a subset of the vector space which, when addition and scalar multiplication are defined the same way as in the "parent" space, is also a vector space. 
### Conditions to be a subspace
Verifying that something is a subspace of another vector space is easy, as most of the work has already been done in verifying that the parent space is a vector space. In fact, only 3 conditions are necessary. We can say that a subset of a vector space is a subspace if and only if it is closed under addition and scalar multiplication and has the additive identity. Proof: one direction is easy: if something is a subspace, then it's a vector space, so closure, etc. must be satisfied. For the other direction: most of the vector space axioms besides the three listed are automatically satisfied in a subset of a vector space by virtue of holding in the parent vector space. E.g. if addition commutes for every pair of vectors in V, then it also commutes for every pair of vectors in U $\subseteq$ V, because every pair of vectors in U is also a pair of vectors in V, and the sum of those two vectors is defined the same in both. The only one that's neither listed nor automatically satisfied is existence of additive inverses, but that follows from closure under scalar multiplication and the fact that -1(v) = -v. 

Note that the condition that the subset includes 0 is "almost unecessary": after all, couldn't we use closure under scalar multiplication and the fact that 0v = 0 to get that a subset closed under scalar multiplication contains the additive identity? But this argument only goes through if the subset contains some vector that can be multiplied by 0; the empty set satisfies every vector space axiom except containing an additive identity. Using the argument above, one could prove that, in the 3 conditions above, "contains 0" could be replaced with "is nonempty".
### Combining subspaces: Intersection and Union
Subspaces are closed under intersection, but generally not closed under union. The idea of a sum of subspaces provides a way to combine subspaces that satisfies many of the properties of a union while always resulting in another subspace.
#### Closure of Subspaces under Intersection
Say $U_1, U_2$ are subspaces of $V$. Then $U_1 \cap U_2$ is also a subspace of $V$. Proof: as is standard, we check only that $U_1 \cap U_2$ contains the additive identity and is closed under addition and scalar multiplication. Clearly it must have the identity, as, in order for $U_1$ and $U_2$ to be subspaces, they must both contain the identity. It also must be closed under scalar multiplication: given a $u \in U_1 \cap U_2$, $au$ (where $a$ is a scalar) must be in $U_1$ (since $u \in U_1$ and $U_1$ is closed under scalar multiplication), and by the same argument, must be in $U_2$ as well. As for closure under addition, let $u, v \in U_1 \cap U_2$; then $u + v \in U_1$, since $u \in U_1, v \in U_1$, and $U_1$ is closed under addition, and by the same argument, $u + v \in U_2$ also, so $u + v \in U_1 \cap U_2$.
#### Conditions for Closure under Union
The union of two subspaces $U_1$ and $U_2$ is a subspace if and only if $U_1 \subseteq U_2$ or $U_2 \subseteq U_1$. Proof: for one direction, suppose without loss of generality that $U_1 \subseteq U_2$. Then $U_1 \cup U_2 = U_2$, which is certainly a subspace. For the other direction, we prove the contrapositive: suppose that $U_1$ is not a subset of $U_2$ and $U_2$ is not a subset of $U_1$. Then we have some $u_1$ with $u_1 \in U_1, u_1 \notin U_2$ and some $u_2$ with $u_2 \in U_2, u_2 \notin U_1$. Now suppose for a contradiction that $U_1 \cup U_2$ is a subspace, and consider $u_1 + u_2$. If $U_1 \cup U_2$ is a subspace, then this must be in it, so it's in $U_1$ or $U_2$. But it can't be in $U_1$, since otherwise we could take $(u_1 + u_2) - u_1$ (which is certainly in $U_1$, given the fact that $-u_1 \in U_1$ and $U_1$ is closed under addition) and conclude that, in fact, $u_2 \in U_1$, contradicting the initial hypothesis that $u_2 \notin U_1$. Nor can it be in $U_2$, since otherwise $(u_1 + u_2) - u_2$ would be in $U_2$, and therefore so would $u_1$--another contradiction. So if the condition that one subspace is contained in the other is not met, the union of the two won't be a subspace itself; although, as shown in the first half of the proof, it really is a subspace as long as the condition is met.
### Combining Subspaces: Sums and Direct Sums

### Dimension of subspaces
Say that $V$ is vector space, $U$ is a subspace of $V$, dim($V$) = $n$, and dim($U$) = $m$. Then $m \leq n$, with equality achieved if and only if $U = V$.
Proof: a basis of $U$ is necessarily a linearly independent list of vectors in $V$ meaning that its length is at most the length of a list that spans $V$. In particular, it must be less than or equal to $n$, since a basis of $V$ certainly spans $V$.
For the second part of the statement: for one direction, suppose that dim($V$) = dim($U$). For convenience let $B$ be an arbitrary basis of $U$. Then $B$ has length $n$, and since it is a linearly independent list of vectors in $V$ with length $n$, it must be a basis of $V$ as well. Therefore span($B$) = $U$ and span$(B)$ = V, or $V = U$. For the other direction, if $V = U$, then obviously they must have the same dimension--the dimension of a vector space can't be "different from itself"!
#### Dimension of a Sum

