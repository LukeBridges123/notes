## Column Vector Notation
Elements of an n-dimensional vector space $V$ over a field F can easily be represented as if they are elements of $F^n$. Because $V$ has dimension n, it has a basis $v_1, ... v_n$, and any $v \in V$ can be represented as $a_1v_1 + ... + a_nv_n$. We further simplify this into the "column vector" $\begin{pmatrix} a_1 \\ \vdots \\ a_n \end{pmatrix}$. Given two vectors $u, v \in V$, addition and scalar multiplication of $u$ and $v$ corresponds precisely to adding and scalar-multiplying their column vectors using the usual rules for $F^n$. 
So it is as though $v$ is just an n-tuple of elements from the field. While $V$ may have some important underlying structure that is obscured by this representation (e.g. surely one does not always want to think of polynomials as just lists of their coefficients), this representation simplifies computations and is key for matrix representations of linear maps.
## The Matrix of a Linear Map
Consider a linear map $L: V -> W$, where $V$ has dimension n and $W$ has dimension m. It is uniquely determined by where it takes the basis vectors of $V$ (i.e. what the values of $L(v_1), ... L(v_n)$ are), and since (using column-vector notation) each of those values can be described by $m$ field elements each (say $L(v_1) = \begin{pmatrix} a_{1,1} \\ \vdots \\a_{m, 1} \end{pmatrix}), ... L(v_n) = \begin{pmatrix} a_{1,n} \\ \vdots \\a_{m, n} \end{pmatrix})$ , we can in fact represent the whole linear map using $m * n$ field elements: $\begin{pmatrix} a_{1,1} &... &a_{1,n} \\ \vdots & &\vdots \\ a_{m, 1}, &... &a_{m, n} \end{pmatrix}$ . This is the "matrix of $L$", an m by n matrix (i.e. with m rows, n columns).
### Matrix-vector multiplication
Given an element $v \in V$, how do we know what $L$ will do to it, given only $A$, the matrix of $L$? We know that $v = b_1v_1 + ... + b_nv_n$ where $v_1, ... v_n$ is a basis of $V$. By linearity, $L(v) = b_1L(v_1) + ... + b_nL(v_n)$. Using the entries in the matrix, we know that this equals $b_1(a_{1,1}w_1 + ... + a_{m, 1}w_n) + ... + b_n(a_{1, n}w_1 + ... + a_{m, n}w_n)$. In other words, $L(v)$ is a weighted sum of the vectors in $W$ corresponding to the columns of the matrix of $L$, with the "weight" on column i of the matrix given by the ith entry of the column vector of $v$. This can be rearranged to $(b_1a_{1,1} + ... + b_na_{1,n})w_1 + ... + (b_1a_{m,1} + ... + b_na_{m,n})w_n$, which has a column-vector representation as $((b_1a_{1,1} + ... + b_na_{1,n}), ... (b_1a_{m,1} + ... + b_na_{m,n}))$. Thus the ith entry of the column vector of $L(v)$ is the dot product of the column vector of $v$ and the ith row of the matrix. This defines *matrix-vector multiplication*, the method of computing the column-vector representation of $L(v)$ given the column-vector representation of $v$ and the matrix of $L$ (all with respect to a certain set of bases).
## Matrix Multiplication

