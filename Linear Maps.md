This note is about linear maps in the most general sense. For more specific sorts of linear maps, topics related to linear maps, etc. see: 

## Definitions
A *linear map* (or linear function, linear transformation, etc.) is a specific sort of function between two vector spaces. By definition, $T: V -> W$ is a linear map if, for every $u, v \in V$, $L(au + bv) = aL(u) + bL(v)$, where $a, b$ are scalars. In other words, $L$ "preserves addition and scalar multiplication": if vectors $u, v, w$ are "related" by addition in the sense that $u + v = w$, then the "transformed" versions of those vectors will be related in the same way: $L(u) + L(v) = L(u + v) = L(w)$, and the same goes for vectors "related" by scalar multiplication. (This makes linear maps the relevant notion of homomorphism for vector spaces.)

The set of all linear maps between $V$ and $W$ is denoted $\mathcal{L}(V, W)$.  With the standard definitions of addition and scalar multiplication of functions (i.e. "pointwise" addition and multiplication), this forms a vector space over the field used in $V$ and $W$. (Note that generally assume that a linear map will be between vector spaces over the same field.) Linear maps from a vector space to itself are often called "linear operators", and the set of all linear operators on $V$ is denoted $\mathcal{L}(V)$. 

Linear maps $T: V->W$ and $S: W -> U$ can be composed in the usual way to give a linear map $S \circ T: V -> U$. Sometimes this is referred to as a "product" of linear maps and denoted $ST$. (Possibly write something about composition of linear maps being linear; invertible linear operators forming a group.)

## Examples

## Linear Map is Determined By its Action on a Basis
Say you know the values of some function $L: V->W$ on a basis $v_1, ... v_n$ of $V$, i.e. you're given $L(v_i) = w_i$ for each relevant $i$,  and know that it is linear. Then for any vector $v \in V$, since $v = a_1v_1 + ... + a_nv_n$, we surely have $L(v) = L(a_1v_1 + ... + a_nv_n) = a_1L(v_1) + ... + a_nL(v_n) = a_1w_1 + ... + a_nw_n$. So we run with this: given those values, define a linear map by $L(v) = a_1w_1 + ... + a_nw_n$ where $v = a_1v_1 + ... + a_nv_n$. Now we prove that this really is a linear map, that it really does take the basis vectors to the desired values, and that there is only one linear map taking those vectors to those values. First, note that it is a function, since every vector has a basis representation and those representations are unique: thus every $v \in V$ will get mapped to one and only one element of $W$. As for linearity, first let $v = a_1v_1 + ... + a_nv_n$ and $u = b_1v_1 + ... + b_nv_n$ be elements of $V$. Then $L(v + w) = L((a_1 + b_1)v_1 + ... + (a_n + b_n)v_n) = (a_1 + b_1)w_1 + ... + (a_n + b_1)w_n$  = $a_1w_1 + ... + a_nv_n + b_1v_1 + ... + b_nv_n = L(v) + L(w)$, as desired. Same goes for homogeneity: $L(\lambda v) = L(\lambda a_1v_1 + ... + \lambda a_nv_n) = \lambda a_1w_1 + ... + \lambda a_nw_n = \lambda L(v)$, as desired. So $L$ is indeed linear. Also, we have $L(v_i) = L(0v_1 + ... + 1v_i + ... + 0v_n) = 0w_1 + ... + w_i + ... + 0w_n = w_i$, so $L$ has the specified values on the given basis. Now we prove that $L$ is unique: this is just the argument used to motivate $L$. I.e., suppose we are given a linear map with the property that $T(v_i) = w_i$ for all $i$; then given $v = a_1v_1 + ... + a_nv_n$, additivity implies that $T(v) = T(a_1v_1) + ... + T(a_nv_n)$, and homogeneity then implies that $T(v) = a_1T(v_1) + ... + a_nT(v_n)$--exactly the same value as given in the definition of $L$. So given a linear map with $L(v_i) = w_i$ for all $i$, it can only have one possible value for any $v \in V$ that can be written as a linear combination of $v_1, ... v_n$--but of course that includes all elements of $V$. So there is only one linear map with the desired properties.
### Representation of a Linear Map by a Matrix
The above leads to the use of matrices to represent linear maps: effectively having the ith column of the matrix show the vector to which the ith basis vector of the domain gets mapped (or, more precisely, showing, for the ith basis vector of the domain, to what linear combination of the basis vectors of the range it gets mapped). See [[Matrices]] for more. 
## Null Space (Kernel) and Range
With each linear map is associated two subspaces: its null space or kernel (a subspace of the domain) and its range (a subspace of its codomain). We prove that these are, in fact, subspaces, and develop a number of results about their dimension and the relation of that to whether the map is injective and/or surjective.
### Null Space and Range are Subspaces
#### Linear maps take 0 to 0
In order for the null space and range to be subspaces, they must both contain the zero vector of their respective "parent" vector spaces. We prove this, and in fact prove the stronger result that, for any linear map $T: V -> W$, $T(0) = 0$. Note that the $0$ on the left hand side is an element of $V$, and the zero on the right is an element of $W$; they may not be the same vector. Proof: Consider $T(v + 0)$. Clearly this must be equal to $T(v)$. Thus, by linearity, we have $T(v) + T(0) = T(v)$. Subtracting $T(v)$ from both sides gives the desired result.
#### Null Space is a Subspace
Having verified that $0 \in \text{null}(L)$, we must verify that the null space is closed under addition and scalar multiplication; this suffices to prove it is a subspace. Let $v, w \in$ null($L$), and let $a$ be a scalar; then, using the linearity of $L$, $L(v + w) = L(v) + L(w) = 0 + 0 = 0$, and $L(av) = aL(v) = 0$.
#### Range is a Subspace
Now we do the same for the range. Let $L(v), L(w)$ be elements of the range of $L$ , and again let $a$ be a scalar. Then, again using the linearity of $L$, $L(v) + L(w) = L(v + w)$ (which is clearly in the range--it is just the vector to which $v + w$ gets mapped), and $aL(v) = L(av)$. 
### Null space and condition for injectivity
The null space of a linear map is never empty, as it always contains $0$. However, if it contains anything else, it must not be injective, and the converse of this is true also. Proof: let $L$ be a linear map. If there is some $v \in$ null($L$) with $L(v) = 0$ but $v \neq 0$ , we have $L(v) = L(0)$ even though $v \neq 0$, so clearly $L$ is not injective. For the other direction, if $L$ is not injective, then we have two vectors $v, w$ with $v \neq w$ and $L(v) = L(w)$. Since $L(v) = L(w), L(v) - L(w) = 0$, so $L(v - w) = 0$. But since $v \neq w$, $v - w \neq 0$, so we have some nonzero vector (namely $v - w$) in the null space.
### Image of Basis Spans the Range
Here's an easy lemma which is occasionally useful: let $L: V -> W$ be a linear map, and let $v_1, ... v_n$ span $V$. Then $\{L(v_1), ... L(v_N)\}$ spans range$(L)$. Proof: let $v$ be an arbitrary element of $V$, so that $L(v)$ is an arbitrary element of the range. Then since $v = a_1v_1 + ... + a_nv_n$ for scalars $a_1, ... a_n$, , $L(v) = L(a_1v_1 + ... + a_nv_n) = a_1L(v_1) + .... + a_nL(v_n)$. Thus every element in the range of $L$ is in the span of $L(v_1), ... L(v_n)$; and certainly every element in span $L(v_1), ... L(v_n)$ is in range($L$) (as the range is a subspace, so it contains all linear combinations of its elements); so in fact range($L$) = span($L(v_1), ... L(v_n)$).
## Rank-Nullity Theorem
The rank-nullity theorem relates the dimensions of a linear map's domain, null space, and range. In particular, for a linear map $L: V->W$, where $V, W$ are finite-dimensional, we have dim($V$) = dim(null($L$)) + dim(range($L$)). Proof: start with a basis of null($L$), $v_1, ... v_m$, and extend it to a basis of $V$, $v_1, ... v_m, u_1, ... u_n$. So dim(null($L$)) = m and dim($V$) = m + n. Now we need to prove that dim(range($L$)) = n; in particular, we show that $L(u_1), ... L(u_n)$ is a basis of the range. By the lemma above, since $v_1, ... v_m, u_1, ... u_n$ spans $V$, $L(v_1), ... L(v_m), L(u_1), ... L(u_n)$ spans range($L$). We want to reduce this to a basis of range($L$). Now, $L(v_1), ... L(v_m)$ can all be removed without changing sthe span, since they are all 0, on account of $v_1, ... v_m$ all being elements of the null space. To show that the remaining vectors, $L(u_1), ... L(u_n)$ are linearly independent, suppose not: i.e. we have $a_1L(u_1) + ... a_nL(u_n) = 0$ with $a_1, ... a_n$ not all 0. Then by linearity, $L(a_1u_1 + ... + a_nu_n) = 0$, so $a_1u_1 + ... + a_nu_n \in$ null($L$). But that means that $a_1u_1 + ... + a_nu_n = b_1v_1 + ... + b_mv_m$, and subtracting $b_1v_1 + ... + b_mv_m$ from both sides implies that $v_1, ... v_m, u_1, ... u_n$ is linearly dependent, contradicting the hypothesis that it is a basis of $V$. So $L(u_1), ...  L(u_n)$ is linearly independent in addition to spanning range($L$), making it a basis of range($L$), and so range($L$) has dimension n. Thus null($L$) has dimension m, range($L$) has dimension n, and $V$ has dimension m + n, so the formula in the statement of the theorem holds.

To rephrase the proof: consider a basis of $V$ constructed from a basis of null($L$), $v_1, ... v_m$, plus some additional vectors $u_1, ... u_n$ whose span does not contain nonzero elements from null($L$) (else the nontrivial overlap in span would make $v_1, ... v_m, u_1, ... u_n$ linearly dependent). Then restrict the domain of $L$ to create a new map $L':$ span($u_1, ... u_n$) -> $W$., defined by $L'(v) = L(v)$. Since nothing in the span of $u_1, ... u_n$ besides 0 (uh oh looks like this proof might end up being a bit circular, relying on rank-nullity to prove that dim(range(L')) = n)
### Corollaries about dimension, injectivity, and surjectivity
The rank-nullity theorem leads to some quick corollaries about necessary conditions for a linear map to be injective or surjective in finite-dimensional spaces. Consider a map $L: V->W$, both spaces being finite-dimensional. 
#### Necessary condition for injectivity
If L is injective, then dim(V) $\leq$ dim(W); in other words, if dim(V) > dim(W), then no linear map from V to W can be injective. Proof: if dim(V) > dim(W), then dim(V) > dim(range($L$)) (since range($L$) is a subspace of $W$, dim(W) $\geq$ dim(range($L$)), so dim(V) - dim(range($L$)) > 0, meaning (by rank-nullity) dim(null($L$)) > 0. So null($L$) must contain vectors other than 0, meaning (by the fact that a linear map is injective iff its null space contains only 0) that $L$ is not injective.
#### Necessary condition for surjectivity
If $L$ is surjective, then dim(V) $\geq$ dim(W); in other words, if dim(V) < dim(W), then no linear map from V to W can be surjective. Proof: we know that, given a basis of V $v_1, ... v_n$, then $L(v_1),...L(v_n)$ spans range($L$); this implies that dim(range($L$)) $\leq$ dim(V). So if dim(V) < dim(W), then dim(range($L$)) < dim(W), so range($L$) $\neq$ W. (This result does not require the rank-nullity theorem--although there is another proof that uses it--but it fits better here.)
#### Equivalence of injectivity and surjectivity for operators on finite-dimensional spaces
Consider the special case of a map from a finite-dimensional vector space to itself. Suppose the map $L$ is injective; then null($L$) = $\{0\}$, and by the rank-nullity theorem, dim(range($L$)) = dim($V$) - dim(null($L$)) = dim($V$). So the range is a subspace of $V$ with dimension equal to dim($V$), i.e. the range is equal to the $V$ itself and so $L$ is surjective. Then suppose that $L$ is surjective; then certainly dim(range($L$)) = dim($V$), so by rank-nullity, dim(null($L$)) = 0, meaning $L$ is injective. Thus for a linear operator on a finite-dimensional space, injectivity or surjectivity alone immediately implies bijectivity and invertibility. 

Note that the hypothesis that $V$ is finite-dimensional is crucial here, as it is what makes it possible to apply the rank-nullity theorem. In fact, the above result is completely false for infinite-dimensional spaces. For instance, the "forward shift" map in $F^\infty$ (vector space of all infinite sequences of elements of $F$) given by $L((a_1, a_2, a_3, ...)) = (0, a_1, a_2, ...) is injective but not surjective, while differentiation in the vector space of all polynomials is surjective but not injective.
## Inverses
For linear maps, we can reasonably speak of an inverse, where $S: W->V$ is the inverse of $T: V->W$ if $ST = TS =$ $I$ (where $I$ is the identity map*), or in other words, $ST(x) = TS(x) = x$ for all x. We know for functions in general that if $T$ is bijective, it has an inverse. But in particular, we would like $T$ to have an inverse that is also linear. This is in fact true:

\* Here we abuse notation a bit and use $I$ to refer to both the identity map on $V$ and the identity map on $W$.
### Linear inverse exists iff map is bijective
Suppose that $T$ has an inverse map $S$. Then whenever $Tx = Ty$, we can apply $S$ to both sides to get $STx = STy$ or $x = y$; thus $T$ is injective. Similarly, given some $y$ in the codomain of $T$, consider $S(y)$; then $T(S(y)) = y$, so there is some element of the domain (namely $S(y)$) which gets mapped to $y$; thus $T$ is surjective also.

For the other direction, suppose $T:V->W$ is bijective. Define $S: W->V$ by $S(w) = v$ where $T(v) = w$. We need to verify a few things: that $S$ is a well-defined function, that it has the defining property of inverses ($ST = TS = I$), and that it is linear. 

As for the first, $S$ is defined over all of $W$ (since $T$ is surjective, every $w \in W$ is of the form $T(v)$ for some $v \in V$) and has a single value for each $w$ (since $T$ is injective.)

As for the second, given that $T(v) = w$, we know by definition that $S(w) = v$ , so $(T \circ S)w = T(S(w)) =  T(v) = w$, so $T \circ S$ is the identity map on $W$. Verifying that $S \circ T$ is the identity map on $V$ is more difficult: consider $(T \circ (S \circ T))v$; by the associativity of function composition, this equals $((T \circ S) \circ T)v$ which, as shown earlier, is equal to $(I \circ T)v$ or $Tv$. So $(T \circ (S \circ T))v = Tv$ or $T((S \circ T)v) = Tv$. Since $T$ is injective, we can "cancel" $T$ from both sides and get $(S \circ T)v = v$ for all $v$; thus $(S \circ T)$ is the identity map on $V$.

Finally, we need to verify that $S$ is linear. Let $w_1, w_2 \in W$ and consider $T(S(w_1) + S(w_2))$. By linearity this equals $T(S(w_1)) + T(S(w_2))$, which equals $w_1 + w_2$ by definition. So, we can say that $T$ maps $S(w_1) + S(w_2)$ to $w_1 + w_2$. But, by definition of $S$, we also know that $S(w_1 + w_2)$ is the element of $V$ (unique element, given the bijectivity of both functions) to which $T$ maps $w_1 + w_2$. Therefore $S(w_1 + w_2)$ and $S(w_1) + S(w_2)$ mean the same element of $V$, so $S$ is additive. The proof of S's homogeneity is basically the same. So, given that $T$ is bijective, we can define a map $S$ which inverts $T$ and is linear. 

## Isomorphisms
An *isomorphism*, in the context of vector spaces, is a bijective (/ invertible) linear map. In other words, it is a map that sets up a one-to-one correspondence between the two spaces which also preserves the additive and scaling structure that defines each vector space. 

### Finite-dimensional vector spaces are isomorphic iff they have the same dimension
Say that $V$ and $W$ both have dimension n (and are over the same field--all talk of isomorphism implicitly assumes this). Then they both have bases $v_1, ... v_n$ and $w_1, ... w_n$, and we can define an isomorphism by just mapping the basis vectors of $V$ to the basis vectors of $W$ (so $L(v_1) = w_1$, etc.). This does give us a linear map (since specifying values on a basis is enough to determine a linear map). It is surjective: every $w \in W$ has a representation $a_1w_1 + ... + a_nw_n$, and this is equal to $a_1L(v_1) + ... + a_nL(v_n) = L(a_1v_1 + ... + a_nv_n)$, and $a_1v_1 + ... + a_nv_n$ is certainly an element of $V$, so for each element of $W$ there is indeed a corresponding element of $V$. The fact that it is injective then follows from surjectivity and the equivalence between the two in finite-dimensional spaces; or one could verify injectivity separately using the uniqueness of basis representations. Thus the given $L$ is an isomorphism.

For the other direction, if $V$ and $W$ are finite-dimensional and isomorphic, then there is a bijective linear map $L$ between them. Since it is surjective, dim(range($L$)) = dim($W$), and since it is injective, dim(null($L$)) = 0. Thus by rank-nullity, dim(V) = dim(range($L$)) + dim(null($L$)) = dim(W). 

