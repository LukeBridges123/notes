# Gaussian Approximations
## Binomial Distribution
Take the binomial distribution for a fair coin flip, $B(n, 0.5)$. For convenience, we will let the total number of trials be $2n$, and instead of measuring the number of successes, we will measure the difference between the actual number $x$ of successes and the expected number $n$. This will give us a mean of $0$ for any $n$, thus avoiding annoyances due to the mean increasing as we increase $n$. 

In this notation, we can write the probability that we succeed $x$ more (or fewer, if $x$ is negative) times than $n$ as $P(x) = (1/2)^{x+n}(1/2)^{n - x} \binom{2n}{x + n} = \frac{1}{2^{2n}}\binom{2n}{x+n}$.

Now we approximate the binomial coefficient using Stirling's approximation, $n! \approx n^n e^{-n} \sqrt{2\pi n}$. We then have $\binom{2n}{x+n} = \frac{(2n)!}{(x+n)!(x-n)!} \approx \frac{(2n)^{2n} e^{-2n} \sqrt{2\pi \cdot 2n}}{(n+x)^{n+x}e^{-(n+x)}\sqrt{2\pi(n+x)}(n-x)^{-(n-x)}e^{n-x}\sqrt{2\pi(n-x)}}$. Combining the $e$ terms in the denominator gets us $e^{-n-x}e^{-n+x} = e^{-2n}$ which cancels with the $e$ term in the numerator. The $\sqrt{4\pi}$ in the numerator cancels with $\sqrt{4\pi^2}$ in the denominator, leaving $\sqrt{\pi}$. Thus, making all these cancellations and rearranging further, we get $\frac{(2n)^{2n}\sqrt{n}}{(n+x)^{n+x}(n-x)^{n-x}\sqrt{\pi(n^2 - x^2})}$.

To handle the first two terms in the denominator, we will need an approximation for expressions like $(1 + a)^m$. Taking logs and using the series for $\ln(1+x)$, we get $m(a - \frac{a^2}{2} + \dots)$, which we can truncate to just $ma - \frac{ma^2}{2}$; exponentiating both sides we get $(1 + a)^m \approx \exp(ma)\exp(-ma^2/2) = \exp(ma - \frac{ma^2}{2})$. If we pull a factor of $n$ out of the parentheses for both terms in the denominator we get $n^{n+x}(1 + \frac{x}{n})^{n+x}n^{n-x}(1 - \frac{x}{n})^{n-x} = n^{2n}(1 + \frac{x}{n})^{n+x}(1 - \frac{x}{n})^{n-x}$, and can now apply that approximation we just mentioned. In the first case we get $(1 + \frac{x}{n})^{n+x} \approx \exp((n+x)\frac{x}{n} - \frac{1}{2}(n+x)\frac{x^2}{n^2}=\exp(x+\frac{x^2}{n}-\frac{x^2}{2n}-\frac{x^3}{2n^2})$. Note that we can neglect that $x^3$ term by assuming that $x$ is substantially smaller than $n$, i.e. the number of successes is close to $n$--for large $n$, the probability that the number of successes deviates from $n$ is very small, so we'll concentrate only at terms near the middle of the distribution, where $x$ is small compared to $n$. We thus get $\exp(x + \frac{x^2}{2n})$. Similar calculations show that the second term is approximately $\exp(-x + \frac{x^2}{2n})$. For the third term, we can again use the approximation that $x$ is much less than $n$ to get $\sqrt{\pi (n^2 - x^2)} \approx \sqrt{\pi n^2} = n\sqrt{\pi}$. 

Substituting this back into $\frac{(2n)^{2n}\sqrt{n}}{(n+x)^{n+x}(n-x)^{n-x}\sqrt{\pi(n^2 - x^2})}$ gets us $\frac{(2n)^{2n}\sqrt{n}}{n^{2n}\exp(x + \frac {x^2}{2n})\exp(-x + \frac{x^2}{2n})n\sqrt{\pi}}$. Cancelling the leftmost terms of the numerator and denominator, combining the $\exp$ terms, and dividing the $\sqrt{n}$ in then numerator by the $n$ in the denominator gets us $\frac{2^{2n}}{\sqrt{n\pi}\exp(x^2/n)}$. What all this shows is that $\binom{2n}{n+x} \approx \frac{2^{2n}}{\sqrt{n\pi}}e^{-x^2/n}$, so the probability of any given $x$ is $\frac{e^{-x^2/n}}{\sqrt{n\pi}}$. This is exactly a Gaussian with $\mu = 0, b = \frac{1}{n}$, as desired. 
## Poisson Distribution
Since the binomial distribution approaches a Gaussian in the limit of large $n$, and the [[Discrete Distributions#Poisson Distribution|Poisson distribution]] is itself a limit of the binomial distribution in a different way, we might expect that the Poisson distribution can also approach a Gaussian in some circumstances. Let the distribution be given by $P(k) = \frac{a^ke^{-a}}{k!}$. We know that its mean and mode occur around $k=a$; as with the binomial distribution, we can rewrite it in terms of the deviation $x$ from the mode ($x = k-a$ or $k=a+x$), so $P(x) = \frac{a^{a+x}e^{-a}}{(a+x)!}$. We will then see that this reduces to a Gaussian in the limit as $a$ gets large.

Now apply Stirling's approximation to the denominator, $(a+x)! \approx (a+x)^{(a+x)}e^{-(a+x)}\sqrt{2\pi(a+x)}$. The $e^{-a}$ here cancels with the $e^{-a}$ in the numerator. We can also pull a factor of $a$ out of $(a+x)^{(a+x)}$, getting $a^{a+x}(1 + \frac{x}{a})^{a+x}$, and the first term there cancels with what's in the numerator. Thus we're left with $P(x) = 1/((1+\frac{x}{a})^{a+x}e^{-x}\sqrt{2\pi(a+x)})$. We apply the same approximation that we did in the binomial case to get $(1 + \frac{x}{a})^{a+x} \approx e^{x + \frac{x^2}{2a}}$. This partially cancels with the $e^{-x}$ term in the denominator and we get $\frac{e^{-x^2/2a}}{\sqrt{2\pi(a+x)}}$, or, neglecting large deviations from the mode and assuming that $x$ is small compared to $a$, just $\frac{e^{-x^2/2a}}{\sqrt{2\pi a}}$, which is a Gaussian with mean $0$ and variance $a$. 
# Law of Large Numbers
Informally, the law of large numbers states that, when you draw from a random variable many times, the probability that the sample mean of your trials will deviate from the actual mean converges to $0$, for any fixed amount of deviation. That is, letting $X$ be a random variable with mean $\mu$, and letting the sample mean for $n$ trials be given by $\ol{X}_n = (X_1 + \dots + X_n)/n$ where the $X_i$ are i.i.d., then for any number $\delta$, the probability that $|\ol{X}_n - \mu| > \delta$ goes to $0$ as $n$ goes to infinity. 

As a special case of this, let $X$ be a discrete random variable, and let $x$ be a possible outcome with probability $p$ of occurring. Then the law of large numbers implies that, as $n$ goes to infinity, the probability that the proportion of trials with a result of $x$ will be far from $p$ converges to $0$. (This is because the event "x occurs" can be modeled with a Bernoulli random variable $B_x$ which is $1$ with probability $p$, $0$ with probability $1-p$. If you instead want to model the situation of sampling $n$ events and looking at the proportion that give an outcome of $X$, this is given by $(B_{x, 1} + \dots + B_{x, n})/n$ where the $B_{x, i}$ are i.i.d. copies of $B_x$. Since $B_x$ has a mean of $p$, the law of large numbers says that this sample mean, and thus the proportion of outcomes which give $x$, converges to $p$.) In other words, the law of large numbers provides an (admittedly somewhat. circular) justification for the frequentist interpretation of probabilities.

To state the law more formally, let $X$ be a random variable with mean $\mu$ and standard deviation $\sigma$. Also, as above, let $\ol{X}_n = (X_1 + \dots + X_n)/n$ where the $X_i$ are i.i.d. and equal to $X$, so that the standard deviation of $\ol{X}_n$ is $\sigma_n = \frac{\sigma}{\sqrt{n}}$. Then for any $\epsilon, \delta$ there exists $N$ such that, for all $n > N$, $P(|\ol{X}_n - \mu| > \delta) < \epsilon$. Proof: choose $k$ large enough that $\frac{1}{k^2} < \epsilon$. Then $k\sigma_n = \frac{k\sigma}{\sqrt{n}}$. Thus, we can choose $N$ large enough that $k\sigma_N < \delta$. In that case, we have, for any $n > N$, $P(|\ol{X}_n -\mu| > k\sigma_n > \delta) < \frac{1}{k}^2 < \epsilon$. 
# Central Limit Theorem
The central limit theorem is in some sense a stronger version of the law of large numbers. It says that, if $X$ is any random variable with mean $\mu$, then the distribution of sample means $(X_1 + \dots + X_n)/n$ converges to a Gaussian with mean $\mu$ as $n$ goes to infinity. Similarly, the distribution of a sum $X_1 + \dots + X_n$ will converge to a Gaussian with mean $n\mu$. The result on Gaussian approximations of a binomial distribution is a special case of this when $X$ is a Bernoulli random variable: in that case the distribution of sample means is given (for any $n$) by a binomial distribution, which then converges to a Gaussian in the large-$n$ limit. 