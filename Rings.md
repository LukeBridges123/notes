# Definitions
A ring is a set $R$ with two operations, $+$, and $*$, satisfying the following properties: addition is associative, commutative, has an identity, and all elements have additive inverses (i.e. $(R, +)$ is an abelian [[Groups|group]]); multiplication is associative and satisfies the distributive property: $a(b + c) = ab + ac, (a + b)c = ac + bc$. We will also require that multiplication is commutative and has an identity, though some authors omit this. 

The most important examples of rings are the integers, the integers mod $n$, and polynomial rings. (For any ring $R$, the set of polynomials with coefficients in $R$, denoted $R[x]$, is also a ring, with addition and multiplication defined in the same way as for e.g. polynomials with real coefficients. The same goes for polynomials in multiple variables, e.g. $R[x, y]$ for polynomials in two variables.) 
## Homomorphisms and Isomorphisms
Ring homomorphisms are defined as follows: $f$ is a homomorphism if $f(a + b) = f(a) + f(b)$ and $f(ab) = f(a)f(b)$ for all $a, b \in R$. We also require that $f(1) = 1$. The fact that $f(0) = 0$ follows from the other properties, since $f(a) = f(a + 0) =  f(a) + f(0)$; subtracting $f(a)$ from both sides gives $0 = f(0)$. On the other hand, we can't repeat the same proof for multiplication, since rings don't necessarily have the cancellation property ($ab = ac$ implies $b = c$). The kernel is the set of all elements which get mapped to $0$. Isomorphisms are then defined as invertible homomorphisms. 

Like with linear maps on vector spaces, a ring homomorphism is injective if and only if $\ker(\phi) = \{0\}$; if $\ker(\phi)$ includes a nonzero element $r$ then $\phi(r) = \phi(0)$ and so $\phi$ is not injective, and if $\phi$ is not injective (say $a \neq b$ but $\phi(a) = \phi(b)$) then $\phi(a - b) = \phi(a) - \phi(b) = 0$, so $a - b \neq 0$ but $a - b \in \ker(\phi)$.
## Subrings
Subrings are defined in the same way as subgroups: sets which contain $0$, $1$, are closed under addition and multiplication, and contain inverses of all of their elements.. The image of a ring homomorphism $R \to S$ is a subring of $S$. However, subrings are less important than subgroups, in large part because the kernel of a ring homomorphism is not, in general, a subring of the codomain, which also means that we do not define quotients in terms of subrings. Rather, we use ideals.
## Ideals
A subset $I$ of a ring $R$ is called an ideal if it is closed under addition (of elements in the ideal) and under multiplication (of any ring element, not just ones already in the ideal). That is, for any $a, b \in I$, we have $a + b \in I$, and for any $a \in I, r \in R$ we have $ra \in I$. That latter property is stronger than the property of closure under multiplication that we require for subrings; on the other hand, we do not require that ideals contain $1$.

The kernel of a homomorphism is an ideal: if $a, b \in \ker(f)$, then $f(a + b) = f(a) + f(b) = 0 + 0 = 0$, so $a + b \in \ker(f)$; also, for any $r \in R$, we have $f(ra) = f(r)f(a) = f(r) \cdot 0 = 0$. 

By analogy with cyclic subgroups, we can define the ideal generated by an element $a$, denoted $(a)$, as the set of all multiples of $a$, i.e. all elements of the form $ra, r \in R$. To verify that this is an ideal, we note that $ra + sa = (r + s)a \in (a)$ and $s(ra) = (sr)a \in (a)$ for any $r, s \in R$. Ideals generated by a single element are called *principal ideals*. 

The intersection of any number of ideals is also an ideal. Suppose that $I_\alpha$ is an indexed collection of ideals, and let $x \in \cap_\alpha I_\alpha$; then for any $r \in R$ we have $rx \in I_\alpha$ for any $\alpha$, since $I_\alpha$ is closed under multiplication by any element, so $rx \in \cap_\alpha I_\alpha$. The proof for closure under addition is similar.

The union of ideals is only an ideal when the ideals form an ascending chain, $I_1 \subseteq I_2 \subseteq \dots$ 

We define the sum of ideals, $I_1 + I_2 + \dots + I_n$, to be the set of all sums $a_1 + \dots + a_n$ where $a_i \in I_i$. This is then also an ideal. We can also define an "infinite sum" of ideals, $\sum_{j=1}^\infty I_j$, even though infinite sums of ring elements are not necessarily well-defined. We just take $\sum_{j=1}^\infty I_j$ to be the set of all (arbitrarily long) finite sums of elements, $a_{i_1} + \dots + a_{i_m}$ for indices $i_1, \dots, i_m$. 
### Examples
Two special principal ideals, which are present in any ring, are the "zero ideal" $(0)$ and the "unit ideal" $(1)$. The zero ideal contains only 0, while the unit ideal is just $R$ itself, since for any $r \in R$ we have $r = r \cdot 1 \in (r)$. 

The integers have the property that the sum of any two principal ideals is another principal ideal; for example, $(2) + (3) = (1)$, and more generally $(a) + (b) = (\gcd(a, b))$. This follows from Bezout's lemma: $(a) + (b)$ is the set of all integers of the form $an + bm$, but all such integers are multiples of $\gcd(a, b)$, and any multiple of $\gcd(a, b)$ can be written in that form. We can extend this reasoning to show that all ideals in $\Z$ are principal ideals; see below. 

Not all ideals are principal. A standard example is the ideal $(x) + (2)$ in $\Z[x]$. Suppose $(x) + (2) = (p)$, for some $p \in \Z[x]$; then $2 = pq$ for some other polynomial $q$. This forces $p, q$ to both be constants, which then forces $p = 1$ or $p = 2$. The first case is impossible since, if true, we would have $1 \in (x) + (2)$, so $1 = px + 2q$ for some polynomials $p, q$; but the constant term in any linear combination $px + 2q$ is even, while $1$ is not. The second is similarly impossible, since any multiple of $2$ has an even leading coefficient; but $x \in (x) + (2)$, and it does not have an even leading coefficient. (Note, in contrast, that in $\Q[x]$ we have $(x) + (2) = (1)$. The proof above breaks down because $1 = 0 \cdot x + (1/2) \cdot 2$; to prove that $(x) + (2) = (1)$, we can then use the exact same strategy that we used for the integers, and show that the "polynomial gcd" exists and $(p) + (q) = \gcd(p, q)$.)
## Units
A *unit* is a ring is an element with a multiplicative inverse. For example, in $\Z$ the units are $1$ and $-1$, in $\Z[i]$, they are $1, -1, i, -i$, and in $\Q[i]$, they are the nonzero constant polynomials. In a field, everything except $0$ is a unit; the set of all units in a ring form a group under multiplication. (They do not form a subfield of the ring, because $0$ isn't there, and in general the set of units is not closed under addition.) 

It turns out that the ideal generated by any unit is precisely the "unit ideal", i.e. the ideal generated by $1$, which is the whole ring. That is, for any $r \in R$, $(r)$ is all of $R$ if and only if $r$ is a unit. On one hand, if $(r) = R$, then $1 \in R$, so there exists $r' \in R$ with $rr' = 1$, so $r'$ is the multiplicative inverse of $r$ and thus $r$ is a unit. On the other hand, if $r$ is a unit, then it has a multiplicative inverse $r^{-1}$, and for any $a \in R$, we have $arr^{-1} \in R$, but $arr^{-1} = a1 = a$, so any element of $R$ is in $(r)$, and so $(r) = R$. 
## Irreducible and Prime Elements
Relatedly, an *irreducible element* $r \in R$ is an element which is neither $0$ nor a unit, and for which if $r = ab$, then either $a$ or $b$ is a unit. For example, the irreducible elements in $\Z$ are primes and negations of primes; in $\Z[i]$ they include the primes which are not sums of squares (since if $n = a^2 + b^2$ then $n$ factors as $(a + bi)(a-bi)$); in $\Q[x]$ the situation is quite complicated, but e.g. $x^2 - a$ where $a$ is not a perfect square will be irreducible, and more generally for degrees $2$ and $3$ a polynomial with no rational roots is irreducible (this holds for $F[x]$ for a general field $F$: a quadratic or cubic with no roots in $F$ is irreducible); and in a field, there are no irreducible elements, since everything except $0$ is a unit. 

A slightly different notion is that of a prime element. $r$ is a prime element if a) it is neither 0 nor a unit, as above, and b) if $r$ divides $ab$, then either $r$ divides $a$ or $r$ divides $b$. In other words, a form of Euclid's lemma (if $p \mid ab$ and $p$ is prime, then $p \mid a$ or $p \mid b$) holds. 
# Constructing New Rings from Old Ones
## Product Rings
If $R, R'$ are rings then we can define a ring structure on $R + R'$, with addition and multiplication defined componentwise: $(a, b) + (c, d) = (a + c, b + d)$ and $(a, b) \cdot (c, d) = (ac, bd)$. 
## Quotient Rings
We can define quotient rings in a way similar to quotient groups; the relevant analogue of a normal subgroup is an ideal (which is after all a particular sort of subring), and we define cosets with addition rather than multiplication (since ideals absorb all multiplication; $aI = I$ for any $a$ while $a + I$ does not in general equal $I$). 

Given a ring $R$ and ideal $I$, define $R/I$ to be the set of all additive cosets of $I$ (left or right doesn't matter, as addition is commutative). As this is a subgroup of the additive group of $R$, some facts about cosets in groups carry over, e.g. that they partition $R$ (and so that "in the same coset" is an equivalence relation on $R$). Thus we can consider the elements of $R/I$ as equivalence classes under the relation: $[a] = [b]$ if and only if $a - b \in R$ (we denote the equivalence class of $a$ by $[a]$ or $\ol{a}$). We define addition on cosets $[a], [b]$ by addition of their representatives: $[a] + [b] = [a + b]$. The same works for multiplication: $[a] \cdot [b] = [ab]$. These operations are well-defined. Say we choose different representatives, $[a + r]$ and $[b + r']$ for $r, r' \in I$; then $[a + r] + [b + r'] = [(a + b) + (r + r')] = [a + b]$ (since $r + r' \in I$). Similarly $[a + r] \cdot [b + r'] = [ab + ar' + br + rr'] = [ab]$. Here we need to use the fact that ideals are closed under arbitrary multiplication--otherwise we can't guarantee that $ar'$ and $br$ are in $I$. This is then one reason why we only quotient by ideals, not subrings. 
### First Isomorphism Theorem
Whenever we quotient out by an ideal we get the "canonical map", a ring homomorphism $\pi: R \to R/I$ given by $\pi(r) = [r]$. The kernel of this is $I$: if $\pi(r) = 0$ then $r \in [0]$ , so $r - 0 \in I$, so $r \in I$, and conversely, if $r \in I$ then $\pi(r) = [0]$. Thus every ideal is the kernel of some homomorphism (analogous to the result for normal subgroups). Similarly we have an analogue of the first isomorphism theorem: given a homomorphism $\phi: R \to S$  and an ideal $I$ which is a subset of $\ker(\phi)$, there is a unique homomorphism $\overline{\phi}: R/I \to S$ with $\overline{\phi}\circ \pi = \phi$, which is an isomorphism if $\phi$ is surjective and $I = \ker(\phi)$. 

We define $\ol{\phi}([x]) = \phi(x)$. This is well-defined: if we choose a different representative $x + r$ for $r \in I$, then $\ol{\phi}([x + r]) = \phi(x + r) = \phi(x) + \phi(r) = \phi(x) + 0 = \phi(x)$ (using the fact that $r \in I \subseteq \ker(\phi)$, so $\phi(r) = 0$). It is also a homomorphism: we have, for instance, $\ol{\phi}([a] + [b]) = \ol{\phi}([a + b]) = \phi(a + b) = \phi(a) + \phi(b) = \ol{\phi}([a]) + \ol{\phi}([b])$, and the other properties of a homomorphism follow similarly. The fact that $\ol{\phi} \circ \pi = \phi$ follows by definition: we have $\ol{\phi}(\pi(a)) = \ol{\phi}([a]) = \phi(a)$. If $I = \ker(\phi)$ then $\ol{\phi}$ is injective: if $[a] \in \ker(\ol{\phi})$ then $\phi(a) = 0$, so $a \in \ker(\phi)$, so $[a] = [0]$; thus $\ker(\ol{\phi})$ is trivial. Finally, if $\phi$ is surjective then so is $\ol{\phi}$: for any $y \in S$ we have $y = \phi(x)$ for some $x \in R$, hence $\ol{\phi}([x]) = y$. Putting these together, we get that if $I = \ker(\phi)$ and $\phi$ is injective then $\ol{\phi}$ is an isomorphism. 
### Correspondence Theorem
This leads us as well to an analogue of the correspondence theorem for groups: there is a bijection between ideals of $R/I$ and ideals in $R$ that contain $I$. (Compare: given a homomorphism $G \to G'$ with kernel $K$ there is a bijection between subgroups of $G$ containing $K$ and subgroups of $G'$.) For example, ideals in $\Z/n\Z$ and the ideals in $\Z$ containing $n$...

In general, we define the bijection as follows: with $J \subseteq R/I$, associate $\pi^{-1}(J)$ (which is a subset of $R$). The preimage of an ideal under any homomorphism is an ideal, so $\pi^{-1}(J)$ is an ideal; it will contain $I$ because $\pi(I) = \overline{0} \in J$. ... This correspondence has an inverse: given an ideal $\tilde{J}$ with $I \subseteq  \tilde{J} \subseteq R$, let $p$ be the canonical homomorphism $R \to R/\tilde{J}$, let $\pi$ be the canonical homomorphism $R \to R/I$, and let $\overline{p}$ be the homomorphism $R/I \to R/\tilde{J}$ given by the first isomorphism theorem; we map $\tilde{J}$ to $J$ where $J$ is the kernel of $\overline{p}$. ...
## Fields of Fractions
We can extend a ring into a field by considering (equivalence classes of) formal fractions $a/b$, where $a, b \in R$ and $b$ is nonzero. Two fractions $a/b, c/d$ are equivalent if $ad = bc$. Addition and multiplication of fractions is defined so as to imitate addition and multiplication of rational numbers: $a/b + c/d = (ad + bc)/bd$ and $a/b \cdot c/d = ac/bd$. Note that we need to assume that $R$ is an integral domain (i.e. there are no elements $x, y$ with $xy = 0$) for this definition to make sense--otherwise the $bd$ in the denominator may be $0$. One can then check that these operations are well-defined (i.e. the result of, say, $a/b + c/d$ does not change if we replace one of the operands with an equivalent fraction) and that all of the properties of a field hold. The original ring is then embedded in the field of fractions, with a ring element $r$ corresponding to the fraction $r/1$. 
# Types of Rings
The most common and useful types of rings can be arranged in a chain, where one condition implies the other: field $\to$ Euclidean domain $\to$ principal ideal domain $\to$ unique factorization domain $\to$ integral domain $\to$ (commutative) ring. 
# Principal Ideal Domains
A principal ideal domain, or PID, is an integral domain where every ideal is principal, i.e. consists of the multiples of a single element. We have as examples $\Z$, $\Q[x]$ (single-variable polynomials with rational coefficients), and $\Z[i]$ (Gaussian integers), and any field (since the only ideals there are the zero ideal and unit ideal, both of which are principal) and as non-examples $\Q[x, y]$ and $\Z[x]$. 
## Euclidean Domains
PIDs tend to be PIDs for the same reason as $\Z$: you can do division with remainder. To formulate a general version of the Euclidean algorithm we first need to define a "Euclidean norm" on a ring $R$: a function $N$ from $R - \{0\}$ to the nonnegative integers with $N(rs) \geq N(r)$ and, more importantly, that for every $a, b \in R$ there exist $q, r \in R$ with $a = bq + r$ and $N(r) < N(b)$ or $r = 0$. The second condition lets us generalize the idea that the remainder should be smaller than the divisor. A ring equipped with a norm function is called a "Euclidean domain".

For example, to do division with remainder on $\Z$ we have $N(x) = x$, while for polynomials we have $N(p) = \deg(p)$, and in $\Z[i]$ we have $N(a + bi) = a^2 + b^2$ (square of the "norm" in the sense of linear algebra). In a field we can, if we really want, define a Euclidean norm by the constant function $N(x) = 1$, and this works since we can divide anything by anything else with remainder $0$; but we already know that a field is a PID from the fact that it has only two ideals, both principal. 

Now we have the following: any integral domain $R$ with a Euclidean norm is a PID. Proof: let $I$ be any ideal in $R$; its image under the Euclidean norm $N$ will be a subset of the nonnegative integers. This subset will have a smallest element, say $n_0$. Now let $b_0$ be any element of $I$ with $N(b_0) = n_0$, i.e. any "smallest" nonzero element of $I$. Then $I = (b_0)$. After all, if we divide any $x \in I$ by $b_0$ with remainder, we get $x = b_0q + r$; either $N(r) < N(b_0)$ or $r = 0$. Since $b_0$ and $a$ are elements of $I$, so are $b_0q$ and $a - b_0q$, so $r = a - b_0q$ is in $I$ as well. Thus we cannot have $N(r) < N(b_0)$ (since $b_0$ was chosen to have the smallest norm in $I$) so $r = 0$ instead, and so $a$ is a multiple of $b_0$. Thus every element of $I$ is a multiple of $b_0$, i.e. $I \subseteq (b_0)$, but certainly $(b_0) \subseteq I$, so in fact $I = (b_0)$, as desired. 
## Irreducible Elements in PIDs
### Irreducible element = maximal ideal
In PIDs we have a nice characterization of maximal ideals (i.e. ideals not contained in any other ideal besides the unit ideal): $(r)$ is maximal if and only if $r$ is irreducible. (Compare: $(n)$ is maximal in $\Z$ if and only if $n$ is prime.)

First, suppose $r$ is reducible; then $r = ab$ where $a, b$ are both not units. Then since $r$ is a multiple of $a$, $(r) \subseteq (a)$, but $(a)$ is not a unit, so $(a) \neq R$, and $(a) \neq (r)$: if it were the case that $(a) = (r)$, then there exists $c$ with $a = rc$, so $ab = rcb$ or $r = rcb$; we can cancel $r$ (by the cancellation law, not assuming $r$ is invertible) to get $1 = bc$, meaning $b$ is a unit, which is a contradiction. Thus $(r) \subsetneq (a) \subsetneq R$ and so $(r)$ is not maximal.

Second, suppose $r$ is irreducible, and let $I = (s)$ be any ideal containing $r$ ($I$ must be principal since we're working in a PID). Then $r = sx$ for some element $x$. But since $r$ is irreducible at least one of $s, x$ must be a unit. In the first case $(s) = (1) = R$; in the second case, we have.. $rx^{-1} = s$, so $s \in (r)$ and so $(s) \subseteq (r)$ meaning $(r) = (s)$. Thus in either case $(s)$ is not a non-unit ideal that properly contains $(r)$, so $(r)$ is maximal.
### Quotienting by an Irreducible Polynomial
If we put this together with the fact that (in any ring) an ideal $I \subseteq R$ is maximal if and only if $R/I$ is a field, we get the following construction, useful for field extensions. Given a field $F$ and an irreducible element $f(x) \in F[x]$, we have that $F[x]/f(x)$ is a field (since $(f(x))$ is maximal); there's an injective homomorphism from $F$ to $F[x]$, and a surjective homomorphism from $F[x]$ to $F[x]/f(x)$, and composing them gives a homomorphism from $F$ to $F[x]/f(x)$, so $F[x]/f(x)$ is an extension of $F$. 

For a sort of trivial example, take a field $F$, and quotient $F[x]$ by $(x)$; the result will be isomorphic to $F$, with the map $F[x] \to F$ given by "plug in 0 for $x$". Similarly quotienting by a linear polynomial $x - a$ gives you $F$ again (the "plug in $a$ for $x$" map).

Now consider $\Q[x]/(x^2 + 1)$. We know this will be a field, but what field? Note that if $f(x) = (x^2 + 1)q(x) + r(x)$ then $f(x) \equiv r(x)$ in the quotient, and we know that $r$ will be constant or linear. In fact the elements of the quotient will be expressions of the form $a + bx$ with addition given by $(a + bx) + (c + dx) = (a + c) + (b + d)x$ and $(a + bx)(c + dx) = ac + adx + bcx + bdx^2$, but since $x^2 \equiv -1$ this is really $(ac - bd) + (ad + bc)x$, and this is exactly the rules for complex numbers. So $\Q[x]/(x^2 + 1) = \Q[i]$, the complex numbers with rational "coordinates". 

We can do the same thing with finite fields, of course. If we quotient $F_2[x]$ by $(x^2 + x + 1)$ we get an extension of $F_2$ with $4$ elements, called $F_4$. To see this, repeat what we did for $\Q[i]$ above: for any $p(x) \in F_2[x]$, we have $p(x) = q(x)(x^2 + x + 1) + r(x)$ with $\deg(r(x)) < 2$, and so any polynomial is equivalent to its remainder "mod $x^2 + x + 1$"; the only possible remainders are $0, 1, x, x + 1$, which are all distinct, and this set then forms a field with $4$ elements. (N.B. we can't quotient out by, say, $x^2$ or $x^2 + 1$ since those are reducible.) This sort of construction is enough to get you all of the finite-dimensional field extensions of a given field (finite-dimensional in the sense of finite-dimensional vector space over the base field).
### Quotienting by any polynomial
The field extensions we get will be vector spaces over $F$. The following is true for any polynomial, irreducible or not: $F[x]/(p(x))$ is an $F$-vector space whose dimension is the degree of $p(x)$. Proof: letting $n =\deg(p(x))$, we claim that $\{\overline{1}, \overline{x}, \dots, \overline{x}^{n-1}\}$ is a basis. (Note: not the $\overline{x^{n-1}},$ rather $\overline{x}^{n-1}$, and the same for the other powers.). If we divide a polynomial $f(x)$ by $p$, we get $f(x) = p(x)q(x) + r(x)$ where $r$ has degree less than $n$ or is equal to $0$, and we then get that $\overline{f(x)} = \overline{r(x)}$. Thus letting $r(x) = c_0 + c_1x + \dots + c_{n-1}x^{n-1}$ we have $\overline{f(x)}  = c_0\overline{1} + c_1\overline{x} + \dots + c_{n-1}\overline{x}^{n-1}$ for any $\overline{f(x)}$ in the field extension. Thus our supposed basis spans $F[x]/(p(x))$. Now for linear independence, suppose $c_0\overline{1} + \dots + c_{n-1}\overline{x}^{n-1} = \overline{0}$; then the LHS, reinterpreted in $F[x]$, is a multiple of $p(x)$, i.e. $c_0 + \dots + c_{n-1}x^{n-1} = p(x)b(x)$. But if $b(x)$ is nonzero, then $p(x)b(x)$ has degree at least $n$, since that's the degree of $p(x)$. Thus $b(x)$ must be $0$, so all of the $c_i$ are $0$, so the $\overline{x}^i$ are linearly independent. 

So we can remember this slogan: $\Z/(n)$ has size $n$, $F[x]/(p(x))$ has dimension $n$, and the arguments you use to get there are broadly similar. 

#### Quotienting By reducible polynomials
If we quotient out by an element that isn't reducible, we generally don't get an integral domain. E.g. in $\Q[x]/(x^2 - 1)$ we have that $(\overline{x} + \overline{1})(\overline{x} - \overline{1}) = 0$, so our quotient ring has zero divisors.

### Quotienting by Polynomials = Adjoining Elements
Often when you quotient $F[x]$ by an irreducible polynomial, what you get is isomorphic to $F[a]$ where $a$ is some new element that will be a root of the polynomial over $F$ that you quotiented by. For example: $\Q[x]/(x^2 + 1)$ is isomorphic to $\Q[i]$, the set of all complex numbers of the form $a + bi$ for rational $a, b$. 

We can get a homomorphism from $\Q[x]$ to $\Q[i]$ : $\phi(f(x)) = f(i)$, i.e. the "plug in $i$ " homomorphism (which we know to be a ring homomorphism in general). The kernel of $\phi$ is some ideal, and it must be a principal ideal because $\Q[x]$ is a PID. No linear polynomial is in the kernel, and $x^2 + 1$ is, so the kernel is equal to $(x^2 + 1)$. Now we use the first isomorphism theorem: it tells us that $\Q[x]/\ker(\phi)$ is isomorphic to the image of $\phi$, so $\Q[x]/(x^2 + 1)$ is isomorphic to $\Q[i]$.

Note that if $\phi$ were the "plug in $-i$" homomorphism instead, we would get the same thing. 

Now we describe what's going on here much more generally. Let $F, \Omega$ be fields where $F$ is a subfield of $\Omega$ (think of e.g. the rational and complex numbers), and let $\alpha \in \Omega$. Now let $\phi$ be the homomorphism $F[x] \to \Omega$ given by $\phi(f(x)) = f(\alpha)$. By the first isomorphism theorem, $F[x]/\ker(\phi)$ will be isomorphic to $F[\alpha]$ (we can kind of think of this as the definition of $F[\alpha]$). $\ker(\phi)$, since $F[x]$ is a PID, will be a principal ideal, generated by some $p(x) \in F[x]$. We claim that $p(x)$ is either irreducible or the zero polynomial. For, if $p(x)$ is nonzero, then $p(x)$ is not a unit (i.e. constant polynomial), since $\ker(\phi)$ is not all of $F[x]$, since $\text{ran}(\phi)$ is not the zero ring. Now assume $p(x) = a(x)b(x)$ where $a(x), b(x)$ are not units; these must then have degree greater than $0$ and less than $\deg(\phi)$. If we plug in $\alpha$, we get $a(\alpha)b(\alpha) = p(\alpha) = 0$, so (since $\Omega$ is a domain) $a(x)$ or $b(x)$ is in $\ker(\phi)$. This is a problem: since $\ker(\phi)$ is generated by $p(x)$, all of its elements except $0$ have degree at least $\deg(p)$. Thus this must not happen, and so $p(x)$ is irreducible if it's nonzero. (N.B this $p$ is called the minimal polynomial of $\alpha$; it is the polynomial (which we can assume to be monic) of smallest degree which has $\alpha$ as a root.)

Now we have two possibilities: $p(x)$ is irreducible, and $p(x)$ is $0$. In the first case, we get that $F[\alpha]$ is isomorphic to $F[x]/(p(x))$. Since $F[x]/p(x)$ is the quotient of a PID by a maximal ideal,  it is a field, and so $F[\alpha]$ is a field as well. Thus $F[\alpha]$ is a subfield of $\Omega$ whenever $p(x)$ is irreducible. In the second case, the kernel is the zero ideal, so $\phi$ is injective, so $F[x]$ is isomorphic to a subring of the field $\Omega$. We know in general that, whenever this happens, there is a unique field homomorphism from $\text{Frac}(F[x])$ to $\Omega$. Thus $\Omega$ includes an isomorphic copy of $F(x)$, the field of rational functions over $F$. This second case happens we adjoin something that is not a root of any element in $F[x]$. For instance, $\Q[\pi]$ is isomorphic to $Q[x]$, and $\Q(\pi)$ is isomorphic to $\Q(x)$. The containments $\Q[\pi] \subseteq \Q(\pi)$ and $\Q[x] \subseteq \Q(x)$ are strict; in particular $\Q[\pi]$ must not have an inverse of $\pi$, for instance.

These two cases give rise to the following distinction: in the first case, $\alpha$ is algebraic over $F$, and in the second $\alpha$ is transcendental over $F$.
#### Types of Extensions: Finite and Infinite, Algebraic and Transcendental
Definition: let $K$ be a field extension of $F$ (so $K$ is an $F$-vector space); we call the dimension of $K$ over $F$ the degree of the $K$, $[K : F]$. (Note that if $K$ is obtained by quotienting by an irreducible polynomial over $F$, the degree of $K$ is equal to the degree of that polynomial). An extension is called finite if it is finite-dimensional over $F$, i.e. $[K : F]$ is finite. Otherwise it is an infinite extension. In a finite extension, all elements are algebraic over the base field. Equivalently, if $K$ has an element which is transcendental over $F$ then $K$ is an infinite extension.

Proof: let $[K : F] = n$, so $K$ is a finite extension; then $\{1, \alpha, \dots \alpha^n\}$ for any $\alpha \in K$ is linearly dependent over $F$. Thus there exist coefficients $c_0, \dots, c_n$ with $c_0 + c_1\alpha + \dots + c_n\alpha^n = 0$, and so $\alpha$ is a root of the polynomial $c_0 + c_1x + \dots + c_nx^n$. 

Similarly, we define an algebraic extension $K$ of a field $F$ where every element of $K$ is algebraic over $F$. A finite extension is algebraic, as seen above, but the converse is not true: if we look at $\ol{\Q}$, a.k.a the algebraic numbers, the field of all complex numbers which are algebraic over $\Q$, then it is an infinite extension. (For, if it had degree $n$, then every rational polynomial of degree more than $n$ would be reducible, which is false.)
#### Restrictions on finite fields
Our results about fields extensions as vector spaces so far hold for arbitrary fields, including finite fields. This lets us prove the following result: every finite field $F$ has $p^n$ elements for some prime $p$. Proof: We know that, if $F$ is finite, then it can't have characteristic $0$, so $F$ has characteristic $p$ for some prime $p$. If we look at just the elements $1, 1 + 1, 1 + 1 + 1, \dots$ all the way up to adding $1$ to itself $p-1$ times, these form a subfield of $F$ isomorphic to $\Z/p\Z$ (as we can see by using the first isomorphism theorem on the obvious homomorphism $\Z \to F$). Thus $F$ is an extension of $\Z/p\Z$. Since it has only finitely many elements, it must be finite-dimensional, and so isomorphic to $\F_p^n$, which has $p^n$ elements. 

#### Tower Lemma
Let $F \subseteq K \subseteq L$ be a sequence of two field extensions (specifically finite extensions); then $[L:F] = [L : K][K : F]$. 

Proof: Let $\alpha_1, \dots, \alpha_n$ be a basis for $K$ over $F$, and let $\beta_1, \dots, \beta_m$ be a basis for $L$ over $K$. We claim that the set of all products of the form $\alpha_i\beta_j$ is a basis of $L$ over $F$; once we've proved this, our claim about the indices will follow immediately (since this basis has $nm = [L : K][K : F]$ elements). 

For the fact that these span $L$, note that for any element in $l \in L$ we have $b_1\beta_1 + \dots + b_m\beta_m = l$ where all the coefficients are in $K$; then we can expand each of those $b_j \in K$ into $\beta_j = \sum_i c_{ij}\alpha_i$ where all the $c_{ij}$ are in $F$. Then we just substitute these in to get $l = \sum_j b_j\beta_j = \sum_j \sum_i c_{ij}\alpha_i\beta_j$, as desired. 

Then we just need that the $\alpha_i\beta_j$ are linearly independent over $F$. Suppose $\sum_i^n \sum_j^m c_{ij}\alpha_i\beta_j = 0$; then this reduces to $(\sum_{i = 1}^n c_{i1}\alpha_i)\beta_1 + \dots + (\sum_{i=1}^n c_{im}\alpha_i)\beta_m$ = 0. Using the linear independence of the $\beta_j$ over $K$ (which contains all of those sub-sums by which the $\beta_j$ are being multiplied), we get that each of these sums individually must be $0$. Then for any $j$ we have $\sum_{i=1}^n c_{ij}\alpha_i = 0$, and using the linear independence of the $\alpha_i$ we get that all of the $c_{ij}$ for this particular $j$ must be $0$. Thus all the coefficents are all equal to $0$ and so the $\alpha_i\beta_j$ are linearly independent. 

Now we go back to quotienting by irreducible polynomials. Letting $K$ be the field extension $F[x]/(p(x))$ of $F$, where $p(x)$ is irreducible, we have that $[K : F] = \deg(p(x))$. Suppose this is more than $1$; then $p(x)$ has no roots in $F$, but we might wonder whether it at least has roots in $K$. The answer is yes: $\ol{x}$ is a root of $p(x)$ in $K$. After all, letting $p(x) = c_0 + \dots + c_nx^n$, if we plug in $\ol{x}$ we get $p(\ol{x}) = \ol{p(x)} = \ol{0}$. So when we extend a field by quotienting, we often end up with more roots. 

We might then wonder at what point we can stop: when does $F$ contain a root for every polynomial in $F[x]$? In this case we call $F$ algebraically closed, like the complex numbers and $\ol{\Q}$. 

Some equivalent conditions: $F$ is algebraically closed if and only if every polynomial in $F[x]$ factors completely into linear factors, if and only if the only irreducible polynomials in $F[x]$ are linear, if and only if the only finite extension of $F$ is $F$ itself. 

Proof: if $F$ is algebraically closed, then for any $p(x) \in F[x]$, $p$ has a root $\alpha$, so we can factor it as $(x - \alpha)q(x)$ where $q(x) \in F[x]$. That will then also have a root, and we can continue inductively until we're left with just linear factors (and a constant in case $p$ isn't monic). Then for the second implication, if every polynomial in $F[x]$ factors completely into linear factors, then if $p$ has degree more than $1$ it won't be irreducible, as it will factor into $\deg(p(x))$ linear factors. For the third, suppose the only irreducible polynomials in $F[x]$ are linear. Then let $K$ be a finite extension of $F$ and suppose we have an element $\alpha$ in $K$ which is not in $F$. Then $F \subseteq F[\alpha] \subseteq K$; $F[\alpha]$ is isomorphic to $F[x]/(p(x))$ where $p$ is the minimal polynomial of $F$. Since $p$ is irreducible, it is linear, so $F[\alpha]$ has degree $1$ over $F$, meaning that $F[\alpha]$ is just $F$ itself. Finally, to complete the circle, suppose $F$ is not algebraically closed, so there exists $p(x) \in F[x]$ with no roots in $F$, and so no linear factors. Then we have an irreducible polynomial (not necessarily $p(x)$) of degree at least $2$, and quotienting by that gives us a field extension of degree at least $2$. Thus $F$ has a finite extension that isn't just $F$. 

We also have a related theorem, that algebraic extensions "compose": if $F \subseteq K$ and $K \subseteq L$ are algebraic extensions, then so is $F \subseteq L$. N.B. if we replaced "algebraic" with "finite" then this would follow from the Tower Lemma, but not all algebraic extensions are finite. Proof: take any $\beta \in L$; we want to show that $\beta$ is algebraic over $F$. We know by assumption that $\beta$ is algebraic over $K$, so that e.g. $p(x) = x^n + c_{n-1}x^{n-1} + \dots + c_0$, whose coefficients are all in $K$, has $\beta$ as a root. Furthermore, since $K$ is an algebraic extension of $F$, each of those coefficients is algebraic over $F$, implying that, for each $i$, $F[c_i]$ is a finite-dimensional vector space over $F$. (This way we don't have to explicitly work with the polynomials over $F$ of which the $c_i$ are roots.) Now consider the tower $F \subseteq F[c_0] \subseteq F[c_0] [c_1] \subseteq \dots \subseteq F[c_0, \dots, c_{n-1}]$. Each of of those $\subseteq$ represents a finite extension, i.e. each time we add adjoin one of the coefficients we get a finite extension of the field we got by adjoining the previous coefficients. Thus $F[c_0, \dots, c_{n-1}]$ is a finite extension of $F$, by repeated use of the tower lemma. Similarly adjoining $\beta$ to this also gives us a finite extension, since $\beta$ is algebraic over $F[c_0, \dots, c_{n-1}]$. Thus $F[c_0, \dots, c_{n-1}, \beta]$ is a finite extension of $F$, meaning $\beta$ is algebraic over $F$. Thus every element of $K$ is algebraic over $F$ and so $K$ is an algebraic extension of $F$.  

#### Fundamental Theorem of Algebra
We can rephrase this as follows: given a polynomial $f \in \C$, does there exist $z$ such that $f(z) = 0$? 

First we consider what monomials $x^n$ do to circles in the complex plane. A circle with radius $R$, consisting of all complex numbers of the form $Re^{i\theta}$, gets sent to the set of all complex numbers of the form $R^ne^{in\theta}$, which is just the circle of radius $R^n$. The $n$ in the $e^{in\theta}$ part just controls how "quickly" elements on the circle get rotated around the center. 

Now consider a general polynomial $x^n + c_{n-1}x^{n-1} + \dots + c_0$. Let $R$ be a number larger than $n$ times the coefficient $c_i$ with the largest absolute value, say, $R > 100n|c_i|$, and consider the circle centered at the origin with radius $R$. The image of this circle will be roughly a circle of radius $R^n$, with some "wobbles", whose size is controlled by $|c_{n-1}x^{n-1} + \dots + c_0|$. But this is at most $|c_{n-1}x^{n-1}| + \dots + |c_0| = |c_{n-1}||x^{n-1}| + \dots + |c_1||x| + c_0$, by the triangle inequality, and if $x$ is on the circle of radius $R$, this is equal to $|c_{n-1}|R^{n-1} + \dots + |c_0|$. This in turn is at most $|c_i|R^{n-1}  + \dots + |c_i| = n|c_i|R^{n-1}$ (recalling that $c_i$ is the largest of the coefficients) which, given how we chose $R$, is less than $\frac{1}{100}R^n$. Thus the image of the circle of radius $R$ differs from a circle of radius $R^n$ by one part in 100 or so. Now consider what happens as we continuously shrink the circle of radius $1$ down to one point in the complex plane, the constant term $c_0$. Since the polynomial is continuous, the shape of the image will vary continuously as well, and at some point it must pass through the origin. The preimage of the point in the image which passes through $0$ will then be a root. 

#### Integer Polynomials
So far we've been working with polynomials whose coefficients are in a field; these are PIDs since they are Euclidean domains, that happens because we can use polynomial long division, and that relies on being able to divide coefficients. 

In $\Z[x]$, on the other hand, consider $I = (2) + (x)$, an ideal consisting of all polynomials whose constant term is even. (N.B. this is not the same as $(2 + x)$.) If $I$ were principal, i.e. $I = (p(x))$. Then everything in $I$ has degree at least $\deg(p(x))$. But $I$ contains $2$, at least, so our generator must be a constant polynomial which must divide $2$, i.e. $1$ or $2$. But if $p(x) = 1$ then $I$ is all of $\Z[x]$ (which it is not) and if $p(x) = 2$ then $I$ consists of all the polynomials whose coefficients are all even, but that doesn't include polynomials like $x + 2$, which $I$ does. We can repeat the same argument, replacing $\Z$ with any nonzero ring $R$ that is not a field.  Just pick any nonzero, non-invertible element $r \in R$ and consider $(r) + (x)$: ....


## Finding Irreducible Polynomials
If we had a good way to figure out which polynomials are irreducible (especially over $\Q$), we could use that to get a lot of information about field extensions. In the degree $2$ or $3$ case, we can use the rational root theorem, but more generally, there are 2 main tools: Gauss' lemma and Eisenstein's criterion.

Gauss' lemma: let $f(x) \in \Z[x]$ be irreducible in $\Z[x]$,i.e. does not factor into integer polynomials; then it is irreducible over $\Q$ as well. 

Eisenstein's criterion: let $f(x) = c_0 + c_1x + \dots + c_nx^n \in \Z$[x]. If there exists some prime number $p$ such that $p$ does not divide $c_n$, $p$ does divide all the other coefficients, and $p^2$ does not divide $c_0$, then $f$ is irreducible over $\Q$. 

Proof: suppose $f(x) = a(x)b(x)$ where $a(x), b(x)$ have degree more than $0$ and less than $\deg(f)$. Now consider the map $\pi: \Z[x] \to Z[x]/(p)$ where you just reduce the coefficients mod $p$. This is a ring homomorphism, and so $\overline{f(x)} = \overline{a(x)}\cdot \overline{b(x)}$. By our assumptions about the coefficients of $f(x)$, when we reduce $f(x)$ mod $p$ we're left with $\overline{c_n}x^n$. Thus $\overline{a(x)}$ and $\overline{b(x)}$ must be monomials, else their product would not be a monomial; thus $\overline{c_n}x^n = \overline{a_k}x^k\overline{b_{n-k}}x^{n-k}$. Since reducing $a$ and $b$ mod $p$ leaves us with nonconstant monomials, reducing $a$ and $b$ mod $p$ gets rid of their constant terms, so $a_0$ and $b_0$ must be divisible by $p$, so $c_0$ is divisible by $p^2$, a contradiction. (N.B. we must have $k > 0$, otherwise $p$ would divide $c_n$.)

# Unique Factorization
A unique factorization domain, or UFD, is an integral domain if any element (besides $0$ and units) can be factored into a product of irreducible elements, which is unique up to multiplication by units. I.e. for every $r \in R$ we have $r = p_1\cdots p_n$ where the $p_i \in R$ are all irreducible; furthermore, if $r = q_1\cdots q_m$ as well, then $m = n$, and there exists a permutation of the $q_i$ such that, after permuting them, we have $p_i = q_i u_i$ for every $i$, where $u_i$ is a unit. 

This is a bit more awkward of a definition than we might like, but it is necessary to take care of some corner cases. In $\Z$ we can talk about a genuinely unique factorization into (positive) primes. In general rings we can't necessarily pick out, say, "positive" irreducible elements, but we still want a way of saying that, e.g. $2^2 \cdot 3 \cdot 5$ and $2^2 \cdot -3 \cdot -5$ are "essentially the same" factorization of $60$. Hence all the concern about units. E.g. in $\Z[i]$ we can factor $5$ as $(2+i)(2 - i)$, or $(-1 + 2i)(-1 - 2i)$, or $(1 + 2i)(-1 - 2i)$, or $(-2 + i)(-2 - i)$ . But we can get from each of those factorizations to another by multiplying one factor by a unit and the other factor by that unit's inverse. Hence $5$ can be factored uniquely, as those four factorizations are really the same. 

Some examples: the integers, of course; $\Z[i]$; $F[x]$ for any field $F$, where the units are the nonzero elements of $F$; fields are "vacuously" UFDs, since there are no elements besides $0$ and units; indeed every PID is a UFD (see below). 

The canonical non-example is $\Z[\sqrt{-5}]$. Then $6$ factors as $2 \cdot 3$ or $(1 + \sqrt{-5})(1 - \sqrt{-5})$. But these are really distinct, as we can see by considering the "norm" map $N(a + b\sqrt{-5}) = a^2 + 5b^2$. This is multiplicative: $N(zw) = N(z)N(w)$. By the same argument as before, units must have norm $1$, and the only elements that fit are $1$ and $-1$. 

## PID implies UFD
UFDs fit into a chain of classes of "nicer" rings, rings $\supseteq$ integral domains $\supseteq$ UFDs $\supseteq$ PIDs $\supseteq$ Euclidean domains $\supseteq$ fields. We've already handled PIDs $\supseteq$ Euclidean domains; now we handle UFDs $\supseteq$ PIDs. 

Proof: let $r \in R$ be a nonzero, non-unit element. If $r$ is irreducible then we're done, just take the factorization consisting of $r$ itself. If not, we have $r = ab$ where $a, b$ are not units. We want to turn this into a statement about ideals, to exploit the PID structure, and we can do so: $(r) \subseteq (a)$, and since $a$ is not a unit, $(a)$ is not the whole ring. This containment must be proper, else $a$ would be a multiple of $r$, $r$ would be a multiple of $a$, and we end up forced to conclude that $b$ is a unit. So $(r) \subset (a) \subset R$. Is this already a factorization into irreducibles? Well, $a$ is irreducible if and only if $(a)$ is maximal. Suppose $a = a_2b_2$, where $a_2, b_2$ are not both units. Then we ask whether $a_2$ is irreducible, and repeat the same process. If this process terminates, then $R$ has a factorization. If we have an infinite chain of proper containments, $(r) \subset (a) \subset (a_2) \subset \cdots$ then we're in trouble. In fact, we claim that in a PID, any ascending chain must stop (for a chain of $\subset$) or stabilize (for a chain of $\subseteq$). (In other words, every PID is Noetherian.) We now stop to prove this lemma.

### Lemma: PID implies Noetherian
Suppose $(a_1) \subseteq (a_2) \subseteq \dots$ Consider the union of all these ideals, $\bigcup_{i=1}^\infty (a_i) = I$. We can easily prove that $I$ is itself an ideal. Thus $I = (a)$ for some $a \in R$. Since $a$ is in the union, $a$ must be in one of the ideals, say $(a_n)$. Then $a \in (a_n)$, so $(a) \subseteq (a_n)$, but $a_n \in (a)$ as well, so $(a_n) \subseteq (a)$. Thus $(a) = (a_n)$, and so $a$ is in all the ideals after $(a_n)$, which means that actually $(a_{n + k}) = (a)$ for any $k$. Thus the chain ends. 

Back to the main argument: since this process terminates, $r$ must have an irreducible factor, $p_1$. So $r = p_1 s$; $s$ has an irreducible factor $p_2$, with $s = p_2t$ ; and so on: we get an ascending chain $(r) \subset (s) \subset (t) \subset \dots$ which must terminate. So the fact that our PID is Noetherian is useful twice, once to prove the existence of an irreducible factor, once to prove that factoring into irreducibles terminates.

Now we handle uniqueness. Let $r = q_1q_2\cdots q_m$; we will show that this is essentially the same as the factorization $p_1p_2 \cdots p_n$. We need a generalization of Euclid's lemma: if $p$ is an irreducible with $p \mid ab$, then $p \mid a$ or $p \mid b$. 

### Lemma: generalized Euclid's lemma
Proof: the fact that $p$ is irreducible implies that $(p)$ is maximal, and so $R/(p)$ is a field. (Intuition: fields have "no" ideals, and the only ideals in a quotient $R/I$ are ideals that contain $I$. If $I$ isn't a subset of anything except the whole ring, then quotienting kills off all ideals.) Applying the canonical homomorphism to $ab = np$ we get $\ol{a}\ol{b} = \ol{0}$. Since we're in a field, we have no zero divisors, so one of $\ol{a}$ and $\ol{b}$ must equal $\ol{0}$, i.e. $a \in (p)$ or $b \in (p)$, but that means $p \mid a$ or $p \mid b$, as desired. 

Now we return to our factorizations $r = p_1\cdots p_n = q_1 \cdots q_m$. Since $p_1$ divides $p_1 \cdots p_n$, it divides $q_1 \cdots q_m$, and so must (by repeated applications of the generalized Euclid's lemma) divide one of the $q_i$. But since the $p$s and $q$s are irreducible, $q_i$ must be a unit multiple of $p_1$. Thus we can cancel out $p_1$ and $q_i$ from the factorizations, possibly leaving behind a unit. Then we can keep doing this: get rid of a $p_i$, cancel out the corresponding $q_j$, and so on, until we're left with just units, and we can conclude that each $p_i$ is a unit multiple of some $q_i$ and vice versa, and so the two factorizations are essentially the same. 

# Noetherian Rings
We showed that PIDs have the "ascending chain condition": a chain of ideals $I_1 \subseteq I_2 \subseteq \dots$ must "stabilize" somewhere, i.e. there exists some $n$ such that, for all $m \geq n$, $I_n = I_m$. 

It turns out that a ring $R$ being Noetherian is equivalent to any ideal in $R$ being finitely generated, i.e. equal to a finite sum of principal ideals. 

Proof: for one direction, assume the Ascending Chain Condition and let $I$ be an ideal. Take an element $r_1 \in I$, so $(r_1) \subseteq I$. If $(r_1) = I$ we're done; otherwise there exists $r_2 \in I$ which is not in $(r_1)$. We can keep doing this if $(r_1) + (r_2) \neq I$ and we get an ascending chain, $(r_1) \subsetneq (r_1) + (r_2) \subsetneq (r_1) + (r_2) + (r_3) \subsetneq \dots$ which can't continue forever by the ascending chain condition. Thus we stabilize at, say, $(r_1) + \dots + (r_n) = I$, so $I$ is finitely generated.

For the other direction, suppose every ideal $I \subseteq R$ is finitely generated. ...

The class of Noetherian rings turns out to be "closed" under some natural operations. For example, if $R$ is a Noetherian ring and $I$ is an ideal, then $R/I$ is Noetherian. Proof: take a chain $J_1 \subseteq J_2 \subseteq \dots$ of ideals in $R/I$. Let $\pi$ be the canonical homomorphism $R \to R/I$. Then $\pi^{-1}(J_i)$ is an ideal in $R$, for any $i$. Furthermore, we have $\pi^{-1}(J_1) \subseteq \pi^{-1}(J_2) \subseteq \dots$. This chain stabilizes, say at $\pi^{-1}(J_n)$ , and so the original chain stabilizes at $J_n$. (Since $\pi$ is surjective, $\pi^{-1}$ is a right inverse, so since e.g. $\pi^{-1}(J_n) = \pi^{-1}(J_{n+1})$ we can get $\pi(\pi^{-1})(J_n) = \pi(\pi^{-1})(J_{n+1})$ and so $J_n = J_{n+1}$; the same goes for $J_{n+k}$.) 


