Let $T$ be a linear map $V -> V$. If, for some nonzero $v \in V$, $Tv = \lambda v$ for some scalar $\lambda$, then we say that $\lambda$ is an eigenvalue of $T$ and $v$ is a corresponding eigenvector. (The condition that $v$ is nonzero is required because $T(0) = 0$ = $\lambda$ 0 for any $T, \lambda$, meaning that if thhe condition were removed, every scalar would be an eigenvalue of every map.) Except in finite vector spaces, each eigenvalue has infinitely many corresponding eigenvalues, since if $Tv = \lambda v$ then $Tav = aTv = a\lambda v = \lambda av$ for any scalar $a$, meaning $av$ is also an eigenvector of $T$ with eigenvalue $\lambda$. 

## Conditions for Being an Eigenvalue
$\lambda$ being an eigenvalue of $T$ is equivalent to the map $T - \lambda I$ not being injective (and, for that matter, to $T - \lambda I$ not being surjective, or not being bijective). To prove the first equivalence, suppose that $\lambda$ is an eigenvalue; then $(T - \lambda I)v = Tv - (\lambda I)v = \lambda v - \lambda v = 0$, so $T - \lambda I$ sends a nonzero vector to 0, making it not injective. Alternatively, suppose $T - \lambda I$ is not injective; then some nonzero vector $v$ gets sent to 0 by it, so $(T - \lambda  I)v = 0$ or $Tv - \lambda Iv = 0$ or $Tv = \lambda Iv = \lambda v$, making $\lambda$ an eigenvalue. (In this criterion, note the special case where $T$ itself is not injective; then it has 0 as an eigenvalue since $T - 0I = T$ is not injective, and any vector in null($T$) is a corresponding eigenvector.)

As for the rest of the equivalences, they follow from the equivalence of injectivity and surjectivity for linear maps between spaces of the same dimension.

Also, recall that a map $T$ is not injective iff det $A$  = 0, where $A$ is the matrix of $T$ with respect to an arbitrary basis. Thus $\lambda$ is an eigenvalue of $T$ iff det $(A - \lambda I)$ = 0. (Because the determinant is invariant with respect to change of basis, it makes sense to use it for things like eigenvalues which are really properties of an operator that do not depend on choice of basis.)

## Existence of Eigenvalues for Maps in Complex Vector Spaces 
Again letting T be a linear operator with A as its matrix for a certain basis, and considering $\lambda$ as a variable, the expression $\det(A - \lambda I$) expands out to a polynomial (the "characteristic polynomial"); its roots are precisely the values of $\lambda$ for which T is not (in/sur/bi)jective, i.e. the eigenvalues of T. When T is an operator on a complex vector space, the characteristic polynomial can be considered as a polynomial over $\mathbb{C}$, and so it has a root, which is an eigenvalue of T.

### Digression: Polynomials of an Operator
An alternative proof of the above which does not rely on matrices and determinants instead uses the idea of a "polynomial of an operator". Letting T be an operator, we can denote by $T^n$ "T composed with itself n times". ($T^0$ denotes the identity function and $T^{-n}$ denotes the inverse of $T$, if it exists, composed with itself n times, but these notations will not be immediately relevant.) Recalling the definition for addition and scalar multiplication of linear maps, we can then define a "polynomial of an operator": a linear operator given by $a_0T^0 + a_1T + \dots + a_nT^n$ for scalars $a_0, ... a_n$. 

## Some Properties of Eigenvalues and Eigenvectors
### Eigenvectors of Distinct Eigenvalues are Linearly Independent
Let $\lambda_1, \dots, \lambda_n$ be distinct eigenvalues of a map $T$, with corresponding eigenvector $v_1, \dots, v_n$. Then $v_1, \dots, v_n$ are all linearly independent. Proof: we use induction, with the base case $n = 1$ being trivial since an eigenvector is never the 0 vector. Now suppose there exist scalars $a_i$, not all $0$, with $v_n = a_1v_1 + \dots + a_{n-1}v_{n-1}$. Applying $T$ to both sides we get $\lambda_nv_n = \lambda_1a_1v_1 + \dots + \lambda_{n-1}a_{n-1}v_{n-1}$, or, subtracting $\lambda_n v_n$  from both sides, we get $$0 = \lambda_1a_1v_1 + \dots + \lambda_{n-1}a_{n-1}v_{n-1} - (\lambda_nv_1 + \dots + \lambda_nv_{n-1}) = (\lambda_1 - \lambda_n)a_1v_1 + \dots + (\lambda_{n-1} - \lambda_n)a_{n-1}v_{n-1}$$ By the inductive hypothesis we know that $v_1, \dots, v_{n-1}$ are linearly independent; thus the scalars in this linear combination are all $0$. But by our initial assumption that $v_1, \dots, v_n$ were linearly independent, one of the $a_i$ must be $0$, in which case, for $(\lambda_i - \lambda_n)a_i$ to be $0$, we must have $\lambda_i = \lambda_n$, contradicting the assumption that all the eigenvalues were distinct. Thus, given that all the eigenvalues are distinct, $v_1, \dots, v_n$ must be linearly independent. 



