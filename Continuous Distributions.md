# Uniform Distribution
The simplest continuous distribution is the uniform distribution on $[a, b]$, which is equally likely to be any number in that interval. Its pdf is $\frac{1}{b-a}$. Its expected value is $\int_a^b \frac{x}{b-a}dx = \frac{b^2}{2(b - a)} - \frac{a^2}{2(b-a)} = \frac{b^2 - a^2}{2(b-a)} = \frac{b + a}{2}$. As for its variance, we have $E(X^2) = \int_a^b \frac{x^2}{b-a} = \frac{b^3 - a^3}{3(b-a)} = \frac{b^2 + ab + a^2}{3}$, and $E(X)^2 = \frac{a^2 + 2ab + b^2}{4}$, so $\var(X) = E(X^2) - (E(X))^2 = \frac{4a^2 + 4ab + 4a^2 - 3a^2 - 6ab - 3b^2}{12} = \frac{a^2 -2ab + b^2}{12} = \frac{(b-a)^2}{12}$. Thus the standard deviation of the uniform distribution is $\frac{b-a}{2\sqrt{3}}$. 

# Exponential Distribution
Suppose that events of some kind happen randomly at a uniform rate, in the sense that the probability of an event happening in some small time interval $[t_1, t_1 + \epsilon]$ is the same as the probability of an event happening in any other time interval of the same size, $[t_2, t_2 + \epsilon]$, regardless of when any past events have occurred. The exponential distribution, with pdf $Ae^{-bt}$ where $A, b$ are positive constants, gives the probability that at least one event will happen in the next $t$ seconds. 
## Derivation of the Exponential Distribution
### Discrete Approximation
First we consider a discretized version of this situation. Divide time into intervals of a given length $\epsilon$, and suppose that at a) at most one event can happen in each interval, and b) the probability that an event will happen in a given interval is some constant $p$, independent of what has happened in previous intervals. Then the probability that we will have to wait exactly $k$ intervals ($k\epsilon$ seconds) for an event to occur, in the sense that we go $k-1$ intervals without an event and get an event on the $k$th interval, is given by the [[Discrete Distributions#Geometric Distribution|geometric distribution]], $(1 - p)^{k-1}p$. The average number of intervals we'll have to wait is $\frac{1}{p}$, so the average amount of time we'll have to wait is $\frac{\epsilon}{p}$. Alternatively, if we know that the average waiting time is $t$, the probability that it will happen in any given length-$\epsilon$ interval is $\frac{\epsilon}{t}$. 
### Continuous Limit
Now we pass to the continuous case. Typically, when we encounter situations where the exponential distribution can be used, we will have some "rate" $\lambda$ of events, defined so that the average number of events in an interval of length $t$ is $\lambda t$; this implies that the average time between events, say $\tau$, is, for any $t$, $\frac{t}{\lambda t} = \frac{1}{\lambda}$ (the total time divided by the average number of events that occur in it). Note also that, if we take a sufficiently small time interval $\epsilon$, the probability that more than two events happen in the same interval is negligible, and the probability that an event happens in the interval is approximately $\lambda \epsilon$. 

Thus, we can approximate the continuous case by a geometric distribution. Say that the current time is $0$, and we want to know what the probability is that the next event will occur on time $t$. More precisely (since we'll eventually be passing to a continuous distribution, where we can only talk about the probabilities of intervals) we want to know the probability that the next event will occur between $t$ and $t + \Delta t$ for some (ideally small) time step $\Delta t$. Divide the interval $[0, t]$ into intervals of length $\epsilon$, so that it consists of $n = \frac{t}{\epsilon}$ intervals. By the arguments from above, the probability that no event happens between $0$ and $t$ is $(1 - p)^n = (1 - \lambda \epsilon)^{t/\epsilon}$. The probability that an event will occur between $t$ and $t + \Delta t$ is approximately $\lambda \cdot \Delta t$. Thus the probability that it will take $t$ seconds before the next event occurs is approximately $(1 - \lambda \epsilon)^{t/\epsilon}(\lambda \Delta t)$. 

Now we look first at the $\epsilon \to 0$ limit. The term $(1 - \lambda \epsilon)^{t/\epsilon}$ can be rewritten as $(1 - \frac{\lambda t}{n})^n = (1 + (-\frac{\lambda t}{n}))^n$ where $n = \frac{t}{\epsilon}$. Then, using $\lim_{n \to 0} (1 + \frac{x}{n})^n = e^x$, this becomes $e^{-\lambda t}$ in the limit. Thus the probability that the next event will happen between $t$ and $\Delta t$ is $e^{-\lambda t}(\lambda \Delta t)$. The probability density is the probability that the next event happens between $t$ and $t + \Delta t$, divided by the interval length $\Delta t$, so we get a pdf of $\lambda e^{-\lambda t}$, and the probability that the next event will happen between times $a$ and $b$ is $\int_a^b \lambda e^{-\lambda t}dt$. Equivalently, we can write the pdf in terms of the average waiting time $\tau = \frac{1}{\lambda}$, in which case we get  $e^{-t/\tau}/\tau$. 
## Properties
First we show that this is a well-defined probability distribution, in that the total probability $\int_0^\infty \lambda e^{-\lambda t}dt$ equals $1$. We have $\int_0^\infty \lambda e^{-\lambda t}dt = \lim_{x \to \infty}\frac{\lambda}{-\lambda}e^{-\lambda x} - \frac{\lambda}{-\lambda}e^{-\lambda 0} = -\frac{\lambda}{-\lambda}e^0 = 1$. 

Notice that this relies on having $b = A$ in $Ae^{-bt}$--arbitrary values of $A, b$ wouldn't actually give us a probability distribution. 
### Expected Value
From the discussion of the meaning of $\lambda$ and $\tau$ above, we would expect that, if $X$ is an exponential distribution with rate $\lambda$, then $E(X) = \tau$. We have $E(X) = \int_0^\infty t\lambda e^{-\lambda t}dt$. We can integrate by parts, setting $u = t, dv = \lambda e^{-\lambda}$ to get $\int t \lambda e^{-\lambda t} dt = -te^{\lambda t} - \int (-e^{-\lambda t})dt = -te^{\lambda t} - \frac{1}{\lambda}e^{-\lambda t}$. In the limit $t \to \infty$ both terms go to $0$, while at $t = 0$ the first term is $0$ and the second is $\frac{-1}{\lambda}$. Thus we have $E(X) = \int_0^\infty t\lambda e^{-\lambda t}dt =  (\int t\lambda e^{-\lambda t}dt)|_{t=0}^{t=\infty} = 0 - (\frac{-1}{\lambda}) = \frac{1}{\lambda} = \tau$. 
### Median
Recall that the median of a continuous distribution is the number $a$ such that $P(X \leq a) = P(X \geq a)$. In this case, this means $\int_0^a \lambda e^{-\lambda t}dt = \int_a^\infty \lambda e^{-\lambda t}dt$. The first integral equals $-e^{-\lambda a} + 1$, while the second equals $e^{-\lambda a}$. Thus we need $-e^{-\lambda a} + 1 = e^{-\lambda a}$, or $2e^{-\lambda a} = 1$, or $a = \frac{-1}{\lambda}\ln(\frac{1}{2}) = \frac{\ln(2)}{\lambda} = \tau \ln(2)$. As we would expect, increasing the rate (or decreasing the average waiting time) decreases the median. 

### Variance 
Calculating the variance of an exponential distribution proceeds very similarly to calculating the expected value. As usual $\var(X) = E(X^2) - E(X)^2$; we have $E(X)^2 = \frac{1}{\lambda^2}$ and $E(X^2) = \int_0^\infty t^2 \lambda e^{-\lambda t}dt$. Once again, we can set $u = t^2, v = \lambda e^{-\lambda t}$ and integrate by parts, to get $\int t^2 \lambda e^{-\lambda t}dt = -t^2 e^{-\lambda t} + 2\int t e^{-\lambda t}dt$. Note however that the second integral (taken from $0$ to $\infty$) is $\frac{1}{\lambda}E(X)$, since $E(X) = \int_0^\infty t\lambda e^{-\lambda t}dt$. Thus we get $-t^2 e^{- \lambda t}|_0^\infty + \frac{2}{\lambda^2}$, and the first term is $0$, so we're left with $E(X^2) = \frac{2}{\lambda^2}$. Thus $\var(X) = E(X^2) - E(X)^2 = \frac{2}{\lambda^2} - \frac{1}{\lambda^2} = \frac{1}{\lambda^2}$. 

# Gaussian Distribution
