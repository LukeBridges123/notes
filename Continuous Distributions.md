# Uniform Distribution
The simplest continuous distribution is the uniform distribution on $[a, b]$, which is equally likely to be any number in that interval. Its pdf is $\frac{1}{b-a}$. Its expected value is $\int_a^b \frac{x}{b-a}dx = \frac{b^2}{2(b - a)} - \frac{a^2}{2(b-a)} = \frac{b^2 - a^2}{2(b-a)} = \frac{b + a}{2}$. As for its variance, we have $E(X^2) = \int_a^b \frac{x^2}{b-a} = \frac{b^3 - a^3}{3(b-a)} = \frac{b^2 + ab + a^2}{3}$, and $E(X)^2 = \frac{a^2 + 2ab + b^2}{4}$, so $\var(X) = E(X^2) - (E(X))^2 = \frac{4a^2 + 4ab + 4a^2 - 3a^2 - 6ab - 3b^2}{12} = \frac{a^2 -2ab + b^2}{12} = \frac{(b-a)^2}{12}$. Thus the standard deviation of the uniform distribution is $\frac{b-a}{2\sqrt{3}}$. 

# Exponential Distribution
Suppose that events of some kind happen randomly at a uniform rate, in the sense that the probability of an event happening in some small time interval $[t_1, t_1 + \epsilon]$ is the same as the probability of an event happening in any other time interval of the same size, $[t_2, t_2 + \epsilon]$, regardless of when any past events have occurred. The exponential distribution, with pdf $Ae^{-bt}$ where $A, b$ are positive constants, gives the probability that at least one event will happen in the next $t$ seconds. 
## Derivation of the Exponential Distribution
### Discrete Approximation
First we consider a discretized version of this situation. Divide time into intervals of a given length $\epsilon$, and suppose that at a) at most one event can happen in each interval, and b) the probability that an event will happen in a given interval is some constant $p$, independent of what has happened in previous intervals. Then the probability that we will have to wait exactly $k$ intervals ($k\epsilon$ seconds) for an event to occur, in the sense that we go $k-1$ intervals without an event and get an event on the $k$th interval, is given by the [[Discrete Distributions#Geometric Distribution|geometric distribution]], $(1 - p)^{k-1}p$. The average number of intervals we'll have to wait is $\frac{1}{p}$, so the average amount of time we'll have to wait is $\frac{\epsilon}{p}$. Alternatively, if we know that the average waiting time is $t$, the probability that it will happen in any given length-$\epsilon$ interval is $\frac{\epsilon}{t}$. 
### Continuous Limit
Now we pass to the continuous case. Typically, when we encounter situations where the exponential distribution can be used, we will have some "rate" $\lambda$ of events, defined so that the average number of events in an interval of length $t$ is $\lambda t$; this implies that the average time between events, say $\tau$, is, for any $t$, $\frac{t}{\lambda t} = \frac{1}{\lambda}$ (the total time divided by the average number of events that occur in it). Note also that, if we take a sufficiently small time interval $\epsilon$, the probability that more than two events happen in the same interval is negligible, and the probability that an event happens in the interval is approximately $\lambda \epsilon$. 

Thus, we can approximate the continuous case by a geometric distribution. Say that the current time is $0$, and we want to know what the probability is that the next event will occur on time $t$. More precisely (since we'll eventually be passing to a continuous distribution, where we can only talk about the probabilities of intervals) we want to know the probability that the next event will occur between $t$ and $t + \Delta t$ for some (ideally small) time step $\Delta t$. Divide the interval $[0, t]$ into intervals of length $\epsilon$, so that it consists of $n = \frac{t}{\epsilon}$ intervals. By the arguments from above, the probability that no event happens between $0$ and $t$ is $(1 - p)^n = (1 - \lambda \epsilon)^{t/\epsilon}$. The probability that an event will occur between $t$ and $t + \Delta t$ is approximately $\lambda \cdot \Delta t$. Thus the probability that it will take $t$ seconds before the next event occurs is approximately $(1 - \lambda \epsilon)^{t/\epsilon}(\lambda \Delta t)$. 

Now we look first at the $\epsilon \to 0$ limit. The term $(1 - \lambda \epsilon)^{t/\epsilon}$ can be rewritten as $(1 - \frac{\lambda t}{n})^n = (1 + (-\frac{\lambda t}{n}))^n$ where $n = \frac{t}{\epsilon}$. Then, using $\lim_{n \to 0} (1 + \frac{x}{n})^n = e^x$, this becomes $e^{-\lambda t}$ in the limit. Thus the probability that the next event will happen between $t$ and $\Delta t$ is $e^{-\lambda t}(\lambda \Delta t)$. The probability density is the probability that the next event happens between $t$ and $t + \Delta t$, divided by the interval length $\Delta t$, so we get a pdf of $\lambda e^{-\lambda t}$, and the probability that the next event will happen between times $a$ and $b$ is $\int_a^b \lambda e^{-\lambda t}dt$. Equivalently, we can write the pdf in terms of the average waiting time $\tau = \frac{1}{\lambda}$, in which case we get  $e^{-t/\tau}/\tau$. 
## Properties
First we show that this is a well-defined probability distribution, in that the total probability $\int_0^\infty \lambda e^{-\lambda t}dt$ equals $1$. We have $\int_0^\infty \lambda e^{-\lambda t}dt = \lim_{x \to \infty}\frac{\lambda}{-\lambda}e^{-\lambda x} - \frac{\lambda}{-\lambda}e^{-\lambda 0} = -\frac{\lambda}{-\lambda}e^0 = 1$. 

Notice that this relies on having $b = A$ in $Ae^{-bt}$--arbitrary values of $A, b$ wouldn't actually give us a probability distribution. 
### Expected Value
From the discussion of the meaning of $\lambda$ and $\tau$ above, we would expect that, if $X$ is an exponential distribution with rate $\lambda$, then $E(X) = \tau$. We have $E(X) = \int_0^\infty t\lambda e^{-\lambda t}dt$. We can integrate by parts, setting $u = t, dv = \lambda e^{-\lambda}$ to get $\int t \lambda e^{-\lambda t} dt = -te^{\lambda t} - \int (-e^{-\lambda t})dt = -te^{\lambda t} - \frac{1}{\lambda}e^{-\lambda t}$. In the limit $t \to \infty$ both terms go to $0$, while at $t = 0$ the first term is $0$ and the second is $\frac{-1}{\lambda}$. Thus we have $E(X) = \int_0^\infty t\lambda e^{-\lambda t}dt =  (\int t\lambda e^{-\lambda t}dt)|_{t=0}^{t=\infty} = 0 - (\frac{-1}{\lambda}) = \frac{1}{\lambda} = \tau$. 
### Median
Recall that the median of a continuous distribution is the number $a$ such that $P(X \leq a) = P(X \geq a)$. In this case, this means $\int_0^a \lambda e^{-\lambda t}dt = \int_a^\infty \lambda e^{-\lambda t}dt$. The first integral equals $-e^{-\lambda a} + 1$, while the second equals $e^{-\lambda a}$. Thus we need $-e^{-\lambda a} + 1 = e^{-\lambda a}$, or $2e^{-\lambda a} = 1$, or $a = \frac{-1}{\lambda}\ln(\frac{1}{2}) = \frac{\ln(2)}{\lambda} = \tau \ln(2)$. As we would expect, increasing the rate (or decreasing the average waiting time) decreases the median. 

### Variance 
Calculating the variance of an exponential distribution proceeds very similarly to calculating the expected value. As usual $\var(X) = E(X^2) - E(X)^2$; we have $E(X)^2 = \frac{1}{\lambda^2}$ and $E(X^2) = \int_0^\infty t^2 \lambda e^{-\lambda t}dt$. Once again, we can set $u = t^2, v = \lambda e^{-\lambda t}$ and integrate by parts, to get $\int t^2 \lambda e^{-\lambda t}dt = -t^2 e^{-\lambda t} + 2\int t e^{-\lambda t}dt$. Note however that the second integral (taken from $0$ to $\infty$) is $\frac{1}{\lambda}E(X)$, since $E(X) = \int_0^\infty t\lambda e^{-\lambda t}dt$. Thus we get $-t^2 e^{- \lambda t}|_0^\infty + \frac{2}{\lambda^2}$, and the first term is $0$, so we're left with $E(X^2) = \frac{2}{\lambda^2}$. Thus $\var(X) = E(X^2) - E(X)^2 = \frac{2}{\lambda^2} - \frac{1}{\lambda^2} = \frac{1}{\lambda^2}$. 

# Gaussian Distribution
The Gaussian or normal distribution has as its pdf $\rho(t) = \sqrt{\frac{1}{2\pi\sigma^2}}e^{-(t - \mu)^2/2\sigma^2}$, sometimes instead written as $\sqrt{\frac{b}{\pi}}e^{-b(t-\mu)^2}$ where $b = \frac{1}{2\sigma^2}$. 
## Evaluating the Gaussian Integral

## Mean, Median, and Mode
A Gaussian with parameters $b, \mu$ has its mean, median, and mode all at $\mu$. 

To see why the mode is at $\mu$, note that the exponent in the pdf is always negative or $0$ (since $(t-\mu)^2$ is always positive), equalling $0$ only when $t = \mu$. Thus the pdf has an absolute maximum at $t = \mu$, and monotonically decreases as you get further from $\mu$. The "height" of the distribution, i.e. the value of the pdf at its maximum, is $\sqrt{\frac{b}{\pi}}$. 

The Gaussian is also symmetric about $\mu$: if we write $t = \mu + h$, we get $\rho(t) = \sqrt{\frac{b}{\pi}}e^{-b(h)^2}$, while if we write $t = \mu - h$, we get $\rho(t) = \sqrt{\frac{b}{\pi}}e^{-b(-h)^2} =  \sqrt{\frac{b}{\pi}}e^{-b(h)^2}$. Thus $\rho(\mu + t) = \rho(\mu - t)$, and so the value of the pdf depends only on the distance from $\mu$. This first implies that $\int_{-\infty}^\mu \rho(t)dt = \int_\mu^\infty \rho(t)dt$, so $\mu$ is the median. It also implies that $\mu$ is the mean. Consider first the case where $\mu = 0$, so $\rho(t) = \sqrt{\frac{b}{\pi}}e^{-bt^2}$. Then $\rho(-t) = \rho(t)$, and we have $E(X) = \int_{-\infty}^\infty t\rho(t)dt = \int_{-\infty}^0 t\rho(t)dt + \int_0^\infty t\rho(t)dt$. Substituting $-t$ for $t$ in the first term and using symmetry gets $\int_0^\infty (-t)\rho(-t)dt + \int_0^\infty t\rho(t)dt = -\int_0^\infty t\rho(t)dt + \int_0^\infty t\rho(t)dt = 0$. Thus when $\mu = 0$ the mean is $0$. If we then make the substitution $t = x - \mu$ we get $\int_{-\infty}^\infty (x - \mu)\rho(x - \mu)dx$. This should be $0$ as before--a translation doesn't change the value of an integral. Thus $0 = \int_{-\infty}^\infty (x - \mu)\rho(x - \mu)dx = \int_{-\infty}^\infty x\rho(x - \mu)dx - \mu\int_{-\infty}^\infty \rho(x - \mu)dx$. The first term is $E(X)$ when $X$ is the Gaussian with parameter $\mu$, and the second is $-\mu$ multiplied by the total probability $\int_{-\infty}^\infty \rho(x - \mu)$, which is $1$. Thus $0 = E(X) - \mu$ and $E(X) = \mu$. 

The same reasoning extends to any probability distribution which is symmetric about some point $c$--it must have a mean of $c$. 

## Variance and Standard Deviation
A Gaussian with parameters $\mu, \sigma$ has variance $\sigma^2$ (or $\frac{1}{2b}$).

To show this, we first translate the distribution so $\mu = 0$. Then $\var(X) = E(X^2) = \int_{-\infty}^\infty x^2 \rho(x)dx$. To find $\int x^2 \rho(x)dx$ we integrate by parts, with $u = x, dv = x\rho(x)$. Then $v = -\sigma^2 \rho(x)$ (since $\rho'(x) = \rho(x) \cdot -\frac{x}{\sigma^2})$ and we get $\int x^2 \rho(x)dx = -\sigma^2 x\rho(x) + \int \sigma^2 \rho(x)dx$. That last term, taken to be the integral from $-\infty$ to $\infty$ of $\sigma^2 \rho(x)$, is $\sigma^2$, since $\int_{-\infty}^\infty \rho(x)dx = 1$. As for the first term, taking limits as $x \to -\infty$ and $x \to \infty$ gets us $0$ in both cases, so it goes to $0$. Thus $\var(X) = \int_{-\infty}^\infty x^2 \rho(x)dx = \sigma^2$, and by extension the standard deviation of $X$ is $\sigma$. 

It is true for any Gaussian distribution that $P(-\sigma \leq X \leq \sigma) \approx 0.68$, $P(-2\sigma \leq X \leq 2\sigma) \approx 0.95$, and $P(-3\sigma \leq X \leq 3\sigma) \approx 0.997$. 

