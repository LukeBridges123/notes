# Nilpotent Operators and the Failure of Diagonalization
Every complex matrix (or more generally, matrix over an algebraically closed field) can be put into upper-triangular form. We might hope that in fact every matrix can be diagonalized, but we run into some easy counterexamples, for instance the matrix $A$ with $A(1, 0) = (0, 1), A(0, 1) = (0, 0)$. All of $A$'s eigenvalues are $0$, so if it's diagonalizable, it's the zero matrix, but clearly it isn't. More generally, "nilpotent" matrices (matrices with $A^n = 0$) have $0$ for all their eigenvalues: if $v$ is an eigenvector of $A$ with eigenvalue $\lambda$, then $A^n(v) = \lambda^nv$, but $A^n(v) = 0$ since $A$ is nilpotent, so we must have $\lambda = 0$. As a consequence of this, the only diagonalizable nilpotent matrix is the zero matrix. After all, if $A$ is a diagonalizable nilpotent matrix, then there is some change of basis that turns it into the zero matrix, but then it must just be the zero matrix in any basis. 

Put another way, it isn't the case that, given an operator $T$, you can write the underlying vector space $V$ as a sum of eigenspaces of $T$. Instead we need to work with so-called "generalized eigenspaces", and doing this leads us to the Jordan form of a matrix. 

# Generalized Eigenvectors
A generalized eigenvector $v$ of an operator $T$ is a nonzero vector with $(T - \lambda I)^kv = 0$ for some integer $k$. When $k = 1$ this reduces to the ordinary definition of an eigenvector. The least such $k$ is called the "exponent" of $v$. In fact, any scalar with this property will also be an eigenvalue: letting $k$ be the exponent of $v$, $(T - \lambda I)^{k-1}v$ will be an eigenvector with eigenvalue $\lambda$. Thus there's no need to speak of "generalized eigenvalues" that may not themselves be eigenvalues, although there can be "generalized eigenvectors" that aren't themselves eigenvectors. 

With these, we have a corresponding notion of a generalized eigenspace: for a given scalar $\lambda$, the set of all vectors with $(T - \lambda I)^kv = 0$ for some $k$. Indeed, letting $x$ be a generalized eigenvector with eigenvalue $\lambda$ and exponent $d$, the set $\{u_i = (T - \lambda I)^iv\}$, with $i$ ranging from $0$ to $d-1$, will be linearly independent, and its span will be an invariant subspace under $T$. First note that any linear combination of the $u_i$ (besides the trivial one) is a generalized eigenvector of $T$. If we take $u = c_1u_1 + \dots + c_{d-1}u_{d-1}$, then $(T-\lambda I)^d$ annihilates all the vectors in the combination, and so we get $(T - \lambda I)^d(u) = 0$. (In fact, letting $i$ be the first index with $c_i \neq 0$, then $(T - \lambda I)^{d - i}(u) = 0$, so the exponent of $u$ will in general be less than $d$. More precisely it'll be $d - i$, since $d-i-1$ and anything less than that that won't annihilate $c_iu_i$.) This proves first that any nontrivial linear combination of the $u_i$ is a generalized eigenvector with the same eigenvalue as $v$, and second that all the $u_i$ are linearly independent, since if not all the $c_i$ are zero then we have a generalized eigenvector which must not be $0$. 


Note that, in this subspace, we have $(T - \lambda I)u_j = u_{j+1}$, and so $Tu_j = \lambda u_j + u_{j+1}$. In general we have $Tu_j = \lambda u_j + u_{j+1}$ if $j < d-1$, $\lambda u_j$ if $j = d-1$, and $0$ otherwise. If we write down the matrix of $T$ restricted to this subspace in this bases, we end up with $\lambda$s along the diagonal and $1$s directly below the diagonal, i.e. $\lambda I$ plus the (nilpotent) operator whose matrix in this basis has 0s everywhere except for 1s below the diagonal. Thus, associated with each generalized eigenvector, we have an invariant subspace (the generalized eigenspace) where the matrix can be "almost diagonalized". We call this "almost diagonalized" matrix, for an exponent $d$ generalized eigenvector of eigenvalue $\lambda$, the "$d \times d$ Jordan block with eigenvalue $\lambda$"; a matrix is in Jordan form if it is built out of Jordan blocks. 

For instance, a 2x2 matrix in Jordan form could either be diagonal or be a 2x2 Jordan block, i.e. the same number on both diagonal entries, a 1 in the lower-left entry, and a 0 in the upper-right. For 3x3 you can either have a diagonal matrix, a 2x2 Jordan block and then the other diagonal entry filled in, or a 3x3 Jordan block. Note that different Jordan blocks can have the same eigenvalues, e.g. $$\begin{pmatrix}\lambda_1 & 0 & 0 \\ 1 & \lambda_1 & 0 \\ 0 & 0 & \lambda_1 \end{pmatrix}$$ is a valid Jordan form, constructed out of 2x2 and 1x1 Jordan blocks. 

# Jordan Form
The statement: let $T: V \to V$ be an operator on a finite-dimensional vector space over an algebraically closed field; then there is some basis such that the matrix of $T$ has Jordan form. Equivalently, any $n \times n$ matrix over an algebraically closed field can be put into Jordan form with a change of basis. 

We can prove this by showing that $V$ can be decomposed into a direct sum of generalized eigenspaces; the matrix of $T$ restricted to each of those will be a Jordan block, per our analysis above, and so the matrix of the whole thing with respect to the basis found by gluing together the relevant bases for the generalized eigenspaces will have Jordan form. 

We proceed by induction on the dimension of $V$. For dimension $1$, the matrix of a transformation with respect to any basis will just be a $1 \times 1$ Jordan block and so will be in Jordan form. Now for the induction step, supposing that matrices of dimension less than $n$ all have Jordan form, if we can just prove that $V$ splits into a direct sum of proper $T-$invariant subspaces, we'll know that each of those have Jordan form, and gluing them together gives us a Jordan form of the whole matrix. In fact, if we can just find one proper subspace, then we're good. 

Outline of the proof: we'll first reduce to the case of a zero eigenvalue, which will then reduce to the case of a nilpotent operator. We can prove Jordan form for a nilpotent operator, and this will imply it for general operators (by the chain of reductions we do). 

## Case of a Zero Eigenvalue
First note that Jordan form is true for an operator $T$, iff it's true for $T - \lambda I$ for any scalar $\lambda$. After all, $\lambda I$ has the same matrix in any basis; if we can find a basis where $T$ is in Jordan form, then $T - \lambda I$ in that basis will also be in Jordan form (the specific values on the diagonal may change, but not the general form); on the other hand if $T - \lambda I$ can be put in Jordan form, then $(T - \lambda I) + \lambda I = T$ will be in Jordan form in the same basis. So if $T$ has a zero eigenvalue we're done; otherwise, it must have some eigenvalue $\lambda$, and then $T - \lambda I$ will have an eigenvalue of $0$. 
## Case of a Nilpotent Operator
Now we can assume WLOG that $\lambda = 0$ is an eigenvalue of $T$. Its eigenspace will just be the kernel. Now if we look at the sequence of subspaces $$\ker(T) \subset \ker(T^2) \subset \ker(T^3) \subset \dots$$ $\ker T$ is the eigenspace of $0$, and the others in the sequence (which we'll denote $K_2, K_3$, etc.) will be generalized eigenspaces for $0$. 

Conversely note that we also have a sequence of nested subspaces $\im(T) \supset \im(T^2) \supset \dots$ which we'll denote $U_1, U_2, \dots$ 

Note that we must have some index $m$ where the kernel stops growing, i.e. $K_m = K_{m+i}$ for all $i$, because the dimension of the kernel cannot grow without bound. The $U_i$ must stabilize eventually as well, at the same index $m$. Define $K$ and $U$ to be the "final", stabilized kernel and image. Note that $V$ is actually a direct sum of $U$ and $K$. To prove this, suppose $v \in K \cap U$; then $T^m(v) = 0$ since $v$ is in the stabilized kernel, and there exists some $w \in V$ with $T^m(w) = v$ since $v$ is in the stabilized range. This then implies $T^{2m}w = 0$, and since $\ker(T^{2m}) = \ker(T^m)$, we then have $w \in \ker(T^m)$. Thus since $T^m(w) = v$, $v = 0$, and so $K$ and $U$ have trivial intersection. Also $V$ is clearly a sum of $K$ and $U$, and by the triviality of the intersection, $V = K \oplus U$. 

We can prove further that $K$ and $U$ are both invariant under $T$. For $K$ (i.e. $K_m$,) any vector $v$ with $T^mv = 0$ will have $(T^{m-1})(Tv) = 0$, and so $Tv \in \ker T^{m-1}$. But $\ker T^{m-1} \subseteq \ker T^m$, so $Tv \in K$ and so $K$ is $T$-invariant. The same goes for $U_m$, where $T(U_m) = U_m$. 

Thus $K$ is a proper $T$-invariant subspace, as is $U$. If we can prove that $T$ restricted to $K$, which is a nilpotent operator, can be put in Jordan form, we'll have that $V = K \oplus U$ where $K$ can be put into Jordan form and $U$ can as well (by induction). 

# Polynomials of Operators
Suppose we have some (without much loss of generality monic) polynomial $p$ which has distinct roots and is such that $p(T) = 0$; then $P$ is diagonalizable. Proof: suppose $T$ is not diagonalizable. This means that it has some generalized eigenvector $v$ with eigenvalue $\lambda$ which is not an eigenvector. Thus $(T - \lambda I)^2v = 0$ (though we could easily modify this from $2$ to $d$ for some $d > 1$), but $(T - \lambda I)v \neq 0$. Let $w = (T - \lambda I)v$. Then $w$ is also an eigenvector of $\lambda$. On the other hand, we must have $p(T)v = 0$. We can factor $p$ as $\prod_{i=1}^n(T - \lambda_i)$, and this multiplied by $v$ will equal $0$. Let $\lambda_n = \lambda$; then $(T - \lambda)\prod_{i=1}^{n-1}(T - \lambda_i)v = 0$, or $\prod_{i=1}^{n-1}(T - \lambda_i)w = 0$. ... (FINISH) A corollary of this is that, if an operator is not diagonalizable, then any polynomial that annihilates it must have repeated roots. 

## Cayley-Hamilton Theorem
The Cayley-Hamilton Theorem says that, if we let $p$ be the characteristic polynomial of $T$, then $p(T) = 0$. Proof: put $T$ in Jordan form, i.e. choose a basis such that the matrix $M$ of $T$ is in Jordan form. The characteristic polynomial of $T$ can be deduced from its Jordan form: since $T - xI$ is lower triangular, its determinant will be the product of the diagonal entries. So the characteristic polynomial is $\prod (x - \lambda_i)^{k_i}$ where $T$ has eigenvalues $\lambda_i$ with $k_i \times k_i$ blocks in the Jordan form. If we plug $T$ into this polynomial then we get $p(T) = \prod_{i = 1}^n(T - \lambda_iI)^{k_i}$ Now consider what happens to the $i$th Jordan block in $(T - \lambda_iI)^{k_i}$. Subtracting $\lambda_i$ turns all the diagonal entries to $0$, so $T$ restricted to the generalized eigenspace of $\lambda_i$ will be nilpotent; raising it to the power $k_i$ then makes that block $0$. Thus $(T - \lambda_iI)^{k_i}$ has all zeros on its $i$th columns. If we take the product over all $i$, we get zeroes in each block and zeroes everywhere else, which means we just have the zero matrix. 

## The Minimal Polynomial
The minimal polynomial of $T$ is the monic polynomial of smallest degree that annihilates $T$. (We'll see why this is unique, and so the "the" is justified, later.)

If the minimal polynomial of $T$ has distinct roots, then $T$ is diagonalizable, as a special case of the general result above. But for the minimal polynomial, we also have the converse, that if $T$ is diagonalizable then the minimal polynomial of $T$ has distinct roots. 

We'll mainly use the following lemma. Let $p_{min}$ be the minimal polynomial of $T$. Any polynomial $q$ that annihilates $T$ will be divisible by $p_{min}$. Proof: divide $q$ by $p_{min}$ to get $q = p_{min}p' + r$ for some polynomials $p', r$ with $\deg(r) < \deg(p_{min})$ . Plugging $T$ into both sides we get $q(T) = p_{min}p'(T) + r(T)$ or $0 = 0 + r(T)$ so $r(T) = 0$. But $r(T)$ has degree less than $p_{min}$, so if it isn't the zero polynomial, that would contradict the minimality of $p_{min}$. Thus $r$ must be the zero polynomial and so $p_{min}$ must divide $q$. Uniqueness of $p_{min}$ is a corollary: if we have two minimal polynomials they must divide each other and so must be scalar multiples of each other; by the assumption that $p_{min}$ is monic we then know they must just be the same polynomial. A further corollary is that the minimal polynomial divides the characteristic polynomial. 

