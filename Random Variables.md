$\newcommand{\var}{\operatorname{Var}}$
# Definitions
A random variable is, roughly speaking, any way of assigning probabilities to numbers. A [[Discrete Distributions|discrete random variable]] is one with a finite or countable set of possible outcomes--usually, but not necessarily, integers. A continuous random variable is one with uncountably many possible outcomes, typically consisting of entire intervals of real numbers.

Discrete random variables are specified by "probability mass functions", or pmfs, which simply give, for any possible outcome $k$, the probability that the random variable is equal to $k$. In continuous random variables, the probability of any particular outcome is $0$--we can only specify the probabilities of entire *intervals*. This is done with a "probability density function" or pdf: a function $p(x)$ such that the probability that $X$ will be between $a$ and $b$ is $\int_a^b p(x)dx$. 
# Statistics of Random Variables
## Expected Value
The expected value, or mean, of a discrete random variable $X$ with probability mass function $p$ is defined as $E(X) = \sum_x xp(x)$, with the sum taken over all possible values $x$ that $X$ can take. Similarly, for a continuous variable with pdf $p$, we have $E(X) = \int xp(x)dx$. Heuristically, if you draw from a random variable $N$ times, the expected value is what the mean of those sampled values will tend towards as $N$ gets larger. 

Note that, from here on out, we informally use ideas about the distribution, expectation, etc. of multiple random variables, which are formally defined in [[#Joint Distributions]] below. 

The most important fact about the expected value is that it is linear: $E(aX) = aE(X)$ and $E(X + Y) = E(X) + E(Y)$, for any $X, Y$--regardless of independence. The first follows immediately from the definition of $E(X)$. As for the second, can argue for it from the heuristic idea of what the expectation is like. If we draw from $X + Y$ a total of $N$ times, the mean of these samples is $\frac{1}{N}\sum_{i=1}^N x_i + y_i = (\frac{1}{N}\sum_i x_i)+\frac{1}{N}(\sum_i y_i)$. For large $N$ those terms will tend towards $E(X)$ and $Y(X)$. Since $E(X + Y)$ is, intuitively, the large-$N$ limit of the left-hand side, and it is equal to the large-$N$ limit of the right-hand side, we have $E(X+Y) + E(X) + E(Y)$. We can also prove this formally from the definition of expectation. We have $E(X+Y) = \sum_{i, j} (x_i + y_j)p(x_i \cap y_j) = \sum_{i, j} x_ip(x_i \cap y_j) + \sum_{i, j}y_j p(x_i \cap y_j)$. The first term is equal to $\sum_i x_i \sum_j p(x_i \cap y_j)$, but that inner sum is equal to $p(x_i)$, hence we get $\sum_i x_i p(x_i) = E(X)$. By much the same argument the second term is equal to $E(Y)$. Thus $E(X+Y) = E(X) + E(Y)$. The case of continuous random variables can be proven similarly. 

We can identify a constant $b$ with the random variable which always takes the value $b$, and can thus identify the random variable $X + b$, given by $p(X + b = a + b) = p(X = a)$, with the sum of the random variables $X$ and $b$. We then have $E(X + b) = E(X) + E(b) = E(X) + b$. Thus "translating" a random variable "translates" the expectation. As a special case of this, letting $E(X) = \mu_X$, we have that $X - \mu_X$ "has the same shape as $X$, but with a mean of $0$", since $E(X - \mu_X) = E(X) - \mu_X = \mu_X - \mu_X = 0$. 

The expectation of a function $f$ of $X$ is given by $\sum_x f(x)p(x)$, or $\int f(x)p(x)dx$ in the continuous case. In special cases like $f(X) = aX + b$, the above results give a simple expression for $E(f(X))$; in general, there may not be such an expression, but we may still be able to calculate it explicitly from $\sum_x f(x)p(x)$. 

More generally, for any finite set of random variables, $E(X_1 + \dots + X_n) = E(X_1) + \dots + E(X_n)$. As a special case of this, when the $X_i$ are independent and identically distributed, we have $E(X_1+ \dots + X_n) = nE(X_1)$. 

When $X, Y$ are independent, we have $E(XY) = E(X)E(Y)$. This is because $E(XY) = \sum_{i, j}x_iy_jp(x_i \cap y_j) = \sum_{i, j} x_iy_j p(x_i)p(y_j)$, by the independence of $X$ and $Y$. This is then equal to $(\sum_i x_ip(x_i))(\sum_j y_j p(y_j)) = E(X)E(Y)$. 

## Variance
If $X$ is a random variable with $E(X) = \mu$, then the variance of $X$, denoted $\sigma_X^2$ or $\var(X)$, is defined as $E((X - \mu)^2)$--the mean squared deviation from the mean. 

If $X, Y$ are independent, then $\var(X + Y) = \var(X) + \var(Y)$. To prove this, let $\mu_X, \mu_Y$ be the expectations of $X$ and $Y$. Since $E(X+Y) = \mu_X + \mu_Y$, we have $\var(X+Y) = E((X + Y - \mu_X - \mu_Y)^2)$. This is then equal to $E(((X - \mu_X) + (Y - \mu_Y))^2) = E((X - \mu_X)^2 + 2(X - \mu_X)(Y - \mu_Y) + (Y - \mu_Y)^2)$ By linearity this equals $E((X - \mu_X)^2) + 2E((X - \mu_X)(Y - \mu_Y)) + E((Y - \mu_Y)^2) = \var(X) + \var(Y) + 2E((X - \mu_X)(Y - \mu_Y))$ Since $X, Y$ are independent, so are $X - \mu_X$ and $Y - \mu_Y$, so that last term is equal to $2E(X - \mu_X)E(Y - \mu_Y) = 2(E(X) - \mu_X)(E(Y) - \mu_Y) = 0$. Thus the last term drops out and we have just $\var(X+Y) = \var(X)+\var(Y)$.  Once again, this generalizes to sums of many random variables. If $X_1, \dots, X_n$ are all independent then $\var(X_1 + \dots + X_N) = \var(X_1) + \dots + \var(X_n)$. Similarly if they are independent and identically distributed then $\var(X_1 + \dots + X_n) = n\var(X_1)$. 

With the expected value, we have $E(aX + b) = aE(X) + b$; with the variance, we have simply $\var(aX + b) = a^2 \var(X)$. The rule $\var(aX) = a^2 \var(X)$ follows by the following calculation: by definition $\var(aX) = E((aX - E(aX))^2)$; letting $\mu = E(X)$, we have $E(aX) = aE(X) = a\mu$; thus $\var(aX) = E((aX - a\mu)^2) = E(a^2(X - \mu)^2) = a^2 E((X - \mu)^2) = a^2 \var(X)$. Similarly, since $E(X + b) = E(X) + b$, we have $\var(X + b) = E(((X + b) - (\mu + b))^2) = E((X - \mu)^2) = \var(X)$. 

It is sometimes more convenient to use the following identity for the variance: $\var(X) = E(X^2) - \mu^2$. To prove this, we have $\var(X) = E((X - \mu)^2) = E(X^2 - 2\mu X + \mu^2) = E(X^2) - 2\mu E(X) + \mu^2$; then, since $E(X) = \mu$, that middle term is equal to $-2\mu^2$, so we get $\var(X) = E(X^2) - 2\mu^2 + \mu^2 = E(X^2) - \mu^2$. 

The variance of a biased coin--a random variable that equals $1$ with probability $p$ and $0$ with probability $1-p$--is $p(1-p)$. This can be calculated from the identity above. We know that, in this case, $\mu = p(1) + (1-p)(0) = p$, so $\mu^2 = p^2$. We also have $X^2 = X$, because $1^2 = 1$ and $0^2 = 0$, so $E(X^2) = E(X) = p$. Thus $\var(X) = p - p^2 = p(1 - p)$. 
## Standard Deviation
The standard deviation of $X$, denoted $\sigma_X$, is defined to be the square root of the variance. All the identities for the variance translate immediately into identities for the standard deviation. 

For independent variables $X_1, \dots, X_n$, letting $X = X_1 + \dots + X_n$, we have $\sigma^2_X = \sigma^2_{X_1} + \dots + \sigma^2_{X_n}$, so $\sigma_X = \sqrt{\sigma^2_{X_1} + \dots + \sigma^2_{X_n}}$. In the special case where the $X_1$ are i.i.d we have $\sigma_X = \sqrt{n \sigma^2_{X_1}} = \sqrt{n}\sigma_{X_1}$. Similarly, since $\var(aX + b) = a^2 \var(X)$, we have $\sigma_{aX + b} = a\sigma_X$. 
### Chebyshev's Inequality
Chebyshev's inequality states that, for a random variable $X$ with mean $\mu$ and standard deviation $\sigma$, the probability that $X$ will be more than $k$ standard deviations from the mean is at most $\frac{1}{k^2}$, i.e. $P(|X - \mu| \geq k\sigma) \leq \frac{1}{k^2}$. This is similar to the "68-95-99" [[Continuous Distributions#Gaussian Distribution#Variance and Standard Deviation|rule for Gaussian distributions]]; it is weaker, since it only tells you that (for example) at least $75\%$ of the probability mass lies between $-2\sigma$ and $2\sigma$, while the rule for Gaussians tells you that $95\%$ lies in that interval, and it doesn't tell you anything about how much lies between $-\sigma$ and $\sigma$, but it applies to just about any random variable, not just normal distributions. 

To prove it, we can use Markov's inequality, which says that $P(|X| > x) \leq \frac{E(|X|)}{x}$ (note the absolute value signs here). Substituting $(X - \mu)^2$ for $X$ and $(k\sigma)^2$ for $x$, we have $E(|(X - \mu)^2|) = E((X - \mu)^2) = \sigma^2$, so $P((X - \mu)^2 > k^2\sigma^2) \leq \frac{\sigma^2}{k^2\sigma^2} = \frac{1}{k^2}$. Of course $(X - \mu)^2 > k^2\sigma^2$ if and only if $|X - \mu| > k\sigma$, so $P(|X - \mu| > k\sigma) \leq \frac{1}{k^2}$. 

# Sampling From Random Variables
Associated with any random variable are many "sampling distributions", random variables that model "drawing" several times from a given distribution and then taking some kind of summary statistic. 
## The Distribution of Sample Means
One example of this is the distribution of sample means. If $X$ is a random variable, then we denote by $\ol{X}_n$ the random variable such that $P(\ol{X}_n = x)$ is the probability that, if you take $n$ results $x_1, \dots, x_n$ from $X$ and take their mean $\frac{1}{n}(x_1 + \dots + x_n)$, you'll get a mean of $x$. Thus, letting $X_1, \dots, X_n$ be i.i.d. variables equal to $X$, we have $\ol{X}_n = \frac{1}{n}(X_1 + \dots + X_n)$. 

From this definition we immediately get that $E(\ol{X}_n) = E(\frac{1}{n}(X_1 + \dots + X_n)) = \frac{1}{n}E(X_1 + \dots + X_n) = \frac{1}{n}(E(X_1) + \dots + E(X_n))$; since the $X_i$ are i.i.d., this is then equal to $\frac{1}{n}(E(X) + \dots + E(X)) = \frac{1}{n}(nE(X)) = E(X)$. Thus the mean of the distribution of sample means is, for any $n$, equal to the mean of $X$. Since the mean of the distribution is equal to the "true" value that we're trying to estimate with the sample means, we say that the sample mean is an "unbiased estimator" of the mean. 

By a similar argument with the identities $\sigma_{X_1 + \dots + X_n} = \sqrt{n}\sigma_X$ and $\sigma_{aX} = a\sigma_X$, where the $X_i$ are i.i.d. as before, we get that the standard deviation of $\ol{X}_n$ is equal to $\frac{1}{n}(\sqrt{n}\sigma_X) = \sigma_X/\sqrt{n}$ (and $\var(\ol{X}_n) = \var(\ol{X})/n$). Thus, as $n$ gets larger and larger, the standard deviation goes to $0$, i.e. the distribution of sample means becomes less "spread out" and more "sharply peaked at its mean". This means that, as we'd expect, taking larger samples reduces the probability that $\ol{X}_n$ will be far from $E(X)$. 
## The Distribution of Sample Variances
We can define a "distribution of sample variances" in much the same way: if $x_1, \dots, x_n$ are samples from a distribution $X$, let $\tilde{s}^2 = \sum_{i=1}^n \frac{1}{n}(x_i - \ol{x})^2$, where $\ol{x}$ is the sample mean of the $x_i$.  In other words, the random variable $\tilde{s}^2$ is defined as $\sum_{i=1}^n \frac{1}{n}(X_i - \ol{X}_n)^2$, where the $X_i$ are i.i.d. to $X$. Here we use $s^2$ since we will end up calling the sample standard deviation $s$, but we put a tilde on the $s$ because what we've just defined is not what we will ultimately call the sample variance. The reason is that $\tilde{s}^2$ is a biased estimator of $\sigma_X^2$, i.e. the mean of the distribution of $\tilde{s}^2$ is not $\sigma_X^2$. Rather, it is equal to $\frac{n-1}{n}\sigma_X^2$. Thus, in the limit of large $n$, the mean of the distribution approaches $\sigma_X^2$, but for any particular $n$, $\tilde{s}^2$ will systematically underestimate $\sigma_X^2$. 
### Bessel's Correction
To prove this, we first write $E(\tilde{s}^2) = E(\frac{1}{n}\sum_{i=1}^n (X_i - \ol{X})^2) = \frac{1}{n}E(\sum_{i=1}^n (X_i - \ol{X})^2)$. Expanding out the square gets us $\frac{1}{n}E(\sum_i X_i^2 - 2X_i\ol{X} + \ol{X}^2) = \frac{1}{n}E(\sum_i X_i^2 - 2\ol{X}(\sum_i X_i) + n\ol{X}^2)$. By linearity of expectation this is equal to $\frac{1}{n}(E(\sum_i X_i^2) - 2E(\ol{X}(\sum_i X_i)) + nE(\ol{X}^2))$. 

Since the $X_i$ are i.i.d. we have $E(\sum_i X_i^2) = nE(X^2)$; we can also rewrite $\sum_i X_i$ as $n\ol{X}$. Making these substitutions we get $\frac{1}{n}(nE(X^2) - 2E(n\ol{X}\ol{X}) + nE(\ol{X}^2)) = \frac{1}{n}(nE(X^2) - 2nE(\ol{X}^2) + nE(\ol{X}^2)) = E(X^2) - E(\ol{X}^2)$. 

Thus $E(\tilde{s}^2) = E(X^2) - E(\ol{X}^2)$, which is analogous to $\var(X) = E(X^2) - \mu^2$. From that second equation we get $E(X^2) = \var(X) + \mu^2$ or $\sigma^2 + \mu^2$. If we had $E(\ol{X}^2) = \mu^2$, then we would get $E(\tilde{s}^2) = \sigma^2$, just as we'd hope. However, it is not the case that $E(\ol{X}^2) = \mu^2$. By definition of $\ol{X}$, $E(\ol{X}^2) = E((\frac{1}{n}(X_1 + \dots + X_n))^2) = \frac{1}{n^2}E((X_1 + \dots + X_n)^2)$. Expanding out the square, we get $\frac{1}{n^2}E(\sum_{i, j} X_iX_j) = \frac{1}{n^2}E(\sum_i X_i^2 + \sum_{i < j}2X_iX_j) = \frac{1}{n^2}(\sum_i E(X_i^2) + \sum_{i < j}2E(X_iX_j))$. By the independence of the $X_i$ we have $2E(X_iX_j) = 2E(X_i)E(X_j) = 2E(X)^2$ for any $i, j$, and there are $\binom{n}{2} = n(n-1)/2$ pairs of indices with $i < j$. As for the first sum, it is just equal to $nE(X^2)$. Thus we get $\frac{1}{n^2}(nE(X^2) + \frac{n(n-1)}{2}2E(X)^2) = \frac{1}{n}E(X^2) + \frac{n-1}{n}E(X)^2$. Substituting in the identity $E(X^2) = \sigma^2 + \mu^2$ this becomes $\frac{\sigma^2}{n} + \frac{\mu^2}{n} + \frac{(n-1)\mu^2}{n} = \frac{\sigma^2}{n} + \frac{n\mu^2}{n} = \frac{\sigma^2}{n} + \mu^2$. 

We now have $E(\tilde{s}^2) = E(X^2) - E(\ol{X}^2)$, $E(X^2) = \sigma^2 + \mu^2$, and $E(\ol{X}^2) = \sigma^2/n + \mu^2$. Thus $E(\tilde{s}^2) = (\sigma^2 + \mu^2) - (\sigma^2/n + \mu^2) = \sigma^2 - \frac{1}{n}\sigma^2 = \frac{n-1}{n}\sigma^2$. 

If we take $\tilde{s}^2$ and multiply it by $\frac{n}{n-1}$, we get a new random variable $s^2$ whose expectation value is $\frac{n}{n-1}\frac{n-1}{n}\sigma^2 = \sigma^2$. Thus $s^2$ is an unbiased estimator of $\sigma^2$. Expanding out $s^2$ in terms of the definition of $\tilde{s}^2$, we get $s^2 = \frac{1}{n-1}\sum_i (x_i - \ol{x})^2$, for sampled values $x_i$ with a sample mean $\ol{x}$. 
# Joint Distributions
Given two random variables $X, Y$, their *joint distribution* is a complete specification of the probabilities of pairs $(x, y)$ of one outcome from $X$ and one outcome for $Y$. 

For discrete distributions, this simply means that the joint distribution $(X, Y)$ has a pmf with $P((X, Y) = (x, y)) = P(X = x \cap Y = y)$. Given a joint distribution, we can always extract the distributions of $X$ and $Y$ individually, in the form of a "marginal pmf". For any particular outcome $x$ of $X$, we define a marginal pmf $P_X(x)$ by $\sum_y P(x, y)$; that is, we sum over all pairs $(x, y)$ with the given $x$ and an arbitrary $y$. This then gives $P(X = x)$. 

In the continuous case, we instead have a joint pdf $p(x, y)$, such that $P(a \leq X \leq b, c \leq Y \leq d) = \int_a^b \int_c^d p(x, y)dydx$; more generally, for any region $A$ in the x-y plane, the probability that we get a pair $(x, y) \in A$ is equal to $\int\int_A p(x)dxdy$. Marginal distributions are then defined in a similar way: the probability that $X = x$ is equal to $\int_{-\infty}^\infty p(x, y)dy$, where the $x$ inside that integral is fixed. 
## Independence
We can extend the notion of independent events to independent random variables as follows. If $X, Y$ are discrete random variables with pmf $P$, then they are independent if $P(x, y) = P_X(x)P_Y(y)$ for any $x, y$. (This is analogous to the rule that $A, B$ are independent events if $P(A \cap B) = P(A)P(B)$.) Similarly, for a continuous distribution, we require that the pdf $p(x, y)$ satisfies $p(x, y) = p_X(x)p_Y(y)$ for all $x, y$. 

For more than two random variables, say $X_1, \dots, X_n$, we say that they are independent if, for every subset of indices $i_1, \dots, i_m$, the joint distribution $P(X_{i_1}, \dots, X_{i_m})$ is given by a pmf or pdf with $p(x_{i_1}, \dots, x_{i_m}) = p_{X_{i_1}}(x_{i_1})\cdots p_{X_{i_m}}(x_{i_m})$, i.e. it is equal to the product of the marginal distributions.
## Conditional Distributions
For individual events, the conditional probability $P(B|A)$ is defined by $P(B|A) = P(A \cap B)/P(A)$. By analogy, we define the "conditional distribution" $Y|X$ by, for any fixed $x$,  $p_{Y|X}(y|x) = p(x, y)/p_X(x)$, where $p(x, y)$ is the pdf or pmf of the joint distribution $(X, Y)$ and $p_X$ is the marginal distribution of $X$. 

The result that, if $A$ and $B$ are independent events, $P(B) = P(B|A)$ carries over in this new setting too, as the rule $p_{Y|X}(y|x) = p_Y(y)$ when $X, Y$ are independent random variables. This follows from our definition of independence above: if $X, Y$ are independent, we have $p(x, y) = p_X(x)p_Y(y)$, so $p_{Y|X}(y|x) = (p_X(x)p_Y(y))/p_X(x) = p_Y(y)$. 
## Expectations
It doesn't make sense to ask for the expectation of a joint distribution--expectations are scalars, but outcomes of a joint distribution are vectors. We could simply define a "vector-valued expectation" as $E(X, Y) = \int_{-\infty}^\infty \int_{-\infty}^\infty (x, y)p(x, y)dxdy$ (or the analogous expression for discrete random variables) but this turns out not to be very useful. Instead, we look only at scalar valued functions of $X$ and $Y$. We then have, just like in the case of a single random variable, $E(g(X, Y)) = \sum_{(x, y)}g(x, y)p(x, y)$ (in the discrete case) or $E(g(X, Y)) = \int \int g(x, y)p(x, y)dxdy$ (in the continuous case). 

Other statistics of a single random variable, like the variance, have more complicated generalizations; see for instance [[Covariance and Correlation]]. 