$\newcommand{\var}{\operatorname{Var}}$
# Definitions
A random variable is, roughly speaking, any way of assigning probabilities to numbers. A discrete random variable is one with a finite or countable set of possible outcomes--usually, but not necessarily, integers. A continuous random variable is one with uncountably many possible outcomes, typically consisting of entire intervals of real numbers.

Discrete random variables are specified by "probability mass functions", or pmfs, which simply give, for any possible outcome $k$, the probability that the random variable is equal to $k$. In continuous random variables, the probability of any particular outcome is $0$--we can only specify the probabilities of entire *intervals*. This is done with a "probability density function" or pdf: a function $p(x)$ such that the probability that $X$ will be between $a$ and $b$ is $\int_a^b p(x)dx$. 
# Statistics of Random Variables
## Expected Value
The expected value, or mean, of a discrete random variable $X$ with probability mass function $p$ is defined as $E(X) = \sum_x xp(x)$, with the sum taken over all possible values $x$ that $X$ can take. Similarly, for a continuous variable with pdf $p$, we have $E(X) = \int xp(x)dx$. Heuristically, if you draw from a random variable $N$ times, the expected value is what the mean of those sampled values will tend towards as $N$ gets larger. 

The most important fact about the expected value is that it is linear: $E(aX) = aE(X)$ and $E(X + Y) = E(X) + E(Y)$, for any $X, Y$--regardless of independence. The first follows immediately from the definition of $E(X)$. As for the second, can argue for it from the heuristic idea of what the expectation is like. If we draw from $X + Y$ a total of $N$ times, the mean of these samples is $\frac{1}{N}\sum_{i=1}^N x_i + y_i = (\frac{1}{N}\sum_i x_i)+\frac{1}{N}(\sum_i y_i)$. For large $N$ those terms will tend towards $E(X)$ and $Y(X)$. Since $E(X + Y)$ is, intuitively, the large-$N$ limit of the left-hand side, and it is equal to the large-$N$ limit of the right-hand side, we have $E(X+Y) + E(X) + E(Y)$. We can also prove this formally from the definition of expectation. We have $E(X+Y) = \sum_{i, j} (x_i + y_j)p(x_i \cap y_j) = \sum_{i, j} x_ip(x_i \cap y_j) + \sum_{i, j}y_j p(x_i \cap y_j)$. The first term is equal to $\sum_i x_i \sum_j p(x_i \cap y_j)$, but that inner sum is equal to $p(x_i)$, hence we get $\sum_i x_i p(x_i) = E(X)$. By much the same argument the second term is equal to $E(Y)$. Thus $E(X+Y) = E(X) + E(Y)$. The case of continuous random variables can be proven similarly. 

We can identify a constant $b$ with the random variable which always takes the value $b$, and can thus identify the random variable $X + b$, given by $p(X + b = a + b) = p(X = a)$, with the sum of the random variables $X$ and $b$. We then have $E(X + b) = E(X) + E(b) = E(X) + b$. Thus "translating" a random variable "translates" the expectation. As a special case of this, letting $E(X) = \mu_X$, we have that $X - \mu_X$ "has the same shape as $X$, but with a mean of $0$", since $E(X - \mu_X) = E(X) - \mu_X = \mu_X - \mu_X = 0$. 

More generally, for any finite set of random variables, $E(X_1 + \dots + X_n) = E(X_1) + \dots + E(X_n)$. As a special case of this, when the $X_i$ are independent and identically distributed, we have $E(X_1+ \dots + X_n) = nE(X_1)$. 

When $X, Y$ are independent, we have $E(XY) = E(X)E(Y)$. This is because $E(XY) = \sum_{i, j}x_iy_jp(x_i \cap y_j) = \sum_{i, j} x_iy_j p(x_i)p(y_j)$, by the independence of $X$ and $Y$. This is then equal to $(\sum_i x_ip(x_i))(\sum_j y_j p(y_j)) = E(X)E(Y)$. 

## Variance
If $X$ is a random variable with $E(X) = \mu$, then the variance of $X$, denoted $\sigma_X^2$ or $\var(X)$, is defined as $E((X - \mu)^2)$--the mean squared deviation from the mean. 

If $X, Y$ are independent, then $\var(X + Y) = \var(X) + \var(Y)$. To prove this, let $\mu_X, \mu_Y$ be the expectations of $X$ and $Y$. Since $E(X+Y) = \mu_X + \mu_Y$, we have $\var(X+Y) = E((X + Y - \mu_X - \mu_Y)^2)$. This is then equal to $E(((X - \mu_X) + (Y - \mu_Y))^2) = E((X - \mu_X)^2 + 2(X - \mu_X)(Y - \mu_Y) + (Y - \mu_Y)^2)$ By linearity this equals $E((X - \mu_X)^2) + 2E((X - \mu_X)(Y - \mu_Y)) + E((Y - \mu_Y)^2) = \var(X) + \var(Y) + 2E((X - \mu_X)(Y - \mu_Y))$ Since $X, Y$ are independent, so are $X - \mu_X$ and $Y - \mu_Y$, so that last term is equal to $2E(X - \mu_X)E(Y - \mu_Y) = 2(E(X) - \mu_X)(E(Y) - \mu_Y) = 0$. Thus the last term drops out and we have just $\var(X+Y) = \var(X)+\var(Y)$.  Once again, this generalizes to sums of many random variables. If $X_1, \dots, X_n$ are all independent then $\var(X_1 + \dots + X_N) = \var(X_1) + \dots + \var(X_n)$. Similarly if they are independent and identically distributed then $\var(X_1 + \dots + X_n) = n\var(X_1)$. 

With the expected value, we have $E(aX + b) = aE(X) + b$; with the variance, we have simply $\var(aX + b) = a^2 \var(X)$. The rule $\var(aX) = a^2 \var(X)$ follows by the following calculation: by definition $\var(aX) = E((aX - E(aX))^2)$; letting $\mu = E(X)$, we have $E(aX) = aE(X) = a\mu$; thus $\var(aX) = E((aX - a\mu)^2) = E(a^2(X - \mu)^2) = a^2 E((X - \mu)^2) = a^2 \var(X)$. Similarly, since $E(X + b) = E(X) + b$, we have $\var(X + b) = E(((X + b) - (\mu + b))^2) = E((X - \mu)^2) = \var(X)$. 

It is sometimes more convenient to use the following identity for the variance: $\var(X) = E(X^2) - \mu^2$. To prove this, we have $\var(X) = E((X - \mu)^2) = E(X^2 - 2\mu X + \mu^2) = E(X^2) - 2\mu E(X) + \mu^2$; then, since $E(X) = \mu$, that middle term is equal to $-2\mu^2$, so we get $\var(X) = E(X^2) - 2\mu^2 + \mu^2 = E(X^2) - \mu^2$. 

The variance of a biased coin--a random variable that equals $1$ with probability $p$ and $0$ with probability $1-p$--is $p(1-p)$. This can be calculated from the identity above. We know that, in this case, $\mu = p(1) + (1-p)(0) = p$, so $\mu^2 = p^2$. We also have $X^2 = X$, because $1^2 = 1$ and $0^2 = 0$, so $E(X^2) = E(X) = p$. Thus $\var(X) = p - p^2 = p(1 - p)$. 
## Standard Deviation
The standard deviation of $X$, denoted $\sigma_X$, is defined to be the square root of the variance. All the identities for the variance translate immediately into identities for the standard deviation. 

For independent variables $X_1, \dots, X_n$, letting $X = X_1 + \dots + X_n$, we have $\sigma^2_X = \sigma^2_{X_1} + \dots + \sigma^2_{X_n}$, so $\sigma_X = \sqrt{\sigma^2_{X_1} + \dots + \sigma^2_{X_n}}$. In the special case where the $X_1$ are i.i.d we have $\sigma_X = \sqrt{n \sigma^2_{X_1}} = \sqrt{n}\sigma_{X_1}$. Similarly, since $\var(aX + b) = a^2 \var(X)$, we have $\sigma_{aX + b} = a\sigma_X$. 

# Sampling From Random Variables
Associated with any random variable are many "sampling distributions", random variables that model "drawing" several times from a given distribution and then taking some kind of summary statistic. 
## The Distribution of Sample Means
One example of this is the distribution of sample means. If $X$ is a random variable, then we denote by $\ol{X}_n$ the random variable such that $P(\ol{X}_n = x)$ is the probability that, if you take $n$ results $x_1, \dots, x_n$ from $X$ and take their mean $\frac{1}{n}(x_1 + \dots + x_n)$, you'll get a mean of $x$. Thus, letting $X_1, \dots, X_n$ be i.i.d. variables equal to $X$, we have $\ol{X}_n = \frac{1}{n}(X_1 + \dots + X_n)$. 

From this definition we immediately get that $E(\ol{X}_n) = E(\frac{1}{n}(X_1 + \dots + X_n)) = \frac{1}{n}E(X_1 + \dots + X_n) = \frac{1}{n}(E(X_1) + \dots + E(X_n))$; since the $X_i$ are i.i.d., this is then equal to $\frac{1}{n}(E(X) + \dots + E(X)) = \frac{1}{n}(nE(X)) = E(X)$. Thus the mean of the distribution of sample means is, for any $n$, equal to the mean of $X$. Since the mean of the distribution is equal to the "true" value that we're trying to estimate with the sample means, we say that the sample mean is an "unbiased estimator" of the mean. 

By a similar argument with the identities $\sigma_{X_1 + \dots + X_n} = \sqrt{n}\sigma_X$ and $\sigma_{aX} = a\sigma_X$, where the $X_i$ are i.i.d. as before, we get that the standard deviation of $\ol{X}_n$ is equal to $\frac{1}{n}(\sqrt{n}\sigma_X) = \sigma_X/\sqrt{n}$ (and $\var(\ol{X}_n) = \var(\ol{X})/n$). Thus, as $n$ gets larger and larger, the standard deviation goes to $0$, i.e. the distribution of sample means becomes less "spread out" and more "sharply peaked at its mean". This means that, as we'd expect, taking larger samples reduces the probability that $\ol{X}_n$ will be far from $E(X)$. 
## The Distribution of Sample Variances
We can define a "distribution of sample variances" in much the same way: if $x_1, \dots, x_n$ are samples from a distribution $X$, let $\tilde{s}^2 = \sum_{i=1}^n \frac{1}{n}(x_i - \ol{x})^2$, where $\ol{x}$ is the sample mean of the $x_i$.  In other words, the random variable $\tilde{s}^2$ is defined as $\sum_{i=1}^n \frac{1}{n}(X_i - \ol{X}_n)^2$, where the $X_i$ are i.i.d. to $X$. Here we use $s^2$ since we will end up calling the sample standard deviation $s$, but we put a tilde on the $s$ because what we've just defined is not what we will ultimately call the sample variance. The reason is that $\tilde{s}^2$ is a biased estimator of $\sigma_X^2$, i.e. the mean of the distribution of $\tilde{s}^2$ is not $\sigma_X^2$. Rather, it is equal to $\frac{n-1}{n}\sigma_X^2$. Thus, in the limit of large $n$, the mean of the distribution approaches $\sigma_X^2$, but for any particular $n$, $\tilde{s}^2$ will systematically underestimate $\sigma_X^2$. 
### Bessel's Correction
To prove this, we first write $E(\tilde{s}^2) = E(\frac{1}{n}\sum_{i=1}^n (X_i - \ol{X})^2) = \frac{1}{n}E(\sum_{i=1}^n (X_i - \ol{X})^2)$. Expanding out the square gets us $\frac{1}{n}E(\sum_i X_i^2 - 2X_i\ol{X} + \ol{X}^2) = \frac{1}{n}E(\sum_i X_i^2 - 2\ol{X}(\sum_i X_i) + n\ol{X}^2)$. By linearity of expectation this is equal to $\frac{1}{n}(E(\sum_i X_i^2) - 2E(\ol{X}(\sum_i X_i)) + nE(\ol{X}^2))$. 

Since the $X_i$ are i.i.d. we have $E(\sum_i X_i^2) = nE(X^2)$; we can also rewrite $\sum_i X_i$ as $n\ol{X}$. Making these substitutions we get $\frac{1}{n}(nE(X^2) - 2E(n\ol{X}\ol{X}) + nE(\ol{X}^2)) = \frac{1}{n}(nE(X^2) - 2nE(\ol{X}^2) + nE(\ol{X}^2)) = E(X^2) - E(\ol{X}^2)$. 

Thus $E(\tilde{s}^2) = E(X^2) - E(\ol{X}^2)$, which is analogous to $\var(X) = E(X^2) - \mu^2$. From that second equation we get $E(X^2) = \var(X) + \mu^2$ or $\sigma^2 + \mu^2$. If we had $E(\ol{X}^2) = \mu^2$, then we would get $E(\tilde{s}^2) = \sigma^2$, just as we'd hope. However, it is not the case that $E(\ol{X}^2) = \mu^2$. By definition of $\ol{X}$, $E(\ol{X}^2) = E((\frac{1}{n}(X_1 + \dots + X_n))^2) = \frac{1}{n^2}E((X_1 + \dots + X_n)^2)$. Expanding out the square, we get $\frac{1}{n^2}E(\sum_{i, j} X_iX_j) = \frac{1}{n^2}E(\sum_i X_i^2 + \sum_{i < j}2X_iX_j) = \frac{1}{n^2}(\sum_i E(X_i^2) + \sum_{i < j}2E(X_iX_j))$. By the independence of the $X_i$ we have $2E(X_iX_j) = 2E(X_i)E(X_j) = 2E(X)^2$ for any $i, j$, and there are $\binom{n}{2} = n(n-1)/2$ pairs of indices with $i < j$. As for the first sum, it is just equal to $nE(X^2)$. Thus we get $\frac{1}{n^2}(nE(X^2) + \frac{n(n-1)}{2}2E(X)^2) = \frac{1}{n}E(X^2) + \frac{n-1}{n}E(X)^2$. Substituting in the identity $E(X^2) = \sigma^2 + \mu^2$ this becomes $\frac{\sigma^2}{n} + \frac{\mu^2}{n} + \frac{(n-1)\mu^2}{n} = \frac{\sigma^2}{n} + \frac{n\mu^2}{n} = \frac{\sigma^2}{n} + \mu^2$. 

We now have $E(\tilde{s}^2) = E(X^2) - E(\ol{X}^2)$, $E(X^2) = \sigma^2 + \mu^2$, and $E(\ol{X}^2) = \sigma^2/n + \mu^2$. Thus $E(\tilde{s}^2) = (\sigma^2 + \mu^2) - (\sigma^2/n + \mu^2) = \sigma^2 - \frac{1}{n}\sigma^2 = \frac{n-1}{n}\sigma^2$. 

If we take $\tilde{s}^2$ and multiply it by $\frac{n}{n-1}$, we get a new random variable $s^2$ whose expectation value is $\frac{n}{n-1}\frac{n-1}{n}\sigma^2 = \sigma^2$. Thus $s^2$ is an unbiased estimator of $\sigma^2$. Expanding out $s^2$ in terms of the definition of $\tilde{s}^2$, we get $s^2 = \frac{1}{n-1}\sum_i (x_i - \ol{x})^2$, for sampled values $x_i$ with a sample mean $\ol{x}$. 
