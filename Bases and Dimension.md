Summary of the most important results about linear independence, span, and bases: in a linearly dependent list, one of the vectors lies in the span of the others, and can be removed from the list without affecting its span. A (finite) linearly independent list is always at most as long as a spanning list. As a consequence of the above, a spanning list can be reduced to a basis, and a linearly independent list can be extended to a basis. Furthermore, any two bases of the same vector space have the same length (the "dimension" of the vector space). This plus earlier results implies that a spanning list with the same length as the dimension of its vector space is automatically a basis, and the same is true of a linearly independent list of that length. 

## Definitions 
A list of vectors is **linearly independent** if the only way to write 0 (the zero vector) as a linear combination of its elements is for all the scalars to be 0, and **linearly dependent** otherwise. (To handle an edge case, the empty list is also linearly independent.)

The **span** of a list of vectors is the set of all vectors equal to linear combinations of the vectors in the list. A list **spans a vector space** V if its span is equal to V (note that it must be *equal* to V, not just include all the vectors in V. This will be important when talking about bases of subspaces, as we want to avoid a basis for the whole vector space automatically being a basis for every subspace.)

A **basis** of a vector space V is a list of elements of V which is linearly independent and spans V. 

A **finite-dimensional** vector space is spanned by some finite list of vectors; an infinite-dimensional space isn't. The **dimension** of a vector space is the length of any basis of that vector space. (This is well-defined since all bases have the same length.)

## Results about linear independence and span

### Linear Dependence Lemma (list is linearly dependent iff one element is in the span of the others)
Given a list of vectors L = $(v_1, ... v_n)$, if it is linearly dependent, then some vector in L lies in the span of the others, and this vector can be removed without affecting the span of L. Proof: given that L is linearly dependent, one of the scalars in the equation $a_1v_1 + ... + a_nv_n = 0$ must be nonzero; say $a_j$. Subtract $a_jv_j$ from both sides of the equation and divide everything by $a_j$ to get $v_j$ as a linear combination of the others, meaning that $v_j$ is in the span of the others. To show that this doesn't affect the span, say we have some $v \in \text{span}(v_1 ... v_n)$; then $v = a_1v_1 + ... + a_jv_j + ... + a_nv_n$ for some scalars $a_1...a_n$, but since $v_j$ is in the span of the other vectors, it can be replaced in the sum by a linear combination of the other $v_i$s, giving $v$ purely as a linear combination of $v_i$s that aren't $v_j$. 

We can also prove the converse, that if one vector in a list lies in the span of the others, then the list is linearly independent. Say that $v_j$ is in the span of the rest, so $v_j = a_1v_1 + ... a_{j-1}v_{j-1} + a_{j+1}v_{j+1} + ... + a_nv_n$; then we can write $0 = a_1v_1 + ... a_{j-1}v_{j-1} + a_{j+1}v_{j+1} + ... + a_nv_n - v_j$, giving 0 as a linear combination where at least one of the scalars (namely the -1 on $v_j$ ) is nonzero, meaning that the list is linearly dependent.

### Length of linearly independent list <= length of spanning list
The linear dependence lemma is used in the following: given a linearly independent list and a spanning list from the same vector space, the linearly independent list cannot be longer than the spanning list. (More formally: given lists of vectors from V, $L_1 = u_1, ... u_m, L_2 = v_1, ... v_n$ , such that L1 is linearly independent and L2 spans V, $m \leq n$.) The proof involves the following iterative process. On step 1, take $u_1$ and adjoin it to the start of $L_2$, creating the list $u_1, v_1, ... v_n$. This new list must be linearly dependent: because the $v$s span V, $u_1$ must be a linear combination of them. This means that, using the linear dependence lemma, we can remove a vector and have the resulting list span V. However, we don't want to remove just any vector; we want to remove the vector with the largest index which, in the equation $a_1u_1 + b_1v_1 + ... + b_nv_n = 0$, has a nonzero scalar. This must be one of the $v$s, since if $u_1$ was the only vector with a nonzero scalar, we would have $a_1u_1 = 0$ with $a_1 \neq 0$, meaning that $u_1$ is the zero vector, but in that case the list of $u$s would be linearly dependent. So, there must be some $v$ with a nonzero scalar, in which case we can remove that from the list $u_1, v_1, ... v_n$ without changing its span.

More generally, on the jth step of the process, we start out with a list of vectors, the first j-1 of which are $u$s, the rest of which are $v$s. We add another $u$. The resulting list must be linearly dependent, so we can use the linear dependence lemma to remove a vector; in particular, we can remove one of the $v$s, since if the scalars on the $v$s were all 0, that would mean that the list of $u$s is linearly dependent. Once we've added a $u$ and removed a $v$, the resulting list still spans V. We keep doing this until we run out of $u$s. This will happen eventually, since there are only finitely many $u$s. 

Now that this process has been outlined, we use it to actually prove the theorem. The key idea is this: suppose we ran out of $v$s before running out of $u$s, i.e. there are strictly more $u$s then $v$s. Then we would have a list that spans V, composed only of $u$s, plus some $u$s left over. This would mean that the leftover $u$s are in the span of the $u$s in the list, which would imply that the initial list of $u$s is linearly dependent. So given that the list of $u$s is not linearly dependent, we must not run out of $v$s before $u$s, i.e. there must be at most as many $u$s as $v$s, or in general, the length of a linearly independent list is less than or equal to that of a spanning list. 
## Results about bases
### Existence and uniqueness of representation as a linear combination
Bases are defined in terms of linear independence and span, but what motivates them is this: we want to be able to represent each vector in a vector space as a linear combination of some basis vectors, and we want these representations to be unique. (This will make computations involving vectors and linear maps much easier.) In fact, the two definitions are equivalent: a list $B = v_1, ... v_n \in V$ is a basis for $V$ if and only if every $v \in V$ has a unique representation as a linear combination of the elements of $B$.

To prove one direction, suppose B is a basis. Then the fact that every vector in V can be represented as a linear combination of the vectors in B follows trivially from the fact that B spans V. As for the uniqueness of such representations, say that for some $v \in V$, we have $a_1v_1 + ... + a_nv_n = v$ and $b_1v_1 + ... + b_nv_n = v$. Subtracting these two equations gets $(a_1-b_1)v_1 + ... + (a_n-b_n)v_n = 0$. Given that B is linearly independent, we must have each $a_i - b_i$ be equal to 0, or in other words, $a_i = b_i$ for each i, meaning that the two representations of $v$ have the same scalars and thus are really the same. 

For the other direction, suppose representations as linear combinations of the elements of B exist and are unique. Then the existence of representations as linear combinations implies that every vector in V is in the span of B, so V $\subseteq$ span(B). Also, span(B) $\subseteq$ V: every element of B is an element of V, and since V is a vector space and is thus closed under addition and scalar multiplication, everything in span(B) must also be an element of V. Thus span(B) = V, or B spans V. As for proving linear independence, surely the zero vector can be represented as $0v_1 + ... + 0v_n$, and since representations are unique (by hypothesis), this must be the only linear combination equal to the zero vector, so B is linearly independent. Thus B spans V and is linearly independent, so it is a basis of V.
### Spanning lists and linearly independent lists can be turned into bases
If a list spans $V$, you can turn it into a basis of $V$ by just repeatedly applying the linear dependence lemma until it's linearly independent, as the removals done by the linear independence lemma preserve the list's span. 

Similarly, a linearly independent list in $V$ can be "extended" to a basis of $V$. Let $v_1, ... v_m$ be such a linearly independent list and let $w_1, ... w_n$ be a basis of $V$. Consider the list $v_1, ... v_m, w_1, ... w_n$. Certainly it spans $V$. If it's already linearly independent (in the case where the initial list of $v$s was actually empty), then we're done. If not, we can repeatedly apply the linear dependence lemma, removing a $w$ each time, until we have a linearly independent list. (As in the proof that a linearly independent list is at most as long as a spanning list, the reason we can always remove a $w$ is that, if $v$s were the only vectors available to be removed by the linear dependence lemma, that would imply that $v_1, ... v_m$ is actually linearly dependent.) By the end, we'll have a list which spans $V$, is linearly independent, and contains all of the $v$s, so it is reasonable to say that we have "extended" the list of $v$s into a basis for $V$.
## Results about Dimension
This section covers dimension in general. For applications of dimension to specific kinds of vector spaces, see "Dimension of Subspaces" in [[Vector Spaces]]
### Dimension is well-defined: all bases of a vector space have the same length
This result is an application of the theorem that the length of a linearly independent list is less than or equal to that of a spanning list. Let $V$ be a vector space and take two bases of $V$, $A$ and $B$. $A$ is a linearly independent list in $V$ and $B$ spans $V$, so the length of $A$ is at most the length of $B$. But by the same argument, since $B$ is linearly independent in $V$ and $A$ spans $V$, the length of $B$ is at most the length of $B$. Therefore, the length of $A$ must be precisely equal to the length of $B$. 

### Dimension tells when a linearly independent or spanning list is a basis
Say that $V$ is a vector space with dimension $n$. Then we can say: if $B_1$ is a linearly independent list with length $n$, then it is a basis of $V$. Similarly, if $B_2$ spans $V$ and has length $n$, it is a basis of $V$. Proof: for the first statement, recall that any linearly independent list can be extended to a basis. If, when extending $B_1$ to a basis, we ended up with a basis of length more than $n$, that would contradict the fact that all bases of $V$ have the same length. So the "extension" must not add any vectors, i.e. $B_1$ was already a basis. For the second statement, recall that any spanning list can be reduced to a basis. By much the same argument was before, the reduction cannot leave $B_2$ with fewer than $n$ vectors, so $B_2$ must already be a basis.

