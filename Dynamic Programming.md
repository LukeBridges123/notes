# Dynamic Programming In General
Dynamic programming is a technique for solving problems (typically optimization problems) where solutions to larger problem instances depend on the solutions to smaller instances in an especially "nice" way. Naive solutions to such problems will often do a lot of redundant work, solving the same subproblems over and over, but by storing the solutions to all the relevant subproblems, we can sometimes get massive speedups, albeit at the cost of some space. We first explain what sort of "niceness" is needed for dynamic programming to be possible.
## Conditions for Dynamic Programming
### Optimal Substructure
The first condition for dynamic programming to apply (at least for optimization problems--obviously this won't make sense for other sorts of problems) is "optimal substructure". Solving a problem instance requires making some choice about how to divide the problem into subproblems (instances of the same problem with smaller sizes), finding the optimal solutions to those subproblems, and then choosing between or combining the subproblems in some way to get a solution to the larger subproblem. An optimal solution requires optimal solutions to subproblems, and optimal solutions to subproblems give you an optimal solution to the larger problem.

An example of optimal substructure is found in the problem of finding the shortest path between two nodes $a, b$ in an (unweighted) graph, i.e. the path with the fewest edges. We can first make a choice of one other node $c$ that the solution must pass through. For any such $c$, the shortest path $p_{ab}$ from $a$ to $b$ passing through $c$ is just the shortest path $p_{ac}$ from $a$ to $c$, joined to the shortest path  $p_{cb}$ from $c$ to $b$. This is because, if $p_{ab}$ passed through $c$ but included, not $p_{ac}$, but some longer path $p_{ac}'$, you could shorten $p_{ab}$ by replacing $p_{ac}'$ with $p_{ac}$--contradicting the optimality of $p_{ac}$. Note also that optimally solving one subproblem won't affect how you solve the others, since the optimal sub-paths $p_{ac}, p_{cb}$ won't share any vertices besides $c$. (If the path went from a to d to c, then from c to d to b, we could cut out $c$ and just go from $a$ to $d$ to $b$, so either $c$ wasn't an optimal choice or one of $p_{ac}, p_{cb}$ isn't optimal.)

Importantly, the subproblems should be "independent" in the sense that the choices you make in solving one subproblem don't change what options are available when solving other subproblems. In the above example, subproblems are independent because shortest sub-paths don't share edges--otherwise they wouldn't be the shortest sub-paths. If, however, we were looking for *longest* paths that don't share any edges, this property would no longer hold: it could be that the longest path from $a$ to $c$ has edges in common with the longest path from $c$ to $b$. In that case, we would no longer have optimal substructure: longest paths from $a$ to $b$ cannot, in general, be combined with longest paths from $c$ to $b$, so we cannot solve the longest-path problem by solving smaller instances of the same problem. 
### Overlapping Subproblems
Optimal substructure is what makes dynamic programming possible: it's what we need in order to build solutions to large problems out of solutions to small ones. Overlapping subproblems are what makes dynamic programming faster than naive methods. 

The subproblems of a problem "overlap" if, when solving the problem recursively, we end up solving the same problems many times. A standard example is the Fibonacci numbers. To find $F_n$ recursively, we need to find $F_{n-1}$ and $F_{n-2}$. To find $F_{n-1}$ recursively, we need $F_{n-2}$ and $F_{n-3}$--so we already need $F_{n-2}$ twice. If we keep going like this, we find that we end up computing $F_1$, say, exponentially many times. Thus, while we could get away with computing only $n$ numbers, $F_1$ through $F_n$, we end up doing an exponential amount of computation due to repeatedly solving the same subproblems. 
## Top-Down vs. Bottom-Up
Sometimes "dynamic programming" is used to refer specifically to bottom-up dynamic programming. Here, you start by solving the "smallest" subproblems, then solve the subproblems that depend only on the smallest subproblems, and so on, until you've solved the "largest" problem, the one you're trying to solve in the first place. While dynamic programming problems often start with a recursive formula expression solutions in terms of solutions to subproblems, bottom-up dynamic programming generally does not use actual recursion. 

Alternatively, you can use "top-down" methods, or "memoization": you can equip the naive recursive solution with a table (kept as a global variable outside of the function scope, so that all the recursive calls can use it) so that, on every call, the function first checks whether the table entry corresponding to the problem it's trying to solve has been filled in; if so, it makes no further recursive calls and just returns the table entry; if not, it computes that entry (possibly making further recursive calls) and fills in the table with its result. For example, suppose we take a naive recursive algorithm for the Fibonacci numbers and give it a table so it can memoize. On the outermost call to compute $F_n$, it calls itself to compute $F_{n-1}$ and $F_{n-2}$. In the process of computing $F_{n-1}$, $F_{n-2}$ gets computed and stored in the table, so that when the outermost call calls itself to compute $F_{n-2}$, it finds it in the table, and no further recursive calls are made. Of course we've only been looking at the top level, but the same ideas will apply to lower levels, with the end result that we only need to solve each subproblem at most once, as we'd hope.

The phrase "at most once" points to an interesting difference between top-down and bottom-up dynamic programming. Bottom-up dynamic programming tends to complete every subproblem possible, while top-down memoization only computes subproblems whose answers are actually needed. This property makes top-down better in some cases, though bottom-up is generally better due to the lack of overhead from recursion. 

# Examples: Polynomial Time
## Longest Increasing Subsequence

## Edit Distance

# Examples: NP-Hard Problems
## Knapsack
## Traveling Salesman
