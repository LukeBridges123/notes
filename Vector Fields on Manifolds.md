# Vector Fields and the Tangent Bundle
Intuitively, a vector field on a manifold is a function that assigns each point on the manifold one of the tangent vectors at that point. This means that, if $X$ is a vector field, it's a map $M \to TM$ with $X(p) \in T_pM$ for all $p$: in other words, a vector fields is just a section of the tangent bundle.

Now we can write vector fields in local coordinates using what we know. For any $x$ in the manifold we have, in local coordinates $x^i$, $X(x) = \sum_i^n a^i(x) \frac{\partial}{\partial x^i}$. $X$ is a smooth vector field if and only if all the coefficient functions $a^i$ are smooth. 

Vector fields can act on functions, by the rule $(Xf)(p) = (X(p))(f)$. (This works because $X(p)$ is just a derivation, so takes in functions and spits out real numbers.). Thus $Xf$ is a function $M \to \R$, and so $X$ itself can be regarded as a function $C^\infty(M) \to C^\infty(M)$, as long as $X$ is smooth. 

Indeed, $X$ is smooth (as a function from the manifold $M$ to the manifold $TM$, in the usual sense in which a function on manifolds is smooth) if and only if $Xf$ is smooth for all smooth functions $f$. Note first that we only have to show this locally, in some smooth chart with coordinates $x^i$. We then have that, if $X$ is smooth, the $a^i(x)$ will be smooth, so $Xf = \sum a^i(x) \frac{\partial f}{\partial x^i}$ is smooth (the partial derivatives of $f$ are smooth, as are the $a^i(x)$, so the sum of their products is smooth). Conversely, suppose $Xf$ is smooth for all $f$. Given $p \in U$, take a smooth bump function $\beta$ which is $1$ everywhere in some neighborhood $V$ of $p$ (which we can take to be smaller than $U$) and $0$ everywhere outside of $U$. Then $\beta x^i$ is a smooth function(thinking of the $x^i$ as the "coordinate functions" on $U$) which is defined on all points of $M$. Then $X(\beta x^i)$ is zero everywhere outside of $U$, and inside of $U$ it's equal to $\sum_j a^j(x)\frac{\partial}{\partial^j}(\beta x^i) = \sum_j a^j(x) [\frac{\partial \beta}{\partial x^i}x^i + \beta \frac{\partial x^i}{\partial x^j}]$ That first partial derivative is $0$ ($\beta$ is locally constant near $p$ and so its derivative in any direction there is $0$) while the second is the Kroenecker delta $\delta_{ij}$. Thus we're left with just $X(\beta x^i) = a^i(x)$, and since $X(\beta x^i)$ is smooth by assumption, $a^i(x)$ must be smooth as well. Since $X$ as a whole is $\sum a^i(x) \frac{\partial }{\partial x^i}$, smoothness of the $a^i$ implies smoothness of $X$. 

We've defined "derivations at p" as linear maps $C^\infty(M) \to \R$, and tended to use "derivation" as shorthand for this. Strictly speaking, though, a derivation (as opposed to a derivation at a point $p$) is a function $X: C^\infty(M) \to C^\infty(M)$ which is linear and satisfies the product rule $X(fg) = X(f)g + fX(g)$. It then turns out that being a derivation in this sense is equivalent to being a vector field, i.e. $D$ is a derivation if and only if, for some smooth vector field $X$, $Df = Xf$ for all $f$. 

So we now have a few equivalent ideas of what a vector field is: a section of the tangent bundle, a derivation (in the $C^\infty(M) \to C^\infty(M)$ sense), or (in local coordinates) as something of the form $\sum a^i(x) \frac{\partial}{\partial x^i}$. 
## The Bracket Operation
We know that the set $X(M)$ of all smooth vector fields on $M$ is a vector space over $\R$ and a module over $C^\infty(M)$. (We proved this using the terminology of sections of $TM$, rather than the terminology of vector fields, but they're the same thing.) In addition to these structures, we have the additional structure of a "bracket operation" $[X, Y]$, which is a function $X(M) \times X(M) \to X(M)$. We define this by $[X, Y](f) = X(Yf) - Y(Xf)$ for any $f$, or in other words $[X, Y]= (X \circ Y) - (Y \circ X)$. Now we check that this is, indeed, a derivation. Linearity is clear, so we just prove the product rule. We have $[X, Y](fg)=X(Y(fg)) - Y(X(fg)) = X(Yf \cdot g + f \cdot Yg) - Y(Xf \cdot g + f \cdot Xg)$. Expanding by linearity and the product rule, the first part becomes $(XYf)g + Yf \cdot Xg + Xf \cdot Yg + f \cdot (XYg)$, while the second becomes $(YXf) \cdot g + Xf \cdot Xg + Yf \cdot Xg + f \cdot (YXg)$. Clearly the middle terms of the first part cancel with the middle terms from the second part. We're left with $(XYf)g + f(XYg) - (YXf) \cdot g - f \cdot (YXg)$. Rearranging things and pulling out factors of $f$ and $g$, we get $(XYf  - YXf)\cdot g + (XYg - YXg)f$. But this is equal to $[X, Y]f \cdot g + f \cdot [X, Y] g$, so $[X, Y]$ satisfies the product rule. 

Some further properties of the bracket operation are as follows. It is bilinear, i.e. $[aX + bX', Y] = a[X, Y] + b[X', Y]$. It is skew-symmetric: $[X, Y] = -[Y, X]$. Finally, it satisfies the "Jacobi identity" $[[X, Y], Z] + [[Y, Z], X] + [[Z, X], Y] = 0$. 
## Lie Algebras
A Lie algebra over a field $K$ is a vector space over $K$ with a bracket operation satisfying the above properties (bilinearity, skew-symmetry, the Jacobi identity). We have just proven that the set $X(M)$ of all vector fields on a given manifold is an infinite-dimensional Lie algebra, with the bracket described above as the bracket operation. The cross product turns $\R^3$ into a Lie algebra, with $[X, Y] = X \times Y$. As a more trivial example, any vector space can be turned into a Lie algebra by letting $[X, Y] = 0$ for any vectors $X, Y$. (These are called "abelian Lie algebras".) 

For a less familiar example, let $\operatorname{gl}(n, \R)$ be the set of all $n \times n$ real matrices (not just invertible ones) with bracket given by the commutator, $[A, B] = AB - BA$. The proof that the commutator satisfies the axioms of a bracket operation is formally identical to the proof that the bracket operation on vector fields satisfies those properties. We can also consider subspaces of $\operatorname{gl}(n, \R)$. For example, we can define $\operatorname{sl}(n, \R)$ as the set of $n \times n$ real matrices with trace $0$. This is a vector subspace of $\text{gl}(n, \R)$, because of the general identities $\tr(A + B) = \tr(A) + \tr(B)$ (hence if $\tr(A) = 0, \tr(B) = 0$ then $\tr(A + B) = 0$) and $\tr(cA) = c\tr(A)$. As for closure under the bracket operation, since $\tr(AB) = \tr(BA)$ for any $A, B$, we have $\tr([A, B]) = \tr(AB - BA) = \tr(AB) - \tr(BA) = 0$. We also get a subspace $\text{so}(n, \R)$, defined as the set of all $n \times n$ matrices with $A^t = -A$. (These are called skew-symmetric matrices.) That it is a vector subspace follows trivially from linearity of the transpose. For the bracket, we have $[A, B]^t = (AB - BA)^t = (AB^t) - (BA)^t = B^t A^t - A^t B^t$; if $A, B \in \text{so}(n, \R)$ then $B^t A^t = (-B)(-A) = BA$, and similarly for the other term, so we get $BA - AB = -(AB - BA) = -[A, B]$. Thus $[A, B]$ is skew-symmetric for any skew-symmetric $A, B$. 

## Integral Curves
We can think of a vector field as a field of velocity vectors. For example, we could imagine the vectors as showing the speed and direction of a fluid flow at each point in that fluid. If we think of dropping a particle in the fluid, then it will follow a trajectory whose velocity at each point is equal to the velocity vector at that point; thus it will be a curve $\gamma$ such that, at each time $t$, the vector $X(\gamma(t))$ is tangent to $\gamma$ at $\gamma(t)$. To formalize this, we say that an "integral curve" of a vector field $X$ is a map $\gamma: (a, b) \to M$ with $\dot{\gamma}(t) = X_{\gamma(t)}$ for all $t$. If $0$ is in the interval on which $\gamma$ is defined, we call $\gamma(0)$ the "initial point" of $\gamma$. If $\gamma$ cannot be smoothly extended to a larger domain on $\R$ (for example, because of a singularity at the edge of its interval of definition), then we call it "maximal". 

In coordinates, we can write the vector field as $X = \sum X^i (x) \frac{\partial}{\partial x^i}$. Then, writing $\gamma(t) = (\gamma^1(t), \dots, \gamma^n(t))$, the equation $\dot{\gamma}(t) = X_{\gamma(t)}$ becomes $\frac{d\gamma^i}{dt} = X^i(\gamma^1(t), \dots, \gamma^n(t))$. This is then a system of ODEs which we can solve to find the integral curves of $X$. 

For example, take polar coordinates on $\R^2$, and consider the vector field $X(r, \theta) = r \frac{\partial}{\partial r}$. Letting $\gamma(t) = (r(t), \theta(t))$, the corresponding system of ODEs is $\frac{dr}{dt} = r, \frac{d\theta}{dt} = 0$, say with initial point $\gamma(0) = (r_0, \theta_0)$. Then we get $\gamma(t) = (r_0e^t, \theta_0)$. Thus the maximal integral curves are all of this form. 

Note, incidentally, that $r\frac{\partial}{\partial r} = \sum_i x^i \frac{\partial}{\partial x^i}$; this lets us deal with the fact that polar coordinates are undefined at the origin. The origin is then a fixed point of the vector field. 

For another example, take $X = r^2 \frac{\partial}{\partial r}$. Then we get the differential equations $\frac{dr}{dt} = r^2$, $\frac{d\theta}{dt} = 0$; solving this by separation of variables gets $r = \frac{r_0}{1 - r_0t}$, $\theta = \theta_0$. This has a singularity near $\frac{1}{r_0}$, but not anywhere else, so the maximal domain is $(-\infty, \frac{1}{r_0})$. 
