# Vector Fields and the Tangent Bundle
Intuitively, a vector field on a manifold is a function that assigns each point on the manifold one of the tangent vectors at that point. This means that, if $X$ is a vector field, it's a map $M \to TM$ with $X(p) \in T_pM$ for all $p$: in other words, a vector fields is just a section of the tangent bundle.

Now we can write vector fields in local coordinates using what we know. For any $x$ in the manifold we have, in local coordinates $x^i$, $X(x) = \sum_i^n a^i(x) \frac{\partial}{\partial x^i}$. $X$ is a smooth vector field if and only if all the coefficient functions $a^i$ are smooth. 

Vector fields can act on functions, by the rule $(Xf)(p) = (X(p))(f)$. (This works because $X(p)$ is just a derivation, so takes in functions and spits out real numbers.). Thus $Xf$ is a function $M \to \R$, and so $X$ itself can be regarded as a function $C^\infty(M) \to C^\infty(M)$, as long as $X$ is smooth. 

Indeed, $X$ is smooth (as a function from the manifold $M$ to the manifold $TM$, in the usual sense in which a function on manifolds is smooth) if and only if $Xf$ is smooth for all smooth functions $f$. Note first that we only have to show this locally, in some smooth chart with coordinates $x^i$. We then have that, if $X$ is smooth, the $a^i(x)$ will be smooth, so $Xf = \sum a^i(x) \frac{\partial f}{\partial x^i}$ is smooth (the partial derivatives of $f$ are smooth, as are the $a^i(x)$, so the sum of their products is smooth). Conversely, suppose $Xf$ is smooth for all $f$. Given $p \in U$, take a smooth bump function $\beta$ which is $1$ everywhere in some neighborhood $V$ of $p$ (which we can take to be smaller than $U$) and $0$ everywhere outside of $U$. Then $\beta x^i$ is a smooth function(thinking of the $x^i$ as the "coordinate functions" on $U$) which is defined on all points of $M$. Then $X(\beta x^i)$ is zero everywhere outside of $U$, and inside of $U$ it's equal to $\sum_j a^j(x)\frac{\partial}{\partial^j}(\beta x^i) = \sum_j a^j(x) [\frac{\partial \beta}{\partial x^i}x^i + \beta \frac{\partial x^i}{\partial x^j}]$ That first partial derivative is $0$ ($\beta$ is locally constant near $p$ and so its derivative in any direction there is $0$) while the second is the Kroenecker delta $\delta_{ij}$. Thus we're left with just $X(\beta x^i) = a^i(x)$, and since $X(\beta x^i)$ is smooth by assumption, $a^i(x)$ must be smooth as well. Since $X$ as a whole is $\sum a^i(x) \frac{\partial }{\partial x^i}$, smoothness of the $a^i$ implies smoothness of $X$. 

We've defined "derivations at p" as linear maps $C^\infty(M) \to \R$, and tended to use "derivation" as shorthand for this. Strictly speaking, though, a derivation (as opposed to a derivation at a point $p$) is a function $X: C^\infty(M) \to C^\infty(M)$ which is linear and satisfies the product rule $X(fg) = X(f)g + fX(g)$. It then turns out that being a derivation in this sense is equivalent to being a vector field, i.e. $D$ is a derivation if and only if, for some smooth vector field $X$, $Df = Xf$ for all $f$. 

So we now have a few equivalent ideas of what a vector field is: a section of the tangent bundle, a derivation (in the $C^\infty(M) \to C^\infty(M)$ sense), or (in local coordinates) as something of the form $\sum a^i(x) \frac{\partial}{\partial x^i}$. 
## The Bracket Operation
We know that the set $X(M)$ of all smooth vector fields on $M$ is a vector space over $\R$ and a module over $C^\infty(M)$. (We proved this using the terminology of sections of $TM$, rather than the terminology of vector fields, but they're the same thing.) In addition to these structures, we have the additional structure of a "bracket operation" $[X, Y]$, which is a function $X(M) \times X(M) \to X(M)$. We define this by $[X, Y](f) = X(Yf) - Y(Xf)$ for any $f$, or in other words $[X, Y]= (X \circ Y) - (Y \circ X)$. Now we check that this is, indeed, a derivation. Linearity is clear, so we just prove the product rule. We have $[X, Y](fg)=X(Y(fg)) - Y(X(fg)) = X(Yf \cdot g + f \cdot Yg) - Y(Xf \cdot g + f \cdot Xg)$. Expanding by linearity and the product rule, the first part becomes $(XYf)g + Yf \cdot Xg + Xf \cdot Yg + f \cdot (XYg)$, while the second becomes $(YXf) \cdot g + Xf \cdot Xg + Yf \cdot Xg + f \cdot (YXg)$. Clearly the middle terms of the first part cancel with the middle terms from the second part. We're left with $(XYf)g + f(XYg) - (YXf) \cdot g - f \cdot (YXg)$. Rearranging things and pulling out factors of $f$ and $g$, we get $(XYf  - YXf)\cdot g + (XYg - YXg)f$. But this is equal to $[X, Y]f \cdot g + f \cdot [X, Y] g$, so $[X, Y]$ satisfies the product rule. 

Some further properties of the bracket operation are as follows. It is bilinear, i.e. $[aX + bX', Y] = a[X, Y] + b[X', Y]$. It is skew-symmetric: $[X, Y] = -[Y, X]$. Finally, it satisfies the "Jacobi identity" $[[X, Y], Z] + [[Y, Z], X] + [[Z, X], Y] = 0$. 
## Lie Algebras
A Lie algebra over a field $K$ is a vector space over $K$ with a bracket operation satisfying the above properties (bilinearity, skew-symmetry, the Jacobi identity). We have just proven that the set $X(M)$ of all vector fields on a given manifold is an infinite-dimensional Lie algebra, with the bracket described above as the bracket operation. The cross product turns $\R^3$ into a Lie algebra, with $[X, Y] = X \times Y$. As a more trivial example, any vector space can be turned into a Lie algebra by letting $[X, Y] = 0$ for any vectors $X, Y$. (These are called "abelian Lie algebras".) 

For a less familiar example, let $\operatorname{gl}(n, \R)$ be the set of all $n \times n$ real matrices (not just invertible ones) with bracket given by the commutator, $[A, B] = AB - BA$. The proof that the commutator satisfies the axioms of a bracket operation is formally identical to the proof that the bracket operation on vector fields satisfies those properties. We can also consider subspaces of $\operatorname{gl}(n, \R)$. For example, we can define $\operatorname{sl}(n, \R)$ as the set of $n \times n$ real matrices with trace $0$. This is a vector subspace of $\text{gl}(n, \R)$, because of the general identities $\tr(A + B) = \tr(A) + \tr(B)$ (hence if $\tr(A) = 0, \tr(B) = 0$ then $\tr(A + B) = 0$) and $\tr(cA) = c\tr(A)$. As for closure under the bracket operation, since $\tr(AB) = \tr(BA)$ for any $A, B$, we have $\tr([A, B]) = \tr(AB - BA) = \tr(AB) - \tr(BA) = 0$. We also get a subspace $\text{so}(n, \R)$, defined as the set of all $n \times n$ matrices with $A^t = -A$. (These are called skew-symmetric matrices.) That it is a vector subspace follows trivially from linearity of the transpose. For the bracket, we have $[A, B]^t = (AB - BA)^t = (AB^t) - (BA)^t = B^t A^t - A^t B^t$; if $A, B \in \text{so}(n, \R)$ then $B^t A^t = (-B)(-A) = BA$, and similarly for the other term, so we get $BA - AB = -(AB - BA) = -[A, B]$. Thus $[A, B]$ is skew-symmetric for any skew-symmetric $A, B$. 

## Integral Curves
We can think of a vector field as a field of velocity vectors. For example, we could imagine the vectors as showing the speed and direction of a fluid flow at each point in that fluid. If we think of dropping a particle in the fluid, then it will follow a trajectory whose velocity at each point is equal to the velocity vector at that point; thus it will be a curve $\gamma$ such that, at each time $t$, the vector $X(\gamma(t))$ is tangent to $\gamma$ at $\gamma(t)$. To formalize this, we say that an "integral curve" of a vector field $X$ is a map $\gamma: (a, b) \to M$ with $\dot{\gamma}(t) = X_{\gamma(t)}$ for all $t$. If $0$ is in the interval on which $\gamma$ is defined, we call $\gamma(0)$ the "initial point" of $\gamma$. If $\gamma$ cannot be smoothly extended to a larger domain on $\R$ (for example, because of a singularity at the edge of its interval of definition), then we call it "maximal". 

In coordinates, we can write the vector field as $X = \sum X^i (x) \frac{\partial}{\partial x^i}$. Then, writing $\gamma(t) = (\gamma^1(t), \dots, \gamma^n(t))$, the equation $\dot{\gamma}(t) = X_{\gamma(t)}$ becomes $\frac{d\gamma^i}{dt} = X^i(\gamma^1(t), \dots, \gamma^n(t))$. This is then a system of ODEs which we can solve to find the integral curves of $X$. 

For example, take polar coordinates on $\R^2$, and consider the vector field $X(r, \theta) = r \frac{\partial}{\partial r}$. Letting $\gamma(t) = (r(t), \theta(t))$, the corresponding system of ODEs is $\frac{dr}{dt} = r, \frac{d\theta}{dt} = 0$, say with initial point $\gamma(0) = (r_0, \theta_0)$. Then we get $\gamma(t) = (r_0e^t, \theta_0)$. Thus the maximal integral curves are all of this form. 

Note, incidentally, that $r\frac{\partial}{\partial r} = \sum_i x^i \frac{\partial}{\partial x^i}$; this lets us deal with the fact that polar coordinates are undefined at the origin. The origin is then a fixed point of the vector field. 

For another example, take $X = r^2 \frac{\partial}{\partial r}$. Then we get the differential equations $\frac{dr}{dt} = r^2$, $\frac{d\theta}{dt} = 0$; solving this by separation of variables gets $r = \frac{r_0}{1 - r_0t}$, $\theta = \theta_0$. This has a singularity near $\frac{1}{r_0}$, but not anywhere else, so the maximal domain is $(-\infty, \frac{1}{r_0})$. 

### Flows on Integral Curves
Thinking back to the intuitive picture of a vector field as the velocity field of a fluid, and integral curves as trajectories through the fluid, we may think of the whole vector field as defining maps from $M$ to itself, for any number $t$, where $F_t(x)$ is where $x$ ends up after flowing along the vector field for a time $t$. To show that we can do this, we first prove the following existence theorem.

Let $X$ be a vector field on $M$; for each $p \in M$, there exists a neighborhood $U$ of $p$ and a number $\epsilon > 0$ such that there exists a unique smooth map $F: (-\epsilon, \epsilon) \times U \to M$ with the following properties:

a) For each $x \in U$, the path $\gamma(t) = F(t, x)$ (holding $x$ fixed) is an integral curve of $X$. 
b) $F(0, x) = x$
c) we have a "semigroup property": $F(t+s, x) = F(t, F(s, x))$ for all $s, t$ with $|t + s| < \epsilon$ and $F(s, x) \in U$. (From here on out we use the notation $F_t(x)$ for $F(t, x)$ with $t$ fixed. 

Intuitively, $F_t$ takes points in $M$ and lets them "flow" along $X$ for a time $t$. This explains property (c), which just says that letting a point $x$ flow for $s$ seconds, ending up at, say, $y$, and then letting $y$ flow for $t$ seconds is the same as letting $x$ flow for $s + t$ seconds. 

Proof: in local coordinates $x^i$ on a neighborhood $V$ of $p$, write $X$ in coordinates as $\sum X^i(x) \frac{\partial}{\partial x^i}$. Furthermore, write $F(t, x)$ as $(F^1(t, x), \dots, F^n(t, x))$. We then get a system of differential equations $\frac{\partial F^i}{\partial t} = x^i (F(t, x))$ that $F$ must follow, and the standard existence and uniqueness theorem implies the existence of a unique smooth solution on $(-\epsilon, \epsilon) \times U$ where $U \subseteq V$. This then satisfies (a) and (b) above. To check (c), let $G(t, x) = F(t, F_s(x))$. Then $G(t, x)$ and $F(t+s, x)$ are both equal at $t=0$, so the uniqueness of flows implies that they are the same for all time that they're both defined. Thus $F(t, F_s(x)) = F(t+s, x)$, as desired. 

We now have "local flows" defined about each point. Our next step will be to turn these into a "global flow" that acts on the whole manifold. When the flow map $F$ is defined for all $t, x$, the vector field $X$ is called "complete". 

On a compact manifold, every vector field is complete. Proof: letting $X$ be a vector field on a compact manifold $M$, we know that for each $p \in M$ there exists a "local flow" defined on a neighborhood $(-\epsilon_p, \epsilon_p) \times U_p$. Then the $U_p$ are an open cover of $M$, and we can take a finite subcover $U_1, \dots, U_k$. Let $\epsilon = \min(\epsilon_{p_1}, \dots, \epsilon_{p_k})$. By the uniqueness of flows, flows on the intersections between open sets $U_i, U_j$ agree. Thus we can unambiguously define a flow on $(-\epsilon, \epsilon) \times M \to M$, by using (for each point) the flow on the open set(s) including it. We can then extend from small times $(-\epsilon, \epsilon)$ to arbitrary times using the semigroup property. For example, we can "stretch" this to a flow on $(-2\epsilon, 2\epsilon)$ using $F_t = F_{t/2} \circ F_{t/2}$, and since these flows are defined everywhere on $M$ we won't run into any singularities. By induction we can get flows extending to any time, starting from any point. 

In the more general case of non-compact manifolds, we have the "maximal flow theorem", which states that any vector field $X$ has a unique "maximal flow" map $F: D \to M$, where $D \subseteq \R \times M$ has the following properties. For each $p \in M$, the restriction of $f$ to $D \cap L_p$, where $L_p$ is the "line along the time axis through $p$" (the set of all $(t, p)$ for $t \in \R$), is the maximal integral curve $\gamma(t)$ through $p$. (That is, this particular curve cannot be extended any further.) Also, for each $T \in \R$, $M_T$  (the set of all points where the flow exists for at least time $(-T, T)$) is open, and $F: M_T \to M_T$ is a diffeomorphism with inverse $F_{-T}$.

As an application of these ideas, suppose we have a vector field $X$ and a point $p$ where $X(p) \neq 0$. Then there exists local coordinates $x^i$ on some open subset $U$ containing $p$, where $X = \frac{\partial}{\partial x^i}$ on $U$. Thus this coordinate system "straightens out" the vector field locally. Proof: choose coordinates centered at $p$. Rotate and rescale so that $X = \frac{\partial}{\partial x^1}$ at $p$. (Ordinarily we would just have $X(p) = \sum_i a^i \frac{\partial}{\partial x^i}$, but by applying suitable transformations we can eliminate all the $a^i$ except $a^1$, and make $a^1 = 1$.) In these coordinates, let $H$ be the hyperplane orthogonal to the $x^i$ direction, i.e. the set of all points of the form $(0, x^2, \dots, x^n)$. Now let $V = H \cap U$. Define a map $\phi: (-\epsilon, \epsilon) \times V \to M$ by $\phi(t, 0, x^2, \dots, x^n) = F_t(0, x^2, \dots, x^n)$. We can write this in coordinates as $(\phi^1(t, x^2, \dots, x^n), \dots, \phi^n(t, x^2, \dots, x^n))$, and take the differential $D\phi$ at $t = 0$, and any $x^2, \dots, x^n$. It will turn out to be invertible, so by the inverse function theorem, $\phi$ is a local diffeomorphism. Thus it gives new coordinates in a smaller neighborhood of $p$. In these new coordinates, our vector field looks locally like $X = \frac{\partial}{\partial t} = \frac{\partial}{\partial x^1}$, as desired. 

This lets us define "flowouts". Let $S \subseteq M$ be a closed, embedded submanifold and let $X$ be a vector field on $M$ that is nowhere tangent to $S$, i.e. $X_p \notin T_pS$ for any $p \in S$. Then there exists a global product chart in a neighborhood $U$ of $S$, so that $(-\epsilon, \epsilon) \times S$ is diffeomorphic to $U$. (Essentially this $U$ is a "thickened" version of $S$ that you get by letting the vector field "flow out of $S$ for a small time $\epsilon$.)
### The Lie Derivative
Under a map $F: M \to N$, functions "pull back": for each $f \in \C^\infty N$ we have an associated function $F^*f = f \circ F$. Vector fields "push forward": for each $X \in X(M)$, we have an associated vector field $F_*X = (dF)(X)$ on $N$. In the special case where $F$ is a diffeomorphism, we can "pull back" vector fields by $(F^{-1}_*)(X)$. Let $F_t$ be the flow associated with $X$. The idea of a Lie derivative is that the flow generated by $X$ gives us a kind of directional derivative. Letting $p$ be a point on $M$, we can use flows to compare points at $p$ and $F_t(p)$ for finite time $p$, and then differentiate to get a "derivative in the direction of the flow", comparing $p$ to $F_{\epsilon}(p)$ for infinitesimal $\epsilon$.

First we define Lie derivatives of functions. Letting $f \in C^\infty(M)$, define the Lie derivative $L_xf(p)$ to be the limit, as $t \to 0$, of $f(F_t(p)) - f(p)/t$. In other words this is $\frac{d}{dt}(F_t^* f)$ at $t = 0$. This turns out to just be $Xf$. Proof: we have $L_xf = \frac{d}{dt}(f \circ F_t)|_{t=0} = df \circ \frac{d}{dt}F_t|_{t=0} = df(X) = X \cdot f$.
## Digression: Partitions of Unity
Recall that the support of a function is the closure of the set of all $x$ in the domain with $f(x) \neq 0$. 

Let $U_\alpha$ be an open cover of a manifold $M$. Then there exists a "partition of unity subordinate to $\{U_\alpha\}$", which is a sequence $\psi_n$ of smooth functions with the following properties: 

$\psi_n(x) \in [0, 1]$ for all $n$ and all $x \in M$. 
For each $n$, the support of $\phi_n$ is compact and lies in some $U_\alpha$. 
Each point $p$ in the manifold has a neighborhood $U$ on which only finitely many of the $\psi_n$ are nonzero.
At each point, $\sum_n \phi_n(x) = 1$. 

Proof: given $p \in M$, take a neighborhood $U_p$ with coordinates $x_p^i$. Then there exists a ball with radius $3\epsilon_p$, $B(p, 3\epsilon_p)$ (here using the coordinates so we can reasonably talk about balls; we'll later put a metric on our space which will give different coordinates) which lies in the intersection of $U_p$ and one of the $U_\alpha$ from the open cover. Then the set of all balls $B(p, \epsilon_p)$ is an open cover of $M$. We assume that manifolds are metrizable, so we can pick a distance $d$ on $M$ and an arbitrary point $q \in M$. Now, for each integer $n$, define $K_n = \ol{B(q, n+1)} \backslash B(q, n)$. These $K_n$ are compact sets whose union is $M$. Note also that, by the triangle inequality, each of the balls $B(p, 3\epsilon_p)$ intersects at most $2$ of the $K_n$, if we choose $\epsilon$ small enough (say $\epsilon < \frac{1}{10})$. If we choose finite subcovers of each of the $K_n$, we get a countable set of balls $B(p_n, \epsilon_{p_n})$ that cover $M$. There then exist bump functions $\beta_n$ which are identically $1$ on $B(p_n, \epsilon_{p_n})$ and identically $0$ outside of $B(p_n, 3\epsilon_{p_n})$. Finally, define $\psi_n(x) = \beta_n(x)/\sum_n(\beta_n(x))$. 

One application of partitions of unity is the construction of smooth bump functions on arbitrary open sets, instead of balls. Given an open set $U \subseteq M$ and a closed set $A \subset U$, there exists a smooth function $f: M \to \R$ with $0 \leq f \leq 1$, $f(x) = 1$ for all $x \in A$, and $f(x) = 0$ for all $x$ outside of $U$. 

As an example of an application of partitions of unity to manifolds, we can prove that, on any manifold $M$ ,there exists a nonzero vector field. Proof: let $\{U_\alpha\}$ be an atlas on $M$, and let $\psi_n$ be a partition of unity subordinate to $\{U_\alpha\}$. For each $n$, choose a chart $U_n$ that includes the support of $\psi_n$. In local coordinates $x_n^i$ on $U_n$, we can define a vector field by, say, $X_n = \frac{\partial}{\partial x^1}$. We can't use this everywhere, since it's only defined on a single $U_n$, and we don't have a way of "stitching together" $X_n$ defined on overlapping $U_n$. We can fix this by defining $X(x) = \sum_n \psi_n(x) X_n(x)$, which is then a vector field defined on the whole manifold. 

We can also use partitions of unity to prove the "smooth Urysohn lemma". Let $A, B \subset M$ be disjoint closed sets; then there exists a smooth function $f$ with $0 \leq f(x) \leq 1$ for all $x$; $f(x) = 1$ for all $x \in A$; and $f(x) = 0$ for all $x \in B$. Proof: letting $U = M \backslash B$, we have exactly the nested open set/closed set structure needed to construct a smooth bump function as before. 