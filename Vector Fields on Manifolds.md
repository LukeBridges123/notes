# Vector Fields and the Tangent Bundle
Intuitively, a vector field on a [[Manifolds|manifold]] is a function that assigns each point on the manifold one of the [[Tangent Spaces|tangent vectors]] at that point. This means that, if $X$ is a vector field, it's a map $M \to TM$ with $X(p) \in T_pM$ for all $p$: in other words, a vector fields is just a section of the tangent bundle.

Now we can write vector fields in local coordinates using what we know. For any $x$ in the manifold we have, in local coordinates $x^i$, $X(x) = \sum_i^n a^i(x) \frac{\partial}{\partial x^i}$. $X$ is a smooth vector field if and only if all the coefficient functions $a^i$ are smooth. 

Vector fields can act on functions, by the rule $(Xf)(p) = (X(p))(f)$. (This works because $X(p)$ is just a derivation, so takes in functions and spits out real numbers.). Thus $Xf$ is a function $M \to \R$, and so $X$ itself can be regarded as a function $C^\infty(M) \to C^\infty(M)$, as long as $X$ is smooth. 

Indeed, $X$ is smooth (as a function from the manifold $M$ to the manifold $TM$, in the usual sense in which a function on manifolds is smooth) if and only if $Xf$ is smooth for all smooth functions $f$. Note first that we only have to show this locally, in some smooth chart with coordinates $x^i$. We then have that, if $X$ is smooth, the $a^i(x)$ will be smooth, so $Xf = \sum a^i(x) \frac{\partial f}{\partial x^i}$ is smooth (the partial derivatives of $f$ are smooth, as are the $a^i(x)$, so the sum of their products is smooth). Conversely, suppose $Xf$ is smooth for all $f$. Given $p \in U$, take a smooth bump function $\beta$ which is $1$ everywhere in some neighborhood $V$ of $p$ (which we can take to be smaller than $U$) and $0$ everywhere outside of $U$. Then $\beta x^i$ is a smooth function(thinking of the $x^i$ as the "coordinate functions" on $U$) which is defined on all points of $M$. Then $X(\beta x^i)$ is zero everywhere outside of $U$, and inside of $U$ it's equal to $\sum_j a^j(x)\frac{\partial}{\partial^j}(\beta x^i) = \sum_j a^j(x) [\frac{\partial \beta}{\partial x^i}x^i + \beta \frac{\partial x^i}{\partial x^j}]$ That first partial derivative is $0$ ($\beta$ is locally constant near $p$ and so its derivative in any direction there is $0$) while the second is the Kroenecker delta $\delta_{ij}$. Thus we're left with just $X(\beta x^i) = a^i(x)$, and since $X(\beta x^i)$ is smooth by assumption, $a^i(x)$ must be smooth as well. Since $X$ as a whole is $\sum a^i(x) \frac{\partial }{\partial x^i}$, smoothness of the $a^i$ implies smoothness of $X$. 

We've defined "derivations at p" as linear maps $C^\infty(M) \to \R$, and tended to use "derivation" as shorthand for this. Strictly speaking, though, a derivation (as opposed to a derivation at a point $p$) is a function $X: C^\infty(M) \to C^\infty(M)$ which is linear and satisfies the product rule $X(fg) = X(f)g + fX(g)$. It then turns out that being a derivation in this sense is equivalent to being a vector field, i.e. $D$ is a derivation if and only if, for some smooth vector field $X$, $Df = Xf$ for all $f$. 

So we now have a few equivalent ideas of what a vector field is: a section of the tangent bundle, a derivation (in the $C^\infty(M) \to C^\infty(M)$ sense), or (in local coordinates) as something of the form $\sum a^i(x) \frac{\partial}{\partial x^i}$. 
## The Bracket Operation
We know that the set $X(M)$ of all smooth vector fields on $M$ is a vector space over $\R$ and a module over $C^\infty(M)$. (We proved this using the terminology of sections of $TM$, rather than the terminology of vector fields, but they're the same thing.) In addition to these structures, we have the additional structure of a "bracket operation" $[X, Y]$, which is a function $X(M) \times X(M) \to X(M)$. We define this by $[X, Y](f) = X(Yf) - Y(Xf)$ for any $f$, or in other words $[X, Y]= (X \circ Y) - (Y \circ X)$. Now we check that this is, indeed, a derivation. Linearity is clear, so we just prove the product rule. We have $[X, Y](fg)=X(Y(fg)) - Y(X(fg)) = X(Yf \cdot g + f \cdot Yg) - Y(Xf \cdot g + f \cdot Xg)$. Expanding by linearity and the product rule, the first part becomes $(XYf)g + Yf \cdot Xg + Xf \cdot Yg + f \cdot (XYg)$, while the second becomes $(YXf) \cdot g + Xf \cdot Xg + Yf \cdot Xg + f \cdot (YXg)$. Clearly the middle terms of the first part cancel with the middle terms from the second part. We're left with $(XYf)g + f(XYg) - (YXf) \cdot g - f \cdot (YXg)$. Rearranging things and pulling out factors of $f$ and $g$, we get $(XYf  - YXf)\cdot g + (XYg - YXg)f$. But this is equal to $[X, Y]f \cdot g + f \cdot [X, Y] g$, so $[X, Y]$ satisfies the product rule. 

Some further properties of the bracket operation are as follows. It is bilinear, i.e. $[aX + bX', Y] = a[X, Y] + b[X', Y]$. It is skew-symmetric: $[X, Y] = -[Y, X]$. Finally, it satisfies the "Jacobi identity" $[[X, Y], Z] + [[Y, Z], X] + [[Z, X], Y] = 0$. 
## Lie Algebras
A Lie algebra over a field $K$ is a vector space over $K$ with a bracket operation satisfying the above properties (bilinearity, skew-symmetry, the Jacobi identity). We have just proven that the set $X(M)$ of all vector fields on a given manifold is an infinite-dimensional Lie algebra, with the bracket described above as the bracket operation. The cross product turns $\R^3$ into a Lie algebra, with $[X, Y] = X \times Y$. As a more trivial example, any vector space can be turned into a Lie algebra by letting $[X, Y] = 0$ for any vectors $X, Y$. (These are called "abelian Lie algebras".) 

For a less familiar example, let $\operatorname{gl}(n, \R)$ be the set of all $n \times n$ real matrices (not just invertible ones) with bracket given by the commutator, $[A, B] = AB - BA$. The proof that the commutator satisfies the axioms of a bracket operation is formally identical to the proof that the bracket operation on vector fields satisfies those properties. We can also consider subspaces of $\operatorname{gl}(n, \R)$. For example, we can define $\operatorname{sl}(n, \R)$ as the set of $n \times n$ real matrices with trace $0$. This is a vector subspace of $\text{gl}(n, \R)$, because of the general identities $\tr(A + B) = \tr(A) + \tr(B)$ (hence if $\tr(A) = 0, \tr(B) = 0$ then $\tr(A + B) = 0$) and $\tr(cA) = c\tr(A)$. As for closure under the bracket operation, since $\tr(AB) = \tr(BA)$ for any $A, B$, we have $\tr([A, B]) = \tr(AB - BA) = \tr(AB) - \tr(BA) = 0$. We also get a subspace $\text{so}(n, \R)$, defined as the set of all $n \times n$ matrices with $A^t = -A$. (These are called skew-symmetric matrices.) That it is a vector subspace follows trivially from linearity of the transpose. For the bracket, we have $[A, B]^t = (AB - BA)^t = (AB^t) - (BA)^t = B^t A^t - A^t B^t$; if $A, B \in \text{so}(n, \R)$ then $B^t A^t = (-B)(-A) = BA$, and similarly for the other term, so we get $BA - AB = -(AB - BA) = -[A, B]$. Thus $[A, B]$ is skew-symmetric for any skew-symmetric $A, B$. 

## Integral Curves
We can think of a vector field as a field of velocity vectors. For example, we could imagine the vectors as showing the speed and direction of a fluid flow at each point in that fluid. If we think of dropping a particle in the fluid, then it will follow a trajectory whose velocity at each point is equal to the velocity vector at that point; thus it will be a curve $\gamma$ such that, at each time $t$, the vector $X(\gamma(t))$ is tangent to $\gamma$ at $\gamma(t)$. To formalize this, we say that an "integral curve" of a vector field $X$ is a map $\gamma: (a, b) \to M$ with $\dot{\gamma}(t) = X_{\gamma(t)}$ for all $t$. If $0$ is in the interval on which $\gamma$ is defined, we call $\gamma(0)$ the "initial point" of $\gamma$. If $\gamma$ cannot be smoothly extended to a larger domain on $\R$ (for example, because of a singularity at the edge of its interval of definition), then we call it "maximal". 

In coordinates, we can write the vector field as $X = \sum X^i (x) \frac{\partial}{\partial x^i}$. Then, writing $\gamma(t) = (\gamma^1(t), \dots, \gamma^n(t))$, the equation $\dot{\gamma}(t) = X_{\gamma(t)}$ becomes $\frac{d\gamma^i}{dt} = X^i(\gamma^1(t), \dots, \gamma^n(t))$. This is then a system of ODEs which we can solve to find the integral curves of $X$. 

For example, take polar coordinates on $\R^2$, and consider the vector field $X(r, \theta) = r \frac{\partial}{\partial r}$. Letting $\gamma(t) = (r(t), \theta(t))$, the corresponding system of ODEs is $\frac{dr}{dt} = r, \frac{d\theta}{dt} = 0$, say with initial point $\gamma(0) = (r_0, \theta_0)$. Then we get $\gamma(t) = (r_0e^t, \theta_0)$. Thus the maximal integral curves are all of this form. 

Note, incidentally, that $r\frac{\partial}{\partial r} = \sum_i x^i \frac{\partial}{\partial x^i}$; this lets us deal with the fact that polar coordinates are undefined at the origin. The origin is then a fixed point of the vector field. 

For another example, take $X = r^2 \frac{\partial}{\partial r}$. Then we get the differential equations $\frac{dr}{dt} = r^2$, $\frac{d\theta}{dt} = 0$; solving this by separation of variables gets $r = \frac{r_0}{1 - r_0t}$, $\theta = \theta_0$. This has a singularity near $\frac{1}{r_0}$, but not anywhere else, so the maximal domain is $(-\infty, \frac{1}{r_0})$. 

### Flows on Integral Curves
Thinking back to the intuitive picture of a vector field as the velocity field of a fluid, and integral curves as trajectories through the fluid, we may think of the whole vector field as defining maps from $M$ to itself, for any number $t$, where $F_t(x)$ is where $x$ ends up after flowing along the vector field for a time $t$. To show that we can do this, we first prove the following existence theorem.
#### Existence and Uniqueness of Local Flows
Let $X$ be a vector field on $M$; for each $p \in M$, there exists a neighborhood $U$ of $p$ and a number $\epsilon > 0$ such that there exists a unique smooth map $F: (-\epsilon, \epsilon) \times U \to M$ with the following properties:

a) For each $x \in U$, the path $\gamma(t) = F(t, x)$ (holding $x$ fixed) is an integral curve of $X$. 
b) $F(0, x) = x$
c) we have a "semigroup property": $F(t+s, x) = F(t, F(s, x))$ for all $s, t$ with $|t + s| < \epsilon$ and $F(s, x) \in U$. (From here on out we use the notation $F_t(x)$ for $F(t, x)$ with $t$ fixed. 

Intuitively, $F_t$ takes points in $M$ and lets them "flow" along $X$ for a time $t$. This explains property (c), which just says that letting a point $x$ flow for $s$ seconds, ending up at, say, $y$, and then letting $y$ flow for $t$ seconds is the same as letting $x$ flow for $s + t$ seconds. 

Proof: in local coordinates $x^i$ on a neighborhood $V$ of $p$, write $X$ in coordinates as $\sum X^i(x) \frac{\partial}{\partial x^i}$. Furthermore, write $F(t, x)$ as $(F^1(t, x), \dots, F^n(t, x))$. We then get a system of differential equations $\frac{\partial F^i}{\partial t} = x^i (F(t, x))$ that $F$ must follow, and the standard existence and uniqueness theorem implies the existence of a unique smooth solution on $(-\epsilon, \epsilon) \times U$ where $U \subseteq V$. This then satisfies (a) and (b) above. To check (c), let $G(t, x) = F(t, F_s(x))$. Then $G(t, x)$ and $F(t+s, x)$ are both equal at $t=0$, so the uniqueness of flows implies that they are the same for all time that they're both defined. Thus $F(t, F_s(x)) = F(t+s, x)$, as desired. 

We now have "local flows" defined about each point. Our next step will be to turn these into a "global flow" that acts on the whole manifold. Note that, when the flow map $F$ is defined for all $t, x$, the vector field $X$ is called "complete". 
#### Complete Flows and the Maximal Flow Theorem
On a compact manifold, every vector field is complete. Proof: letting $X$ be a vector field on a compact manifold $M$, we know that for each $p \in M$ there exists a "local flow" defined on a neighborhood $(-\epsilon_p, \epsilon_p) \times U_p$. Then the $U_p$ are an open cover of $M$, and we can take a finite subcover $U_1, \dots, U_k$. Let $\epsilon = \min(\epsilon_{p_1}, \dots, \epsilon_{p_k})$. By the uniqueness of flows, flows on the intersections between open sets $U_i, U_j$ agree. Thus we can unambiguously define a flow on $(-\epsilon, \epsilon) \times M \to M$, by using (for each point) the flow on the open set(s) including it. We can then extend from small times $(-\epsilon, \epsilon)$ to arbitrary times using the semigroup property. For example, we can "stretch" this to a flow on $(-2\epsilon, 2\epsilon)$ using $F_t = F_{t/2} \circ F_{t/2}$, and since these flows are defined everywhere on $M$ we won't run into any singularities. By induction we can get flows extending to any time, starting from any point. 

In the more general case of non-compact manifolds, we have the "maximal flow theorem", which states that any vector field $X$ has a unique "maximal flow" map $F: D \to M$, where $D \subseteq \R \times M$ has the following properties. For each $p \in M$, the restriction of $f$ to $D \cap L_p$, where $L_p$ is the "line along the time axis through $p$" (the set of all $(t, p)$ for $t \in \R$), is the maximal integral curve $\gamma(t)$ through $p$. (That is, this particular curve cannot be extended any further.) Also, for each $T \in \R$, $M_T$  (the set of all points where the flow exists for at least time $(-T, T)$) is open, and $F: M_T \to M_T$ is a diffeomorphism with inverse $F_{-T}$.

As an application of these ideas, suppose we have a vector field $X$ and a point $p$ where $X(p) \neq 0$. Then there exists local coordinates $x^i$ on some open subset $U$ containing $p$, where $X = \frac{\partial}{\partial x^i}$ on $U$. Thus this coordinate system "straightens out" the vector field locally. Proof: choose coordinates centered at $p$. Rotate and rescale so that $X = \frac{\partial}{\partial x^1}$ at $p$. (Ordinarily we would just have $X(p) = \sum_i a^i \frac{\partial}{\partial x^i}$, but by applying suitable transformations we can eliminate all the $a^i$ except $a^1$, and make $a^1 = 1$.) In these coordinates, let $H$ be the hyperplane orthogonal to the $x^i$ direction, i.e. the set of all points of the form $(0, x^2, \dots, x^n)$. Now let $V = H \cap U$. Define a map $\phi: (-\epsilon, \epsilon) \times V \to M$ by $\phi(t, 0, x^2, \dots, x^n) = F_t(0, x^2, \dots, x^n)$. We can write this in coordinates as $(\phi^1(t, x^2, \dots, x^n), \dots, \phi^n(t, x^2, \dots, x^n))$, and take the differential $D\phi$ at $t = 0$, and any $x^2, \dots, x^n$. It will turn out to be invertible, so by the inverse function theorem, $\phi$ is a local diffeomorphism. Thus it gives new coordinates in a smaller neighborhood of $p$. In these new coordinates, our vector field looks locally like $X = \frac{\partial}{\partial t} = \frac{\partial}{\partial x^1}$, as desired. 
#### Flowouts
This lets us define "flowouts". Let $S \subseteq M$ be a closed, embedded submanifold and let $X$ be a vector field on $M$ that is nowhere tangent to $S$, i.e. $X_p \notin T_pS$ for any $p \in S$. Then there exists a global product chart in a neighborhood $U$ of $S$, so that $(-\epsilon, \epsilon) \times S$ is diffeomorphic to $U$. (Essentially this $U$ is a "thickened" version of $S$ that you get by letting the vector field "flow out of $S$ for a small time $\epsilon$.) 
### The Lie Derivative
Under a map $F: M \to N$, functions "pull back": for each $f \in \C^\infty N$ we have an associated function $F^*f = f \circ F$. Vector fields "push forward": for each $X \in X(M)$, we have an associated vector field $F_*X = (dF)(X)$ on $N$. In the special case where $F$ is a diffeomorphism, we can "pull back" vector fields by $(F^{-1}_*)(X)$. Let $F_t$ be the flow associated with $X$. The idea of a Lie derivative is that the flow generated by $X$ gives us a kind of directional derivative. Letting $p$ be a point on $M$, we can use flows to compare points at $p$ and $F_t(p)$ for finite time $p$, and then differentiate to get a "derivative in the direction of the flow", comparing $p$ to $F_{\epsilon}(p)$ for infinitesimal $\epsilon$.

First we define Lie derivatives of functions. Letting $f \in C^\infty(M)$, define the Lie derivative $L_xf(p)$ to be the limit, as $t \to 0$, of $f(F_t(p)) - f(p)/t$. In other words this is $\frac{d}{dt}(F_t^* f)$ at $t = 0$. This turns out to just be $Xf$. Proof: we have $L_xf = \frac{d}{dt}(f \circ F_t)|_{t=0} = df \circ \frac{d}{dt}F_t|_{t=0} = df(X) = X \cdot f$.

The most general definition--which works where $S$ is a function, a vector field, or a "tensor field" more generally--is that the Lie derivative of $S$ in the "direction" of $S$, $L_XS$, is equal to $\lim_{t \to 0} (F_t^*S - S)/t = \frac{d}{dt}(F_t^*S)|_{t=0}$. (Recall that the $^*$ represents the pullback.) For a vector field $Y$, we "pull back" by $F_{-t}$, and the Lie derivative is defined as $\lim_{t \to 0} ((F_{-t})_*Y - Y)/t$. In fact, $L_X(Y)$ turns out to be just $[X, Y]$. Proof: note first that both sides are independent of any coordinate system, so if we can prove that they're equal in one coordinate system, they must be equal in general. Fix a point $p \in M$ and coordinates $x^i$ around $p$ with $X = \frac{\partial}{\partial x^1}$ (this is possible by a lemma above). If $X_p = 0$ then both sides are $0$ and we're done. Otherwise, we try to find the flow of $X$. This is simply $F_t(x^1, \dots, x^n) = (x^1 + t, \dots, x^n)$--moving in the $x^1$ direction with constant "velocity". ...........Thus $dF_t$ looks like the identity matrix, except for a $t$ in the upper left corner. Now write $Y = \sum_i Y^i(x) \frac{\partial}{\partial x^i}$. We get $L_XY = \frac{d}{dt}(F_{-t})_* Y|_{t=0} = \frac{d}{dt} (dF_{-t})Y(F_t(p))$. .............

Some corollaries about the Lie derivative which follow immediately from the above: $L_X(Y)$ is linear in both $X$ and $Y$; $L_X(Y) = -L_Y(X)$ (skew-symmetry); $L_X$ is a derivation, both in the sense that $L_X(fY) = L_Xf \cdot Y + fL_X(Y) = (Xf)Y + fL_X(Y)$, and in the sense that $L_X([Y, Z]) = [L_XY, Z] + [Y, L_XZ]$. Finally, $L_{[X, Y]} = L_XL_Y - L_YL_X$. The first few properties are just properties of the Lie bracket, translated into the notation of Lie derivatives. For the third.... The fourth is just the Jacobi identity. The fifth...

The following is called the "naturality of flows"; intuitively, it says that "diffeomorphisms map flows to flows". If $\phi: M \to N$ is a diffeomorphism, and $X$ generates $F_t$ on $M$, then $\phi_*(X)$ generates $\phi \circ F_t \circ \phi^{-1}$. Proof: let $G_t = \phi \circ F_t \circ \phi^{-1}$. Then $G_0$ is the identity map. For each time $t$ and point $p \in N$ , the time derivative $\dot{G_t}$ applied to $p$ is equal to $\frac{d}{dt}(\phi \circ F_t \circ \phi^{-1})(p)$. Setting $q = \phi^{-1}(p)$, we can let $q_t = F_t(\phi^{-1})(p)$, and rewrite $\frac{d}{dt}(\phi \circ F_t \circ \phi^{-1})(p)$ as $\frac{d}{dt}(\phi \circ F_t(q)))$. Using the chain rule we get $(d\phi)_{q(t)}\frac{d}{dt}F_t(q) = (d\phi)_{q_t}X_{q_t}$, since $X$ generates $F_t$. Note however that this is just the definition of $(\phi_*X)_{\phi(q_t)}=(\phi_*X)_{\phi F_t\phi^{-1}(p)} = (\phi_*X)_{G_t(p)}$. Thus $G_t(p)$ is an integral curve of this new vector field $(\phi_*X)$, as desired. By uniqueness, it must be the only integral curve, hence $\phi_*X$ generates $G_t$. 

 Now consider the following situation. We have two vector fields, $X$ and $Y$. We can flow along $X$ for some small time $dt$, then flow along $Y$ for the same time; or we can flow along $Y$ for a small time and then flow along $X$. We may not end up in the same place; the Lie bracket measures whether these flows "commute". More specifically, letting $X$ generate $F_t$ and $Y$ generate $G_t$, we have $[X, Y] = 0$ if and only if $F_tG_s = G_sF_t$ for all $s, t$. Proof: let $\phi = F_t$ in the previous lemma. Then the lemma, along with uniqueness of flows, says that $F_tG_sF_{-t} = G_s$ if and only if $(F_t)_*Y = Y$ for all $t$. Thus we just need to show that $[X, Y] = 0$ if and only if $(F_t)_*Y = Y$ for all $t$. For one direction, suppose $(F_t)_*(Y) = Y$ for all $Y$. We have $[X, Y] = L_XY = \frac{d}{dt}(F_t)_* Y= 0$, with the last equality following since $(F_t)_*Y = Y$ and $Y$ is independent of $t$. For the other direction, fix $p \in M$ and consider $\gamma(t) = (F_{-t})_*Y$ in $T_pM$. Then $\dot{\gamma} = \frac{d}{dt}((F_{-t})Y) = L_{X}Y =[X, Y]$. If this equals $0$, then $\dot{\gamma}$ is $0$ for all time, so since $\gamma(0) = Y$, $\gamma(t) = Y$ for all $t$, and the claim from the statement then follows.  
## Digression: Partitions of Unity
Recall that the support of a function is the closure of the set of all $x$ in the domain with $f(x) \neq 0$. 

Let $U_\alpha$ be an open cover of a manifold $M$. Then there exists a "partition of unity subordinate to $\{U_\alpha\}$", which is a sequence $\psi_n$ of smooth functions with the following properties: 

$\psi_n(x) \in [0, 1]$ for all $n$ and all $x \in M$. 
For each $n$, the support of $\phi_n$ is compact and lies in some $U_\alpha$. 
Each point $p$ in the manifold has a neighborhood $U$ on which only finitely many of the $\psi_n$ are nonzero.
At each point, $\sum_n \phi_n(x) = 1$. 

Proof: given $p \in M$, take a neighborhood $U_p$ with coordinates $x_p^i$. Then there exists a ball with radius $3\epsilon_p$, $B(p, 3\epsilon_p)$ (here using the coordinates so we can reasonably talk about balls; we'll later put a metric on our space which will give different coordinates) which lies in the intersection of $U_p$ and one of the $U_\alpha$ from the open cover. Then the set of all balls $B(p, \epsilon_p)$ is an open cover of $M$. We assume that manifolds are metrizable, so we can pick a distance $d$ on $M$ and an arbitrary point $q \in M$. Now, for each integer $n$, define $K_n = \ol{B(q, n+1)} \backslash B(q, n)$. These $K_n$ are compact sets whose union is $M$. Note also that, by the triangle inequality, each of the balls $B(p, 3\epsilon_p)$ intersects at most $2$ of the $K_n$, if we choose $\epsilon$ small enough (say $\epsilon < \frac{1}{10})$. If we choose finite subcovers of each of the $K_n$, we get a countable set of balls $B(p_n, \epsilon_{p_n})$ that cover $M$. There then exist bump functions $\beta_n$ which are identically $1$ on $B(p_n, \epsilon_{p_n})$ and identically $0$ outside of $B(p_n, 3\epsilon_{p_n})$. Finally, define $\psi_n(x) = \beta_n(x)/\sum_n(\beta_n(x))$. 

One application of partitions of unity is the construction of smooth bump functions on arbitrary open sets, instead of balls. Given an open set $U \subseteq M$ and a closed set $A \subset U$, there exists a smooth function $f: M \to \R$ with $0 \leq f \leq 1$, $f(x) = 1$ for all $x \in A$, and $f(x) = 0$ for all $x$ outside of $U$. 

As an example of an application of partitions of unity to manifolds, we can prove that, on any manifold $M$ ,there exists a nonzero vector field. Proof: let $\{U_\alpha\}$ be an atlas on $M$, and let $\psi_n$ be a partition of unity subordinate to $\{U_\alpha\}$. For each $n$, choose a chart $U_n$ that includes the support of $\psi_n$. In local coordinates $x_n^i$ on $U_n$, we can define a vector field by, say, $X_n = \frac{\partial}{\partial x^1}$. We can't use this everywhere, since it's only defined on a single $U_n$, and we don't have a way of "stitching together" $X_n$ defined on overlapping $U_n$. We can fix this by defining $X(x) = \sum_n \psi_n(x) X_n(x)$, which is then a vector field defined on the whole manifold. 

We can also use partitions of unity to prove the "smooth Urysohn lemma". Let $A, B \subset M$ be disjoint closed sets; then there exists a smooth function $f$ with $0 \leq f(x) \leq 1$ for all $x$; $f(x) = 1$ for all $x \in A$; and $f(x) = 0$ for all $x \in B$. Proof: letting $U = M \backslash B$, we have exactly the nested open set/closed set structure needed to construct a smooth bump function as before. 